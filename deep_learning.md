Deep Learning
=============
* [입문자를 위한 딥러닝 튜토리얼 ](http://courseshare.co.kr/course/39?pageType=Intro)
* [쉽게 풀어쓴 딥러닝(Deep Learning)의 거의 모든 것](http://t-robotics.blogspot.kr/2015/05/deep-learning.html)
* [구글, 유다시티에 딥러닝 강의 무료 공개](http://www.bloter.net/archives/248374)
* [[딥러닝] Gradient Descent & Normal Eq.](https://www.youtube.com/watch?v=M9Gsi3VBTYM&feature=youtu.be)
* [모두를 위한 딥러닝 강좌](http://www.se.or.kr/m/post/161)
  * **[기본적인 머신러닝과 딥러닝 강의](http://hunkim.github.io/ml/)**
  * [TensorFlow-Tutorials](https://github.com/hunkim/TensorFlow-Tutorials)
  * [Lec 00 - Machine/Deep learning 수업의 개요와 일정](https://www.youtube.com/watch?v=BS6O0zOGX4E)
  * [lec11-1 ConvNet의 Conv 레이어 만들기](https://www.youtube.com/watch?v=Em63mknbtWo&feature=youtu.be)
  * [lab11: ConvNet을 TensorFlow로 구현하자 (MNIST 99%)](https://www.youtube.com/watch?v=6KlkiKyjEu0&feature=youtu.be)
  * [lec12: NN의 꽃 RNN 이야기](https://www.youtube.com/watch?v=-SHPG_KMUkQ&feature=youtu.be)
  * [lab12: TensorFlow에서 RNN 구현하기](https://www.youtube.com/watch?v=A8wJYfDUYCk&feature=youtu.be)
* [모두를 위한 딥러닝 강좌](http://www.se.or.kr/161)
* [C++로 배우는 딥러닝](http://blog.naver.com/atelierjpro/220697890605)
  * [딥러닝 4-3. 프로그래머를 위한 경사 하강법 The Gradient Descent Method for Programmers](http://blog.naver.com/atelierjpro/220755873110)
  * [딥러닝 4-4. 프로그래머를 위한 연쇄 미분 Chain Rule](http://m.blog.naver.com/atelierjpro/220760659825)
  * [딥러닝 4.4 - 연쇄 미분 ChainRule](https://www.youtube.com/watch?v=g3nhLjYRT5I&feature=youtu.be)
  * [딥러닝 6. Fully Connected Neural Network](http://blog.naver.com/atelierjpro/220773276384)
  * [딥러닝 7. Implementing FCNN](http://blog.naver.com/atelierjpro/220774988242)
* [수학포기자를 위한 딥러닝-#1 머신러닝과 딥러닝 개요](http://bcho.tistory.com/1140)
* [수학포기자를 위한 딥러닝-#2 - 선형회귀분석을 통한 머신러닝의 기본 개념 이해](http://bcho.tistory.com/1139)
* [수학포기자를 위한 딥러닝-#3 텐서플로우로 선형회귀 학습을 구현해보자](http://bcho.tistory.com/1141)
* [수학포기자를 위한 딥러닝-#4 로지스틱 회귀를 이용한 분류 모델](http://bcho.tistory.com/1142)
* [[딥러닝] 1. Introduction](https://www.youtube.com/watch?v=E6Dqu4THRu8)
* [A Beginner’s Guide to Deep Neural Networks](http://googleresearch.blogspot.kr/2015/09/a-beginners-guide-to-deep-neural.html)
* [Deep Learning for Beginners](http://randomekek.github.io/deep/deeplearning.html)
* [Welcome to the Deep Learning Tutorial!](http://deeplearning.stanford.edu/tutorial/)
* [ISBA 2015 Morning Tutorial: Deep Learning (March 23, 2015)](https://www.youtube.com/watch?v=gCwYO7zVJs0)
* [tutorial, implementations](https://github.com/dsindex/blog/wiki/%5Bdeep-learning%5D-tutorial,-implementations)
* [Deep Learning - Taking machine learning to the next level](https://www.udacity.com/course/deep-learning--ud730)
* **[Awesome Deep Learning](https://github.com/ChristosChristofidis/awesome-deep-learning)**
* [Awesome - Most Cited Deep Learning Papers](https://github.com/terryum/awesome-deep-learning-papers)
* [Up to Speed on Deep Learning: September, Part 2 and October, Part 1](https://medium.com/the-mission/up-to-speed-on-deep-learning-september-part-2-and-october-part-1-d72d7e5df1ea)
* [My playlist – Top YouTube Videos on Machine Learning, Neural Network & Deep Learning](http://www.analyticsvidhya.com/blog/2015/07/top-youtube-videos-machine-learning-neural-network-deep-learning/)
* [Designing Machine Learning Models: A Tale of Precision and Recall](http://nerds.airbnb.com/designing-machine-learning-models/)
* [[deep learning] tutorial, implementations](https://github.com/dsindex/blog/wiki/%5Bdeep-learning%5D-tutorial,-implementations)
* [Deep Learning Study](http://deeplearningstudy.github.io/material/) Caffe, TensorFlow
* [The Deep Learning Playbook](https://medium.com/@jiefeng/deep-learning-playbook-c5ebe34f8a1a)
* [A Brief Overview of Deep Learning](http://yyue.blogspot.kr/2015/01/a-brief-overview-of-deep-learning.html)
* [github.com/wbaek/deeplearing_exercise](https://github.com/wbaek/deeplearing_exercise)
* [deepcumen.com](http://deepcumen.com/)
* [Deep Learning Study - Study of HeXA at Ulsan National Institute of Science and Technology](https://github.com/carpedm20/deep-learning-study)
* [[ML] My Journal from Neural Network to Deep Learning: A Brief Introduction to Deep Learning. Contents](http://haohanw.blogspot.kr/2015/01/deep-learning-introduction.html)
* [Deep learning - Yann LeCun, Yoshua Bengio & Geoffrey Hinton](http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html)
* [Deep learning](http://neuralnetworksanddeeplearning.com)
* [nvidia Deep Learning Courses](https://developer.nvidia.com/deep-learning-courses)
  * [Deep Learning Course](https://www.youtube.com/playlist?list=PL5B692fm6--tI-ijknnVZWbXU2H4JpSYe)
* [CS224d: Deep Learning for Natural Language Processing](http://cs224d.stanford.edu/)
* [Why GEMM is at the heart of deep learning](http://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/)
* [GitHub Special: Data Scientists to Follow & Best Tutorials on GitHub](http://www.analyticsvidhya.com/blog/2015/07/github-special-data-scientists-to-follow-best-tutorials/)
* [딥러닝 워크샵: 딥러닝의 현재와 미래](http://mlcenter.postech.ac.kr/workshop)
  * [후기](http://whydsp.org/262)
* [Deep Learning at Flickr, Pierre Garrigues](http://techjaw.com/2015/03/04/deep-learning-at-flickr-pierre-garrigues/)
* [Andrew Ng: Why ‘Deep Learning’ Is a Mandate for Humans, Not Just Machines](http://www.wired.com/2015/05/andrew-ng-deep-learning-mandate-humans-not-just-machines/)
* [Andrew Ng: Deep Learning, Self-Taught Learning and Unsupervised Feature Learning](https://www.youtube.com/watch?v=n1ViNeWhC24)
* [The tensor renaissance in data science](http://radar.oreilly.com/2015/05/the-tensor-renaissance-in-data-science.html)
* [The Paradox of Deeper Learning: The Unlearning Curve](http://blogs.edweek.org/edweek/learning_deeply/2015/04/the_paradox_of_deeper_learning_the_unlearning_curve.html)
* [Are there Deep Reasons Underlying the Pathologies of Today’s Deep Learning Algorithms?](http://goertzel.org/DeepLearning_v1.pdf)
* [집단지성프로그래밍 05. 최적화(optimization) 김지은_20150522](http://www.slideshare.net/yeswldms/05-optimization-20150522)
* [ICLR 2015](http://www.iclr.cc/doku.php?id=iclr2015%3Amain)
  * [Artificial Tasks for Artificial Intelligence](https://www.dropbox.com/s/ly9y136saba0915/ICLR2015_Oral_Slides_All.pdf?oref=e&n=117881854)
* [Deep Learning Trends @ ICLR 2016](http://www.computervisionblog.com/2016/06/deep-learning-trends-iclr-2016.html)
* [Deep Learning for Computer Vision Barcelona](http://imatge-upc.github.io/telecombcn-2016-dlcv/)
* [Algorithms of the Mind](https://medium.com/deep-learning-101/algorithms-of-the-mind-10eb13f61fc4)
* [What's Wrong With Deep Learning?](https://drive.google.com/file/d/0BxKBnD5y2M8NVHRiVXBnOVpiYUk/edit)
* [Google DeepMind Teaches Artificial Intelligence Machines to Read](http://www.technologyreview.com/view/538616/google-deepmind-teaches-artificial-intelligence-machines-to-read/)
* [Why does Deep Learning work?](https://charlesmartin14.wordpress.com/2015/03/25/why-does-deep-learning-work/)
* [Why Deep Learning Works II: the Renormalization Group](https://charlesmartin14.wordpress.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/)
* [18 Great Deep Learning Resources, most free](http://blog.sense.io/18-great-deep-learning-resources)
* [인공지능의 눈으로 바라본 세상](http://techneedle.com/archives/20800)
* [XLDB2015: Accelerating Deep Learning at Facebook](https://www.youtube.com/watch?v=KviuMAF4pEA)
* [The Brain vs Deep Learning Part I: Computational Complexity — Or Why the Singularity Is Nowhere Near](https://timdettmers.wordpress.com/2015/07/27/brain-vs-deep-learning-singularity/)
* [A Beginner’s Guide to Restricted Boltzmann Machines](http://deeplearning4j.org/restrictedboltzmannmachine.html)
* [Energy based models and boltzmann machines - v2.0](http://www.slideshare.net/blaswan/energy-based-models-and-boltzmann-machines-v20)
* [내맘대로 이해하는 Deep Belief Network와Restricted Boltzmann Machine](http://whydsp.org/283)
* [Deep Learning - Geoffrey Hinton - how to do backpropagation in a brain](https://www.youtube.com/watch?v=kxp7eWZa-2M&feature=youtu.be&t=38m13s)
* [Deep learning for assisting the process of music composition (part 1)](https://highnoongmt.wordpress.com/2015/08/11/deep-learning-for-assisting-the-process-of-music-composition-part-1/)
* [Andrew Ng: Deep Learning, Self-Taught Learning and Unsupervised Feature Learning](https://www.youtube.com/watch?v=n1ViNeWhC24)
* [Deep Learning Summer School 2015](https://sites.google.com/site/deeplearningsummerschool/)
  * [Deep Learning Summer School, Montreal 2015](http://videolectures.net/deeplearning2015_montreal/)
* [Deep Learning Summer School 2016](https://sites.google.com/site/deeplearningsummerschool2016/)
  * [Deep Learning Summer School, Montreal 2016](http://videolectures.net/deeplearning2016_montreal/)
  * [What I learned from Deep Learning Summer School 2016](https://www.linkedin.com/pulse/what-i-learned-from-deep-learning-summer-school-2016-hamid-palangi)
* [26 THINGS I LEARNED IN THE DEEP LEARNING SUMMER SCHOOL](http://www.marekrei.com/blog/26-things-i-learned-in-the-deep-learning-summer-school/)
* **[Calculus on Computational Graphs: Backpropagation](http://colah.github.io/posts/2015-08-Backprop/index.html)**
* [Gradient Descent with Backpropagation](http://outlace.com/Beginner-Tutorial-Backpropagation/)
* [Deep Learning and Neural Networks](http://cl.naist.jp/~kevinduh/a/deep2014/)
* [한국에서 처음 열린 GTC, 딥러닝의 현재를 이야기하다](http://chitsol.com/2273)
* [네이버, 사람 없이 이미지 뉴스 만든다](http://www.bloter.net/archives/238742)
* [Deep Learning Startups, Applications and Acquisitions – A Summary](http://blog.dennybritz.com/2015/10/13/deep-learning-startups-applications-and-acquisitions-a-summary/)
* [Theoretical Motivations for Deep Learning](http://rinuboney.github.io/2015/10/18/theoretical-motivations-deep-learning.html)
* [How We Use Deep Learning to Classify Business Photos at Yelp](http://engineeringblog.yelp.com/2015/10/how-we-use-deep-learning-to-classify-business-photos-at-yelp.html)
* [Artificial Intelligence, Neural Networks, and Deep Learning](http://kimschmidtsbrain.com/2015/10/29/artificial-intelligence-neural-networks-and-deep-learning/)
* [Deep Learning in a Single File for Smart Devices](http://dmlc.ml/mxnet/2015/11/10/deep-learning-in-a-single-file-for-smart-device.html)
* [Boosting Methods](http://enginius.tistory.com/m/post/606)
* [Evaluation of Deep Learning Toolkits](https://github.com/zer0n/deepframeworks/blob/master/README.md)
* [10 Deep Learning Trends at NIPS 2015](http://codinginparadise.org/ebooks/html/blog/ten_deep_learning_trends_at_nips_2015.html)
  * [딥러닝의 10가지 트렌드 from NIPS 2015](http://t-robotics.blogspot.com/2016/01/10-from-nips-2015.html)
* [Deep Residual Networks](https://github.com/KaimingHe/deep-residual-networks)
* [stat212b - Topics Course on Deep Learning for Spring 2016](https://github.com/joanbruna/stat212b)
* [Fujitsu develops new deep learning technology to analyze time-series data with high precision](http://phys.org/news/2016-02-fujitsu-deep-technology-time-series-high.html)
* 2016-02-17~18 자연어처리 튜토리얼 심층학습과 언어처리 응용
  * TensorFlow Tutorial; SKT 정상근 박사님
    * [GitHub](https://github.com/hugman/deep_learning)
    * mnist.py 파일을 먼저 본다.
    * mnist_with_monitoring.py
      * TensorBoard 서버에 모니터링 로그를 보내고 웹으로 볼 수 있다
    * Keras
      * computaion backend를 추상화하여 편하게 적용할 수 있도록 하는 라이브러리
      * theano, tensorflow 둘다 지원
      * 형태소 분석기같은 걸 빠르게 만들어 보기에 좋다
    * pos_tagger_fcn.py
      * word embedding => wikipedia로 SENNA에서 만들어서 오픈한 데이터
      * 변수명은 mnist와 일부러 동일하게 뒀으니 비교해서 보면 좋음
      * 참고: fcn => fully connected network
    * pos_tagger_rnn_seq.py
      * 궁극의 코드!
      * learning rate를 처음에는 크게해서 점점 작게 만들어 준다
      * optimizer를 바꿔가며 학습을 할 수도 있다
      * epoch이 50이 넘으면 중간에 바꿔라! 등
    * TensorFlow github 소스에 보면,
      * models -> rnn -> translate 구글의 기계번역 소스가 있다
      * https://github.com/tensorflow/tensorflow/tree/master/tensorflow/models/rnn/translate
  * Deep Learning for NLP 응용; 강원대 이창기 박사님
    * [NLP from Scratch](http://arxiv.org/abs/1103.0398)
    * [SENNA](http://ronan.collobert.com/senna)
      * CNN + CRF
    * RNN(LSTM) + attention score
      * attention score는 alignment prob과 동일한 역할을 해준다.
    * t-SNE scatter plot
      * 자질의 차원을 축소하여 2차원으로 뿌려주는 방식
* [오픈 소스 딥러닝 소프트웨어](http://kernelstudy.net/t/topic/188)
* [DataScience/Deep Learning](http://khanrc.tistory.com/category/DataScience/Deep%20Learning)
* [Introduction to Deep Learning for Image Analysis at Strata NYC, Sep 2015](http://www.slideshare.net/dato-inc/introduction-to-deep-learning-for-image-analysis-at-strata-nyc-sep-2015)
* [Show and tell takmin: A Neural Image Caption Generator](http://www.slideshare.net/takmin/show-andtell-takmin)
* [딥러닝 임팩트가 온다](http://techholic.co.kr/archives/51820)
* [Deep Learning for Visual Question Answering](http://avisingh599.github.io/deeplearning/visual-qa/)
* [When Does Deep Learning Work Better Than SVMs or Random Forests?](http://www.kdnuggets.com/2016/04/deep-learning-vs-svm-random-forest.html)
* [openai.com](https://openai.com)
  * [OpenAI Gym Beta](https://openai.com/blog/openai-gym-beta/)
  * [requests-for-research](https://openai.com/requests-for-research/)
* [Neural Programmer-Interpreters](http://www-personal.umich.edu/~reedscot/iclr_project.html)
* [Video Recordings of the ICML’15 Deep Learning Workshop](http://dpkingma.com/?page_id=483)
  * [딥러닝 워크샵 패널토의 @ ICML2015](http://t-robotics.blogspot.com/2015/07/icml2015.html#.VyyRohWLSZ0)
* [Deep Learning: Nine Lectures at Collège de France](http://cilvr.nyu.edu/doku.php?id=courses%3Adeeplearning-cdf2016%3Astart)
* [Deep Learning with RE•WORK #reworkDL](https://www.youtube.com/playlist?list=PLnDbcXCpYZ8m412d2KX5paGKdGUxxWCEP)
* [csl.sony.fr/publications](https://www.csl.sony.fr/publications.php)
  * [인공지능이 편곡한 '환희의 송가'](http://www.yonhapnews.co.kr/local/0899000000.html?cid=MYH20160511017400797)
  * [Machine Learning Techniques for Reorchestrating the European Anthem](https://www.youtube.com/watch?list=PLuOoXrWK6Kz5ySULxGMtAUdZEg9SkXDoq&v=0qnTaAz-xtQ)
* [Deep Patient: An Unsupervised Representation to Predict the Future of Patients from the Electronic Health Records](http://www.nature.com/articles/srep26094)
* [deepnumbers.com](http://www.deepnumbers.com/)
* [10 Deep Learning Terms Explained in Simple English](http://www.datasciencecentral.com/m/blogpost?id=6448529%3ABlogPost%3A410633)
* [A Statistical View of Deep Learning](http://blog.shakirm.com/ml-series/a-statistical-view-of-deep-learning/)
* [Deep Learning in Practice: Speech Recognition and Beyond](http://events.technologyreview.com/emtech/digital/16/video/watch/andrew-ng-deep-learning/)
* [WaveNet: A Generative Model for Raw Audio](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)
  * [A TensorFlow implementation of DeepMind's WaveNet paper](https://github.com/ibab/tensorflow-wavenet)
* [[모두의연구소] 쫄지말자딥러닝](http://www.slideshare.net/modulabs/ss-62503747)
* [www.modulabs.co.kr/DeepLAB](http://www.modulabs.co.kr/DeepLAB)
  * [딥러닝연구실](http://whydsp.org/m/post?categoryId=525022) 과거 자료
* [Bayesian Deep Learning](http://twiecki.github.io/blog/2016/06/01/bayesian-deep-learning/)
* [Bayesian Machine Learning, Explained](http://www.rightrelevance.com/search/articles/hero?article=5f8cc010177776a7f4d48089ec4e539dc42a1ff9)
* [Five Hundred Deep Learning Papers, Graphviz and Python](http://dnlcrl.github.io/projects/2015/10/10/500-deep-learning-papers-graphviz-python.html?imm_mid=0dd0f3&cmp=em-data-na-na-newsltr_20151202)
* [What My Deep Model Doesn't Know...](http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html)
* [Deep Learning with Eigenvalue Decay Regularizer](https://www.researchgate.net/publication/301648136_Deep_Learning_with_Eigenvalue_Decay_Regularizer)
* [Deep Network with Stochastic Depth](https://www.evernote.com/shard/s462/sh/2de09526-e8fe-48d9-90da-9baa356d5e1a/7a4259299b26c41d60e05e894dbbc2fa)
* [Prof. Geoff Hinton - Deep Learning](https://www.youtube.com/watch?v=VhmE_UXDOGs&feature=youtu.be)
* [Autoencoders](http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/)
* [인공 신경망에 관한 설명. 스탠포드 대학 앤드류 응 교수의 sparse autoencoder 정리 노트로 인공신경망 이해하기](http://woongheelee.com/m/entry/%EC%9D%B8%EA%B3%B5-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%97%90-%EA%B4%80%ED%95%9C-%EC%84%A4%EB%AA%85-%EC%8A%A4%ED%83%A0%ED%8F%AC%EB%93%9C-%EB%8C%80%ED%95%99-%EC%95%A4%EB%93%9C%EB%A5%98-%EC%9D%91-%EA%B5%90%EC%88%98%EC%9D%98-sparse-autoencoder-%EC%A0%95%EB%A6%AC-%EB%85%B8%ED%8A%B8%EB%A1%9C-%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0)
* [What're the differences between PCA and autoencoder?](http://stats.stackexchange.com/questions/120080/whatre-the-differences-between-pca-and-autoencoder)
* [1. 오토인코더(Sparse Autoencoder) 1 – AutoEncoders & Sparsity](http://solarisailab.com/archives/113?ckattempt=1)
* [Stanford Seminar - Song Han of Stanford University](https://www.youtube.com/watch?v=hfFkS_vHslI&feature=youtu.be)
  * ["Techniques for Efficient Implementation of Deep Neural Networks," a Presentation from Stanford](http://www.slideshare.net/embeddedvision/techniques-for-efficient-implementation-of-deep-neural-networks-a-presentation-from-stanford)
* [Deep Learning, Tools and Methods workshop](https://portal.klewel.com/watch/webcast/deep-learning-tools-and-methods-workshop/)
* [How to Start Learning Deep Learning](http://www.kdnuggets.com/2016/07/start-learning-deep-learning.html)
* [Summary of Deep Learning Environments](https://www.facebook.com/notes/239472486233783/Summary%20of%20Deep%20Learning%20Environments/587130401467988/)
* [Deep learning tutorials (2nd ed.)](https://github.com/sjchoi86/dl_tutorials)
* [Deep Learning for Everyone – and (Almost) Free](http://www.datasciencecentral.com/profiles/blogs/deep-learning-for-everyone-and-almost-free)
* [Deep Learning for Object Detection with DIGITS](https://devblogs.nvidia.com/parallelforall/deep-learning-object-detection-digits/)
* [khanrc.tistory.com/category/DataScience/Deep Learning](http://khanrc.tistory.com/category/DataScience/Deep%20Learning)
* [도커와 AWS를 활용한 클라우드 딥러닝 환경 구축](https://gist.github.com/haje01/f13053738853f39ce5a2)
  * [Decoupled Neural Interfaces using Synthetic Gradients[1608.05343] Summary](https://tensorflowkorea.wordpress.com/2016/08/22/decoupled-neural-interfaces-using-synthetic-gradients1608-05343-summary/)
* [Initialization Of Deep Networks Case of Rectifiers](http://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/)
* [ANN 구현하고 x^2 근사함수 찾기](https://github.com/dgtgrade/HumanLearning/blob/master/1001.py)
  * Universal Approximation Theorem 에 따르면 간단한 ANN으로도 가능
  * 구현된 간단한 ANN
    * 입력 레이어: 노드 1개 
      * x 그대로임. 즉, feature 로서 다항식이나 비선형 함수 사용하지 않음
    * 히든 레이어: 1개, 노드: 50개
      * a = sigmoid(wx + b1)
    * 출력 레이어: 노드 1개
      * o = wa + b2
    * 코스트 함수: Squared Error
    * 히든 레이어가 하나라서 DNN이라고 적지 않음
    * ANN 중 가장 표준적이고 기초적이라고 할 수 있는 ANN 그대로, 또는 그중에서도 가장 간단한 형태라고 보면 됨
  * 실험 결과
    * t(x)=x^2 함수: x=[1.0~12.0]까지 학습 시켰는데 잘 되었음
    * t(x)=sin(x) 함수: x=[1.0~10.0]까지 학습 시켰는데 잘 되었음
    * t(x)=x^3+3*sin(x)^2-10 함수: x=[-5.5~5.5]까지 학습 시켰는데 잘 되었음
    * 히든 노드수를 늘릴 수록 더 넓은 범위의 x 값을 커버할 수 있음을 일정 범위 내에서 확인
  * [코드](https://github.com/dgtgra…/HumanLearning/blob/master/1001.py)
    * numpy 외에 아무런 라이브러리도 사용하지 않았음
    * Back Propagation 외에 요구되는 배경 지식은 없음
    * python 3.5 환경에서 작성 하였고, numpy만 있으면 실행 됨
  * [실행 동영상](https://www.facebook.com/dgtgrade/videos/1161340170591514/)
    * iteration: 학습 회수
    * train: 학습 데이터
    * test: 학습되지 않은 데이터
    * x: 입력
    * h: 학습된 네트워크의 결과
    * y: 정답 출력
    * cost: squared error
* "손으로 아무렇게나 그린 함수"를 "초간단 ANN으로 근사 시키기"; Universal Approximation Theorem의 내용 직접 실험
  * [Human Learning #1003 : Visual Test of Universal Approximation Theorem]((https://www.youtube.com/watch?v=SahmdQs6X74&list=PLefQdA1SdkhtRUuN_D3PdxaR2XTGQw8Ph&index=9)
  * ANN; 지난 글에서와 마찬가지 초간단 (Deep도 아닌) ANN
    * 히든 레이어 1개, 히든 노드 100개
    * 입력 레이어: 노드 1개 
      * x 그대로임. 즉, feature 로서 다항식이나 비선형 함수 사용하지 않음
    * 히든 레이어: 1개, 노드: 100개
      * a = sigmoid(w1*x + b1)
    * 출력 레이어: 노드 1개
      * o = w2*a + b2
    * 코스트 함수: Squared Error
  * [코드](https://github.com/dgtgra…/HumanLearning/blob/master/1003.py)
    * Artificial Neural Network의 기초, Gradient Descent의 기초, Back Propagation의 기초
    * python에서 매트릭스 다루는 법: numpy
    * python에서 그래프 그리는 법: matplotlib 
    * python에서 이미지 읽는 법: skimage
    * Learning Rate 변경에 따른 학습 능력의 변화, 히든 노드수 변경에 따른 학습 능력의 변화
  * 실행환경
    * python3.5 및 numpy
    * [처음부터 새로 설치 하려면 다음 영상을 참고](https://www.youtube.com/watch?v=pMkwjXFZdH4)
  * 실행방법
    * python 1003.py [이미지파일경로]
    * 이미지 파일 경로에 data/1003_plot1.png 등을 넣어주면 됨
    * [예제 이미지 git 페이지](https://github.com/dgtgrade/HumanLearning/tree/master/data)
* [Universal Approximation Theorem](https://en.wikipedia.org/wi…/Universal_approximation_theorem)
  * 수식 t(x)는 아무 곡선이나 임의로 그려 보기 위해서 사용한 도구일 뿐인 것으로 이해해야 함
  * 즉, 수식 t(x)의 식이 중요한 것은 아님. 수식 t(x)의 내용을 문제 출제자도 모르는 상태에서 아무렇게나 (물론 함수로 표현은 가능하게) 곡선들을 그려넣으면 단순한 ANN으로도 언제나 그 곡선이 표현 가능
  * 즉 그 곡선에 거의 딱 맞는 함수 t(x)를 (사람은 모르고, 아마 만들 낼수도 없어도) 기계가 (단순 함수 f1, f2 등의 조합으로) 만들수 낼수 있다는 것을 실험해 본 것
  * 그렇기 때문에 학습한 범위 밖의 t(x)를 추정 할수 있느냐 없느냐는 여기서는 중요하지 않음
  * 왜냐하면 수식 t(x)는 범위 안의 값을 그리기 위해서 사용한 도구였으므로 사실 t(x)가 아니라 (범위 안의 출력만 일치 한다면) 수식 t2(x) 또는 t3(x)였어도 상관이 없음
  * ANN은 머신러닝을 통해서 수식 t(x)가 아니라 수식 t10(x)를 만들어 낸 것
  * 자율운전의 이상적인 정답 함수 세트 T(x)가 존재 한다면 그 함수들은 사람이 수식으로 쓸수는 없으나(!) (운전 데이터를 통해서) 곡선 그림 t(x)를 그려주면 기계가 머신러닝으로 근사함수 h(x)를 만들어 낼수도 있지 않을까? 하는 것을 보여주는 실험
  * 바둑의 이상적인 정답 함수 세트 T(x) 또한 사람은 그 함수들의 수식을 정리할 능력이 없지만 기초 학습을 위한 그림 t(x)는 (기보 데이터를 통해서) 그려줄수는 있고 알파고는 우선 그 사람이 그려준 그림 t(x)에 근사하는 h(x)를 머신러닝을 통해서 만들어 보는 것으로 시작
  * 그 근사가 어느 정도 완료된 이후엔 자기 h1(x) vs 자기 h2(x) 싸움을 통한 학습에 들어가므로 t(x)는 불필요
  * 실험 실행 영상; 다음 3가지 함수에 대하여 실험한 영상 첨부
    * x^2
    * 8*x^2-X^3
    * 10*sin(X)+(X-4)^2-10
    * 초록색 선: 실제 함수
    * 파란색 점: 학습용 정답 데이터
    * 빨간색 점: 학습 결과 만들어진 근사 함수의 출력 데이터
  * [코드](https://github.com/dgtgra…/HumanLearning/blob/master/1002.py)
  * ANN; 지난 글에서와 마찬가지 초간단 (Deep도 아닌) ANN이고, 히든 노드만 100개로 변경함
    * 입력 레이어: 노드 1개 
      * x 그대로임. 즉, feature 로서 다항식이나 비선형 함수 사용하지 않음
    * 히든 레이어: 1개, 노드: 100개
      * a = sigmoid(w1*x + b1)
    * 출력 레이어: 노드 1개
      * o = w2*a + b2
    * 코스트 함수: Squared Error
  * 실행 환경 준비; python3.5, numpy, matplotlib [설치](https://www.youtube.com/watch?v=pMkwjXFZdH4)
  * 목표 함수 t에 따라서 사람이 조정해야 하는 값
    * Learning Rate: 너무 작게 하면 학습이 느리고, 너무 크게 하면 학습이 안 됨
    * 히든 노드수: 너무 적으면 학습이 불가능할테고, 너무 많으면 학습이 느려짐
  * [머신 러닝이란 무엇일까?](https://www.youtube.com/watch?v=3vcG61VC90c)
* [research.artifacia.com](http://research.artifacia.com/)
* [Source Code Classification Using Deep Learning](http://blog.aylien.com/source-code-classification-using-deep-learning/)
* [Deep Learning Cases: Text and Image Processing](http://www.slideshare.net/grigorysapunov/deep-learning-cases-text-and-image-processing)
* [Introduction to Deep Learning part 1](https://www.youtube.com/watch?v=hoN1mnUBUyI)
* [Introduction to Deep Learning part 2](https://www.youtube.com/watch?v=E71SNUqi2cw)
* [딥러닝의 인공지능 수단으로서의 성격과 방향](http://www.slideshare.net/neuralix/deep-learning-aswaveextractor)
* [CM 세미나](https://www.youtube.com/playlist?list=PLzWH6Ydh35ggVGbBh48TNs635gv2nxkFI)
* [Deep Learning in real world @Deep Learning Tokyo](http://www.slideshare.net/pfi/deep-learning-in-real-world-deep-learning-tokyo)
* [Deep learning tutorials](https://github.com/sjchoi86/dl-workshop)
* [Bay Area DL School Live Stream!](https://tensorflowkorea.wordpress.com/2016/09/24/bay-area-dl-school-live-stream/)
  * [Bay Area Deep Learning School Day 1 at CEMEX auditorium, Stanford](https://www.youtube.com/watch?v=eyovmAtoUx0&feature=youtu.be)
  * [Bay Area Deep Learning School Day 2 at CEMEX auditorium, Stanford](https://www.youtube.com/watch?v=9dXiAecyJrY&feature=youtu.be)
* [Generative Model 101](https://www.facebook.com/SKTBrain/posts/313726382331516) 실제와 유사한 음악이나 이미지를 만들어내는 "Generative Model" 주요 논문 정리
* [Deep Advances in Generative Modeling](https://www.youtube.com/watch?v=KeJINHjyzOU)
* [A tensorflow implementation of Junbo et al's Energy-based generative adversarial network ( EBGAN ) paper](https://github.com/buriburisuri/ebgan)
* [Nuts and Bolts of Applying Deep Learning: Tips and Tricks by Andrew Ng](https://bigdatascientistblog.wordpress.com/2016/09/26/nuts-and-bolts-of-applying-deep-learning-tips-and-tricks-by-andrew-ng/)
* [Deep Learning Frameworks](https://developer.nvidia.com/deep-learning-frameworks) 주요 프레임워크들의 설치를 쉽게 안내하는 엔비디아 페이지
* [Comparison of deep learning software](https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software) 위키피디아의 방대한 딥러닝 프레임워크 비교 표
* [Comparison of deep learning software/Resources](https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software/Resources) 위에서 커버되지 않은 최신 프레임워크들
* [A Look at Popular Machine Learning Frameworks](http://redmonk.com/fryan/2016/06/06/a-look-at-popular-machine-learning-frameworks/) 프레임워크들의 깃허브와 스택오버플로에서의 관심도 차이
* [DEEP LEARNING Your daily dose of Deep learning](http://www.notey.com/blogs/deep-learning) 딥러닝에 대한 기사
* [The Next Wave of Deep Learning Architectures](http://www.nextplatform.com/2016/09/07/next-wave-deep-learning-architectures/) 이후 딥러닝 HW에 대한 전망 (2016년 3Q 기준)
* [Reward Augmented Maximum Likelihood for Neural Structured Prediction](http://static.googleusercontent.com/…/pubs/archive/45580.pdf)
  * reinforcement learning에서의 아이디어를 가져와 maximum likelihood objective를 확장해 training data로부터 추가적인 데이터를 샘플링
  * 결과적으로 알고리즘은 간단한 데이터 전처리에 불과한, Speech recognition과 neural machine translation 모두에 있어서 상당한 성능의 향상
  * reinforcement learning과 supervised learning의 아이디어가 결합. structured prediction에서 전통적인 기계학습의 아이디어와 신경망이 결합해 좋은 결과를 가져옴
* [Deep Learning Reading Group: SqueezeNet](http://www.kdnuggets.com/2016/09/deep-learning-reading-group-squeezenet.html)
* [youtube.com/user/dvbuntu/featured Self-drinving car Transfer Learning Model](https://www.youtube.com/user/dvbuntu/featured)
* [Uncertainty in Deep Learning (PhD Thesis)](http://mlg.eng.cam.ac.uk/yarin/blog_2248.html)
* [Tensor Physics for Deep Learning](http://www.slideshare.net/uspace/tensor-physics-for-deep-learning)
* [Deep Visualization Toolbox](https://www.youtube.com/watch?v=AgkfIQ4IGaM)
* [DEVIEW 2016](https://deview.kr/2016/schedule) 딥러닝/머신러닝 관련 슬라이드
  * [통역하는 앵무새 파파고 이야기](http://www.slideshare.net/deview/134papago)
  * [딥러닝을 이용한 지역 컨텍스트 검색](http://www.slideshare.net/deview/221-67605830)
  * [딥러닝을 활용한 이미지 검색: 포토요약과 타임라인](http://www.slideshare.net/deview/222-20161024)
  * [딥러닝과 강화 학습으로 나보다 잘하는 쿠키런 AI 구현하기](http://www.slideshare.net/deview/ai-67608549)
    * [[리뷰] DEVIEW : 쿠키런 AI 구현하기](https://mingrammer.com/review-deview-cookierun-ai)
    * [발표 자료](http://www.slideshare.net/carpedm20/ai-67616630)
    * [데모 영상](https://www.youtube.com/watch?v=exXD6wJLJ6s)
    * [첫번째 시도 (Deep Q-Network)](https://youtu.be/XsJWbAd6rYk)
    * [두번째 시도 (+ Double Q-learning)](https://youtu.be/rurJICmHfHQ)
    * [세번째 시도 (+ Dueling Network)](https://youtu.be/XQJA1Rob0ng)
    * [Deep Q-learning](https://github.com/devsisters/DQN-tensorflow/)
    * [Dueling Newtork, Double Q-learning](https://github.com/carpedm20/deep-rl-tensorflow/)
    * [쿠키런과 같은 discrete action space가 아닌 continuous action space에서의 강화 학습 방법](https://github.com/carpedm20/NAF-tensorflow/)
  * [Backend 개발자의 Neural Machine Translation 개발기](http://www.slideshare.net/deview/224-backend-neural-machine-translation-67608580)
  * [YARN 기반의 Deep Learning Application Cluster 구축](http://www.slideshare.net/deview/225yarn-deep-learning-application-cluster)
  * [Multimodal Residual Learning for Visual Question-Answering](http://www.slideshare.net/deview/multimodal-residual-learning-for-visual-questionanswering)
  * [딥러닝 예제로 보는 개발자를 위한 통계](http://www.slideshare.net/deview/216-67609104)
  * [Deep Recurrent Neural Network를 이용한 대용량 텍스트 마이닝 기술 및 실제 응용사례](http://www.slideshare.net/deview/226-67609105)
  * [빅데이터 분석에 적합한 LDA & HDP 베이지안 토픽모형에 대한 알고리즘](http://www.slideshare.net/deview/214-67608573)

# AlphaGo
* [Rochester-NRT/AlphaGo](https://github.com/Rochester-NRT/AlphaGo)
* [AlphaGo의 인공지능 알고리즘 분석](http://spri.kr/post/14725)
* [AlphaGo 알고리즘 요약](http://www.slideshare.net/zenithon/alphago?from_m_app=android)
* [알파고 (바둑 인공지능)의 작동 원리](http://www.slideshare.net/ShaneSeungwhanMoon/ss-59226902)
* [이세돌과 대국으로 ‘알파고’ 설계자가 꿈꾸는 것은?](http://www.bloter.net/archives/251528)
* [모두의 알파고](http://www.slideshare.net/DonghunLee20/ss-59338971)
* [Mastering the game of Go with deep neural networks and tree search](http://www.willamette.edu/~levenick/cs448/goNature.pdf)
* [AlphaGo에 적용된 딥러닝 알고리즘 분석](https://brunch.co.kr/@justinleeanac/2)
* [알파고는 어떻게 바둑을 둘까](https://brunch.co.kr/@madlymissyou/9)
* [이세돌이 알파고를 이기려면 선입견을 버려야 한다](http://m.blog.daum.net/_blog/_m/articleView.do?blogid=0NovT&articleno=3331)
* [바둑인을 위한 알파고](http://www.slideshare.net/DonghunLee20/ss-59413793)
* [알파고 해부하기 1부](http://www.slideshare.net/DonghunLee20/1-59501887)
* [알파고 해부하기 2부](http://www.slideshare.net/DonghunLee20/2-59620244)
* [알파고 해부하기 3부](http://www.slideshare.net/DonghunLee20/3-61454159)
* [알파고, 강화학습을 현실에 데뷔시키다](http://t-robotics.blogspot.co.id/2016/03/blog-post_26.html)
* [알파고는 어떤 컴퓨터를 썼을까?](http://www.slideshare.net/jysoo/ss-61950212)
* [AlphaGo 대국 - 한국어](https://deepmind.com/research/alphago/alphago-games-korean/)

# Amazon
* [Amazon DSSTNE: Deep Scalable Sparse Tensor Network Engine](https://github.com/amznlabs/amazon-dsstne)

# Baidu
* [Silicon Valley AI Lab](https://svail.github.io/)

# Book
* [[eBook] 머신러닝에서 딥러닝까지](http://digital.kyobobook.co.kr/digital/ebook/ebookDetail.ink?selectedLargeCategory=001&barcode=480150001023P&orderClick=LAN&Kc)
* [C++와 CUDA C로 구현하는 딥러닝 알고리즘 Vol.1 [Restricted Boltzman Machine의 이해와 Deep Belief Nets 구현]](http://www.acornpub.co.kr/book/dbn-cuda-vol1)
* [Deep Learning 이론과 실습 (개정중)](https://wikidocs.net/book/498)
* [Deep Learning - A Practitioner's Approach](http://shop.oreilly.com/product/0636920035343.do)
* [Fundamentals of Deep Learning](http://shop.oreilly.com/product/0636920039709.do)
  * [‘Fundamental of Deep Learning’ Preview](https://tensorflowkorea.wordpress.com/2016/04/18/fundamental-of-deep-learning-preview/#more-2018)
* [Deep Learning - An MIT Press book in preparation](http://www.deeplearningbook.org/)
  * [DeepLearningBook](https://github.com/HFTrader/DeepLearningBook)
* [Book: Deep Learning With Python](http://www.datasciencecentral.com/forum/topics/book-deep-learning-with-python) Theano and TensorFlow using Keras
* [Deep Learning With Python](https://machinelearningmastery.com/deep-learning-with-python/)
* [Fundamental of Reinforcement Learning](https://dnddnjs.gitbooks.io/rl/content/)

# Deep Q Learning DQL
* [Deep Q-Learning (Space Invaders)](http://maciejjaskowski.github.io/2016/03/09/space-invaders.html)
* [Using Deep Q-Network to Learn How To Play Flappy Bird](https://github.com/DeepLearningProjects/DeepLearningFlappyBird)
* [Hello DeepQ](http://koaning.io/hello-deepq.html)

# Extreme Learning Machines
* [Extreme Learning Machines](http://www.ntu.edu.sg/home/egbhuang/pdf/IEEE-IS-ELM.pdf)
* [Basic ELM Algorithms](http://www.ntu.edu.sg/home/egbhuang/elm_codes.html)

# Neural Network
* [Google's AI Chief Geoffrey Hinton - How Neural Networks Really Work](https://www.youtube.com/watch?v=l2dVjADTEDU&feature=player_embedded)
* [1. Overview of Mini Batch Gradient Descent](https://www.youtube.com/watch?v=GvHmwBc9N30&feature=share)
* [Learning How To Code Neural Networks](https://medium.com/learning-new-stuff/how-to-learn-neural-networks-758b78f2736e)
  * [뉴럴네트워크 코드 짜는 법 배우기](http://ddanggle.github.io/ml/ai/cs/2016/07/16/LearningHowToCodeNeuralNetworks.html)
* [Machine Learning 스터디 (18) Neural Network Introduction](http://sanghyukchun.github.io/74/)
* [Artificial Neural Networks for Beginners](http://blogs.mathworks.com/loren/2015/08/04/artificial-neural-networks-for-beginners/)
* [A Step by Step Backpropagation Example](http://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)
* [A Visual Explanation of the Back Propagation Algorithm for Neural Networks](http://www.kdnuggets.com/2016/06/visual-explanation-backpropagation-algorithm-neural-networks.html)
* [Machine Learning - Neural Networks Tutorial](http://www.existor.com/en/news-neural-networks.html)
* [A Fast and Accurate Dependency Parser using Neural Networks](http://cs.stanford.edu/~danqi/papers/emnlp2014.pdf)
* [waifu2x - Image Super-Resolution for Anime/Fan-Art](https://github.com/nagadomi/waifu2x)
* [Visualizing and Understanding Deep Neural Networks by Matt Zeiler](https://www.youtube.com/watch?v=ghEmQSxT6tw)
* [Machine-Learning Algorithm Mines Rap Lyrics, Then Writes Its Own](http://www.technologyreview.com/view/537716/machine-learning-algorithm-mines-rap-lyrics-then-writes-its-own/)
* [시인 뉴럴](http://pail.unist.ac.kr/carpedm20/poet/)
* [VGG Convolutional Neural Networks Practical](http://www.robots.ox.ac.uk/~vgg/practicals/cnn/index.html)
* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
* [Spectral Representations for Convolutional Neural Networks](http://arxiv.org/pdf/1506.03767.pdf)
* [How to implement a neural network: Part 1](http://peterroelants.github.io/posts/neural_network_implementation_part01/)
* [Inceptionism: Going Deeper into Neural Networks](http://googleresearch.blogspot.kr/2015/06/inceptionism-going-deeper-into-neural.html)
* [Quantifying Creativity in Art Networks](http://arxiv.org/pdf/1506.00711v1.pdf)
* [Neural network의 변천사 이태영](https://www.slideshare.net/secret/dzVcikxOkWg8TP)
* [ai junkie - neural networks in plain english](http://www.ai-junkie.com/ann/evolved/nnt1.html)
* [10 Billion Parameter Neural Networks in your Basement](http://on-demand.gputechconf.com/gtc/2014/presentations/S4694-10-billion-parameter-neural-networks.pdf)
* [Understanding Neural Networks Through Deep Visualization](http://yosinski.com/deepvis)
  * ["Understanding Neural Networks Through Deep Visualization" (2015), J. Yosinski et al.](http://yosinski.com/media/papers/Yosinski__2015__ICML_DL__Understanding_Neural_Networks_Through_Deep_Visualization__.pdf)
  * [github.com/yosinski/deep-visualization-toolbox](https://github.com/yosinski/deep-visualization-toolbox)
* [Interactive Deep Neural Net Hallucinations (+source code) Large Scale Deep Neural Net visualizing top level features](https://317070.github.io/Dream/)
* [Neural Network for Concrete Strength using R](http://andersonjo.github.io/neural-network/2015/07/25/Neural-Network-for-concrete/)
* [fann.js - FANN compiled through Emscripten](https://github.com/louisstow/fann.js/)
* [neurogram - Creating abstract art by evolving neural networks in Javascript](http://blog.otoro.net/2015/07/31/neurogram/)
* [Neural Network for Concrete Strength using R](http://andersonjo.github.io/neural-network/2015/07/25/Neural-Network-for-concrete/)
* [Recent Trends in Neural Net Policy Learning](http://www.slideshare.net/samchoi7/recent-trends-in-neural-net-policy-learning)
* [Hardware Guide: Neural Networks on GPUs](http://pjreddie.com/darknet/hardware-guide/)
* [neural networks by browser](http://neurovis.dataphoric.com/)
* [Scalable Bayesian Optimization Using Deep Neural Networks](http://arxiv.org/abs/1502.05700)
* **[An implementation of the paper 'A Neural Algorithm of Artistic Style'](https://github.com/kaishengtai/neuralart)**
  * [거장의 그림을 30초만에 만들다: DeepStyle](http://redtea.kr/?b=3&n=951)
* [neural-style - Torch implementation of neural style algorithm](https://github.com/jcjohnson/neural-style)
* [Comparing Artificial Artists](https://medium.com/@kcimc/comparing-artificial-artists-7d889428fce4)
* [Neural Networks, Types, and Functional Programming](http://colah.github.io/posts/2015-09-NN-Types-FP/)
* **[Implementing a Neural Network from Scratch – An Introduction](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/)**
* [Hacker's guide to Neural Networks](http://karpathy.github.io/neuralnets/)
  * **[해커가 알려주는 뉴럴 네트워크](https://tensorflowkorea.wordpress.com/2016/09/13/%ED%95%B4%EC%BB%A4%EA%B0%80-%EC%95%8C%EB%A0%A4%EC%A3%BC%EB%8A%94-%EB%89%B4%EB%9F%B4-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC/)**
* [neural-network-papers](https://github.com/robertsdionne/neural-network-papers)
* [Pedestrian detection using convolutional neural networks](http://www.diva-portal.org/smash/get/diva2:839692/FULLTEXT01.pdf)
* [Scalable Distributed DNN Training Using Commodity GPU Cloud Computing](https://drive.google.com/file/d/0B6dKRGPLFSd0UGNOYkNaSC1UZTA/view)
* [Deep Style: Inferring the Unknown to Predict the Future of Fashion](http://multithreaded.stitchfix.com/blog/2015/09/17/deep-style/)
* [DeepHear - Composing and harmonizing music with neural networks](http://web.mit.edu/felixsun/www/neural-music.html)
* [Why are Eight Bits Enough for Deep Neural Networks?](http://petewarden.com/2015/05/23/why-are-eight-bits-enough-for-deep-neural-networks/)
* [Neural Net in C++ Tutorial](https://vimeo.com/19569529)
* [A too naive approach to video compression using artificial neural networks](https://github.com/Dobiasd/articles/blob/master/a_too_naive_approach_to_video_compression_using_artificial_neural_networks.md)
* [An interactive introduction to neural network](http://neurovis.mitchcrowe.com/)
* [Understanding Natural Language with Deep Neural Networks Using Torch](http://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch)
* [Skynet for Beginners - Using a Neural Network to Train a Ruby Twitter bot](http://www.fullstackfest.com/agenda/skynet-for-beginners-using-a-neural-network-to-train-a-ruby-twitter-bot)
* [Neural Networks, Manifolds, and Topology](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)
* [Deep Neural Network with Pre-training](http://enginius.tistory.com/607)
* [Build your own neural network classifier in R](http://junma5.weebly.com/data-blog/build-your-own-neural-network-classifier-in-r)
* [GNU Gneural Network](https://www.gnu.org/software/gneuralnetwork/)
* [Neural Networks Demystified](http://lumiverse.io/series/neural-networks-demystified)
* [colornet](https://techstory.shma.so/colornet-c10ec398cd45)
* [Sketch-simplifying neural network lets artists leap from pencil to ink](http://boingboing.net/2016/04/28/sketch-simplifying-neural-netw.html)
* [Neural Networks Are Impressively Good At Compression](https://probablydance.com/2016/04/30/neural-networks-are-impressively-good-at-compression/)
* [An Analysis of Deep Neural Network Models for Practical Applications](https://arxiv.org/pdf/1605.07678v1.pdf)
* [Why are Eight Bits Enough for Deep Neural Networks?](https://petewarden.com/2015/05/23/why-are-eight-bits-enough-for-deep-neural-networks/)
* [Adventures learning Neural Nets and Python](http://katbailey.github.io/post/neural-nets-in-python/)
* [Deep Learning in Neural Networks: An Overview](http://arxiv.org/abs/1404.7828)
* [Using Neural Networks With Regression](http://deeplearning4j.org/linear-regression.html)
* [Neural networks for algorithmic trading. Part One — Simple time series forecasting](https://medium.com/@alexrachnog/neural-networks-for-algorithmic-trading-part-one-simple-time-series-forecasting-f992daa1045a)
* [Deep Learning using Deep Neural Networks](https://www.linkedin.com/pulse/deep-learning-using-neural-networks-niraj-kumar)
* [K-Fold Cross-Validation for Neural Networks](https://jamesmccaffrey.wordpress.com/2013/10/25/k-fold-cross-validation-for-neural-networks/)
* [What is the Role of the Activation Function in a Neural Network?](http://www.kdnuggets.com/2016/08/role-activation-function-neural-network.html)
* [Neural Network-based Sketch Simplification](http://hi.cs.waseda.ac.jp:8081/)
* [논문 요약 - Deep Neural Networks for YouTube Recommendations](http://keunwoochoi.blogspot.com/2016/09/deep-neural-networks-for-youtube.html)
* [Neural Network Architectures](https://culurciello.github.io/tech/2016/06/04/nets.html)
* [THE NEURAL NETWORK ZOO](http://www.asimovinstitute.org/neural-network-zoo/)
  * [번역](https://www.facebook.com/SKTBrain/photos/pcb.306040569766764/306035899767231/?type=3&theater)
* [10 misconceptions about Neural Networks](http://www.turingfinance.com/misconceptions-about-neural-networks/)
* [딥러닝_Neural Network_멀티 퍼셉트론1](http://m.blog.naver.com/dunopiorg/220180453865)
* [인공지능(뉴럴 네트워크) 베토벤 월광소나타 훈련시키기](http://blog.naver.com/atelierjpro/220851418829)

## ConvNets
* [컨볼루셔널 뉴럴넷 (Convolutional Neural Network)](http://t-robotics.blogspot.com/2016/05/convolutional-neural-network_31.html)
  * [ConvNet을 시계열 데이터에 적용하는 세가지 방법](https://www.facebook.com/terryum/posts/10154337242359417)
    * Convolutional Neural Network (ConvNet, 또는 CNN)은 원래는 2D 이미지를 인식하기 위해 만듦
      * 뛰어난 성능에 다른 영역에서도 점점 CNN을 적용
      * CNN은 기본적으로 shared parameter를 통해 계산량을 줄이는 동시에 overfitting도 완화해주고 더욱 유용한 피쳐를 생성해주는 등 classification에 좋음
    * 이것을 시계열 데이터(time-series data)에 적용하려면 기본적으로 각각의 데이터마다 길이가 다른 문제를 해결해야 함
      * 예를 들어 음성인식을 한다고 하면 각각 단어마다 길이가 다른데, 뉴럴넷은 기본적으로 고정된 사이즈의 벡터를 인풋으로 받는다는 것이 문제
    * 가장 간단한 해결책은 아마 fixed size window를 슬라이딩하면서 적용하는 것
      * 예를 들어 길이가 하나는 1000이고, 하나는 1200이라면 사이즈 100짜리 윈도우로 각각 10개, 12개의 벡터들을 뽑고 각각을 독립된 예제들로 간주
    * 하지만 이건 그렇게 좋은 방법은 아님
      * 왜냐하면 어떤건 앞쪽 부분을 보고, 어떤건 가운데를 보고, 어떤건 뒤쪽을 보는데 이들을 모두 같은 데이터로 학습해야하기 때문
      * 물론 이 데이터 위에 RNN과 같은 것을 쌓을 수도 있겠지만, 암튼 이건 좀 bruteforce
    * 음성인식에선 이것을 HMM을 통해 해결
      * 딥러닝이 나오기 이전, 음성인식은 보통 HMM-GMM (Hidden Markov Model - Gaussian Mixture Model)을 이용해 해결
      * 아주 간단히 말해 연속된 데이터를 몇 개의 Gaussian의 states로 모델링하고 이를 학습
      * 최근의 딥러닝의 도입은 GMM을 딥러닝으로 대체함으로서 GMM-DNN모델을 제시
    * CNN을 음성인식에 적용하는 기본적인 방법은 먼저 HMM-GMM을 통해 대략의 states를 학습한 후 GMM을 CNN으로 대체해 다시 학습
      * 이렇게 하면 기존엔 아주 많은 윈도우를 각각 학습했어야 하는 것과 달리, 이제는 적절한 크기의 states들만 학습하면 됨
    * 자연어처리에선 max pooling over time을 통해 이 문제를 해결
      * 예를 들어 "나는 오늘 아침에 학교에 갔어요"란 문장을 배운다면 Convolution window를 (나는, 오늘), (나는, 오늘, 아침에), (오늘, 아침에) 등등에 적용한 이후 각각의
윈도우로부터 딱 한 개의 값들만을 max pool
      * 이렇게 하면 만약 feature map의 갯수만 같다면 원래 문장의 길이와는 상관없이 동일한 길이의 벡터가 추출
        *  각각의 피쳐맵에서 딱 한 개씩만 값들을 추출하기 때문
      * 이걸 마지막에 기본 뉴럴넷(FFNN)에 넣음으로서 문장 분류와 같은 일을 함
    * 음성인식과 자연어처리가 다른 점
      * 자연어처리(문장 분류)는 이미 문장 단위로 segment 되어있는 상태에서 다른 길이들을 처리
      * 음성인식은 연속적인 데이터에서 임의로 states를 나누는 경우라는 점
    * [Convolutional neural networks for speech recognition (2014)](http://research-srv.microsoft.com/…/…/TASLP2339736-proof.pdf)
    * [Convolutional neural networks for sentence classification (2014)](http://arxiv.org/pdf/1408.5882)
    * [[1509.01626] Character-level Convolutional Networks for Text Classification](http://arxiv.org/abs/1509.01626) 자연어를 word 단위로 보는 것이 아니라 character 단위로 보고 마치 한글자 한글자를 웨이브의 한 점처럼 생각
    * Max over time pooling같은 경우 대부분의 sentence classification류의 문제에서 '실용적으로' 잘 동작
      * 굳이 한계점을 꼽자면 feature가 (예로 들어주신 것 처럼, '나는 오늘'과 같은 단어들을 검출할거라고 예상되는) 문장 내에서 나왔는지/없었는지만을 볼 수 있고, 몇 번 나왔는지는 알 수 없다는 단점
      * 따라서, 긴 문장, 혹은 대화/문서까지를 다룬다고 하면 feature extractor로써 적절하지 않을 것
      * 이를 조금 보완한 것이 [dynamic k-max pooling](http://www.aclweb.org/anthology/P14-1062)
    * 시계열을 다룰때는 (음성인식이나, 자연어처리나) RNN이 더 적합하다고 생각
      * 물론 task가 단순하고, 데이터가 적다면 CNN이나 심지어는 전통적인 TF-IDF방법이 더 좋은 경우도 있음
* [My 1st Kaggle ConvNet: Getting to 3rd Percentile in 3 months](http://ilyakava.tumblr.com/post/125230881527/my-1st-kaggle-convnet-getting-to-3rd-percentile)
* [Image Scaling using Deep Convolutional Neural Networks](http://engineering.flipboard.com/2015/05/scaling-convnets/)
* [ConvnetJS demo: Image "Painting"](http://cs.stanford.edu/people/karpathy/convnetjs/demo/image_regression.html)
* [Fast Convolutional Nets With fbfft: A GPU Performance Evaluation](https://research.facebook.com/publications/695244360582147/fast-convolutional-nets-with-fbfft-a-gpu-performance-evaluation/)
* [Learning Game of Life with a Convolutional Neural Network](http://danielrapp.github.io/cnn-gol/)
* [A Tutorial on Deep Learning Part 2: Autoencoders, Convolutional Neural Networks and Recurrent Neural Networks](http://www-cs.stanford.edu/~quocle/tutorial2.pdf)
* [Texture Synthesis with Convolutional Neural Networks](http://bethgelab.org/deeptextures/)
* [Understanding Convolutional Neural Networks for NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)
* [Convolutional Neural Network (CNN)](http://enginius.tistory.com/608)
* [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/syllabus.html)
  * [Convolutional Neural Networks (CNNs / ConvNets)](http://cs231n.github.io/convolutional-networks/)
  * [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/)
  * [archive.org/download/cs231n-CNNs](https://archive.org/download/cs231n-CNNs)
  * [Andrej Karpathy](https://www.youtube.com/channel/UCPk8m_r6fkUSYmvgCBwq-sw)
  * [CS231n : Neural Networks Part 1: Setting up the Architecture (한국어 번역)](http://ishuca.tistory.com/381)
  * [CS231n Winter 2016 Lecture 4 Backpropagation, Neural Networks 1-Q_UWHTY_TEQ.mp4](https://www.youtube.com/watch?v=GZTvxoSHZIo&feature=youtu.be&t=1h11m38s)
  * [Visualizing what ConvNets learn](http://cs231n.github.io/understanding-cnn/)
  * [CS231n/Module 1: Neural Networks](http://ishuca.tistory.com/category/CS231n/Module%201%3A%20Neural%20Networks)
* [Implementing a CNN for Text Classification in TensorFlow](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/)
* [Case Study of Convolutional Neural Network](http://www.slideshare.net/nmhkahn/case-study-of-convolutional-neural-network-61556303)
* [Denoising auto encoders(d a)](http://www.slideshare.net/taeyounglee1447/denoising-auto-encodersd-a)
* [Must Know Tips/Tricks in Deep Neural Networks (by Xiu-Shen Wei)](http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html)
* [Kashif Rasul - Intro to ConvNets](https://www.youtube.com/watch?v=W9_SNGymRwo)
* [ConvNetJS CIFAR-10 demo](http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html)
* [CNN(Convolution Neural Network)으로 인물을 인식 시켜보자...](https://github.com/jaeho-kang/deep-learning/blob/master/blog/post1/contents.md)
* [VGG Convolutional Neural Networks Practical](http://www.robots.ox.ac.uk/~vgg/practicals/cnn/)
* [Q Learning과 CNN을 이용한 Object Localization](http://www.slideshare.net/ssuser06e0c5/q-learning-cnn-object-localization)
* [Benchmarks for popular CNN models](https://github.com/jcjohnson/cnn-benchmarks)
* [A Beginner's Guide To Understanding Convolutional Neural Networks](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/)
  * [[번역] A Beginner's Guide To Understanding Convolutional Neural Networks](http://steady7.tistory.com/m/7)
* [A Beginner's Guide To Understanding Convolutional Neural Networks Part 2](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/)
  * [[번역] A Beginner's Guide To Understanding Convolutional Neural Networks Part 2](http://steady7.tistory.com/8)
* [The 9 Deep Learning Papers You Need To Know About (Understanding CNNs Part 3)](https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)
* [Faster R-CNN in MXNet with distributed implementation and data parallelization](https://github.com/dmlc/mxnet/tree/master/example/rcnn)
* [A guide to convolution arithmetic for deep learning](https://tensorflowkorea.wordpress.com/a-guide-to-convolution-arithmetic-for-deep-learning/)
* [An Intuitive Explanation of Convolutional Neural Networks](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)
* [‘구글 맵’ 영상에 AI 접목하니, 빈곤국가 경제실태 한눈에](http://www.dongascience.com/news/view/13461)
* [Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks](https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721)
* [Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78)
* [NeoCognitron](https://youtu.be/Qil4kmvm2Sw)
  * [1980년 논문: Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position](http://www.cs.princeton.edu/…/co…/Readings/Fukushima1980.pdf)
  * [홈페이지](http://personalpage.flsi.or.jp/fukushima/)
  * [Scholarpedia](http://www.scholarpedia.org/article/Neocognitron)
* [GRAPH CONVOLUTIONAL NETWORKS](http://tkipf.github.io/graph-convolutional-networks/)

## LSTM
* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
* [엘에스티엠 네트워크 이해하기 (Understanding LSTM Networks)](http://roboticist.tistory.com/m/post/571)
* [GRU & LSTM for machine translations.ipynb](https://github.com/kobikun/study/blob/master/babelpish/GRU_LSTM_for_machine_translation/GRU%20%26%20LSTM%20for%20machine%20translations.ipynb)
* [Backpropogating an LSTM: A Numerical Example](http://blog.aidangomez.ca/2016/04/17/Backpropogating-an-LSTM-A-Numerical-Example/)
* [LSTM Networks for Sentiment Analysis](http://deeplearning.net/tutorial/lstm.html)
* [[번역] 니코니코동화의 공개코멘트 데이터를 Deep Learning로 해석하기](https://blog.umay.be/2016/06/02/niconico-nlp.html)
  * [わかるLSTM ～ 最近の動向と共に](http://qiita.com/t_Signull/items/21b82be280b46f467d1b)
* [Unsupervised Learning of Video Representations using LSTMs](http://nbviewer.jupyter.org/github/babelpish/deep-elastic/blob/master/part2/paper/unsupervised_lstms_video/Unsupervised_Learning_of_Video_Representations_using_LSTMs.ipynb)
* [LSTMVis - Visual Analysis for Recurrent Neural Networks](http://lstm.seas.harvard.edu/)

## Python
* [A Neural Network in 11 lines of Python (Part 1)](http://iamtrask.github.io/2015/07/12/basic-python-network/)
  * [11줄의 파이썬 코드로 뉴럴 네트워크를 만들어보자](http://ddanggle.github.io/ml/ai/cs/2016/07/16/11lines.html)
* [A Neural Network in 13 lines of Python (Part 2 - Gradient Descent)](http://iamtrask.github.io/2015/07/27/python-network-part2/)
  * [13줄의 파이썬 코드로 뉴럴 네트워크를 만들어보자. (파트2 - 경사하강법)](http://ddanggle.github.io/ml/ai/cs/2016/09/03/13lines.html)
  * [13Lines.ipynb](https://github.com/DDanggle/blogNetwork/blob/master/13Lines.ipynb)
* [Hinton's Dropout in 3 Lines of Python](http://iamtrask.github.io/2015/07/28/dropout/)
* [NeuPy - Neural Networks in Python](http://neupy.com/)
* [Neural Doodle - Use a deep neural network to borrow the skills of real artists and turn your two-bit doodles into masterpieces](https://github.com/alexjc/neural-doodle)
  * [Feed-forward neural doodle](http://dmitryulyanov.github.io/feed-forward-neural-doodle/)
  * [Online neural doodle](https://likemo.net/)
* [Training (deep) Neural Networks Part: 1](http://upul.github.io/2015/10/12/Training-(deep)-Neural-Networks-Part:-1/)
* [Deep learning – Convolutional neural networks and feature extraction with Python](http://blog.christianperone.com/2015/08/convolutional-neural-networks-and-feature-extraction-with-python/)
* [Irene Chen A Beginner's Guide to Deep Learning PyCon 2016](https://www.youtube.com/watch?v=nCPf8zDJ0d0)
* [Introduction to Deep Learning with Python](https://www.youtube.com/watch?v=S75EdAcXHKk&feature=share)
* [A Complete Guide on Getting Started with Deep Learning in Python](https://www.analyticsvidhya.com/blog/2016/08/deep-learning-path/)
* [Python Code Suggestions Using a Long Short-Term Memory RNN](http://blog.algorithmia.com/python-code-suggestions-lstm-rnn)
* [Python for Image Understanding: Deep Learning with Convolutional Neural Nets](http://www.slideshare.net/roelofp/python-for-image-understanding-deep-learning-with-convolutional-neural-nets)

## Recurrent Flow Net
* [Recurrent Flow Network on Different Error Rates](https://www.youtube.com/watch?v=twR3wYjwLrM)
  * binary matrix를 입력으로 받아서, 미래의 matrix를 예측하며, 각 cell의 속도 역시 구할 수 있음
  * 예를 들어서 matrx의 1과 0이 해당 공간이 점유됨과 비어있음을 의미하면, 이 네트워크는 무인 자동차와 같은 어플리케이션에서 장애물들의 속도와 미래에 어떻게 움직일지를 예측 가능
  * 구조적으론 재귀 신경망 구조이지만, 기존의 RNN과는 모든 연산과 구조가 다릅니다.
  * 장점
    * 노이즈에 강인. 동영상에서 알 수 있듯이 왼쪽의 입력 matrix에 노이즈가 많이 있어도 오른쪽의 예측된 방향은 꽤나 정확
    * 빠른 속도. 200 * 200의 행렬을 받아 처리하는데 40ms 이하(MATLAB coder 환경)
  * [매트랩 코드](https://github.com/sjchoi86/RecurrentFlowNet)

## Recurrent Neural Net
* [Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN) Baby steps to your neural network's first memories.](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)
* **[Awesome Recurrent Neural Networks - A curated list of resources dedicated to recurrent neural networks](https://github.com/kjw0612/awesome-rnn)**
* **[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)**
* [opinion mining with deep recurrent nets](http://www.cs.cornell.edu/~oirsoy/drnt.htm)
* [Composing Music With Recurrent Neural Networks](http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/)
* [Composing Music with LSTM Recurrent Networks - Blues Improvisation](http://people.idsia.ch/~juergen/blues/)
* [Training a Recurrent Neural Network to Compose Music](https://maraoz.com/2016/02/02/abc-rnn/)
* [A Recurrent Neural Network Music Generation Tutorial](https://magenta.tensorflow.org/2016/06/10/recurrent-neural-network-generation-tutorial/)
* [Recurrent Neural Network, Fractal for Deep Learning](http://www.slideshare.net/uspace/recurrent-neural-network-fractal-for-deep-learning)
* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
* [Multi-layer Recurrent Neural Networks (LSTM, GRU, RNN) for character-level language models in Torch](https://github.com/karpathy/char-rnn)
* [mrchrisjohnson Recurrent Neural Shady](https://soundcloud.com/mrchrisjohnson/recurrent-neural-shady)
* [Recurrent neural network (depth=3) generates next 1,000 bytes of "Let It Go":](http://elnn.snucse.org/sandbox/music-rnn/)
* [recurrent neural network handwriting generation demo](http://www.cs.toronto.edu/~graves/handwriting.cgi?text=Recurrent+neural+nets+are+fucking+magical.&style=&bias=0.5&samples=3)
* [Teaching recurrent Neural Networks about Monet](http://blog.manugarri.com/teaching-recurrent-neural-networks-about-monet/)
* [Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)
  * [Recurrent Neural Network (RNN) Tutorial - Part 1](http://aikorea.org/blog/rnn-tutorial-1/)
* [Recurrent Neural Networks Tutorial, Part 2 – Implementing a RNN with Python, Numpy and Theano](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/)
  * [RNN Tutorial Part 2 - Python, NumPy와 Theano로 RNN 구현하기](http://aikorea.org/blog/rnn-tutorial-2/)
* [Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)
  * [RNN Tutorial Part 3 - BPTT와 Vanishing Gradient 문제](http://aikorea.org/blog/rnn-tutorial-3/)
* [Recurrent Neural Network Tutorial, Part 4 – Implementing a GRU/LSTM RNN with Python and Theano](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/)
  * [RNN Tutorial Part 4 - GRU/LSTM RNN 구조를 Python과 Theano를 이용하여 구현하기](http://aikorea.org/blog/rnn-tutorial-4/)
* [Auto-Generating Clickbait With Recurrent Neural Networks](http://larseidnes.com/2015/10/13/auto-generating-clickbait-with-recurrent-neural-networks/)
* [Modeling Molecules with Recurrent Neural Networks](http://csvoss.github.io/projects/2015/10/08/rnns-and-chemistry.html)
* [Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN) Baby steps to your neural network's first memories](http://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)
* [char_rnn_ari 한글 Character RNN 구현](https://github.com/bluedisk/char_rnn_ari)
* [Introduction to Recurrent Networks in TensorFlow](http://www.kdnuggets.com/2016/05/intro-recurrent-networks-tensorflow.html)
* [Recurrent Flow Network for Occupancy Flow](https://github.com/sjchoi86/RecurrentFlowNet)
* [Large-Scale Item Categorization in e-Commerce Using Multiple Recurrent Neural Networks](http://www.kdd.org/kdd2016/subtopic/view/large-scale-item-categorization-in-e-commerce-using-multiple-recurrent-neur/)
* [RNNS IN TENSORFLOW, A PRACTICAL GUIDE AND UNDOCUMENTED FEATURES](http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/)
  * [RNNs in Tensorflow, A Practical Guide and Undocumented Features](https://tgjeon.github.io/post/rnns-in-tensorflow/)
* [Recurrent Neural Network tutorial (2nd)](http://www.slideshare.net/uspace/recurrent-neural-network-tutorial-2nd)
* [Attention and Augmented Recurrent Neural Networks](http://distill.pub/2016/augmented-rnns/)
* [Fast Weights RNN](https://tensorflowkorea.wordpress.com/2016/10/25/fast-weights-rnn/)

# Library
* [프로그래밍 언어별 딥러닝 라이브러리 정리](http://aikorea.org/blog/dl-libraries/)
* [어떤 Deep Learning Library를 선택해야하나요?](http://tmmse.xyz/choosing-deep-learning-libraries/)
* [15 Deep Learning Libraries](http://www.datasciencecentral.com/profiles/blogs/here-are-15-libraries-in-various-languages-to-help-implement-your)
* [15 Deep Learning Tutorials](http://www.datasciencecentral.com/profiles/blogs/15-deep-learning-tutorials)
* [50 Deep Learning Software Tools and Platforms, Updated](http://www.kdnuggets.com/2015/12/deep-learning-tools.html)
* [Computational Network Toolkit (CNTK)](https://cntk.codeplex.com)
* Caffe
  * [윈도우에서 Caffe 이용하기](https://github.com/jaeho-kang/deep-learning/blob/master/%EC%9C%88%EB%8F%84%EC%9A%B0%EC%97%90%EC%84%9C%20caffe%20%EC%9D%B4%EC%9A%A9%ED%95%98%EA%B8%B0.md)
  * [Setting Caffe on Windows with CUDA & Python](http://m.blog.naver.com/bsh0128/220733003127)
  * [A DSL for deep neural networks, supporting Caffe and Torch http://ajtulloch.github.io/dnngraph](https://github.com/ajtulloch/dnngraph)
  * [Deep Dreams (with Caffe)](https://github.com/google/deepdream/blob/master/dream.ipynb)
  * [Running Google’s Deep Dream on Windows (with or without CUDA) – The Easy Way](http://thirdeyesqueegee.com/deepdream/2015/07/19/running-googles-deep-dream-on-windows-with-or-without-cuda-the-easy-way/)
  * [Deep Learning and Caffe](http://whydsp.org/319)
  * [[Deep Learning] 영상을 이용하기위한 Convolutional Neural Networks, CNN](http://jangjy.tistory.com/181)
  * [Modeling Images, Videos and Text Using the Caffe Deep Learning Library, part 1 (by Kate Saenko)](http://www.slideshare.net/ktoshik/kate-saenko-msr-russia-summer-school-modeling-images-video-text-caffe-dl-part1)
  * [Apply simple pruning on Caffemodel](https://github.com/garion9013/impl-pruning-caffemodel)
  * [Caffe to TensorFlow](https://github.com/ethereon/caffe-tensorflow)
  * [C++ Example 1. Hello Caffe](http://deeplearningstudy.github.io/doc_caffe_example_1hellocaffe.html)
    * [Caffe C++ API on Windows](http://blog.naver.com/atelierjpro/220835313030)
* [deepart.io](http://www.deepart.io/) - Generate images styled like your favorite artist
* [DL4J Deep Learning for Java](http://deeplearning4j.org/)
  * [DL4J Java자바를 위한 딥 러닝](http://deeplearning4j.org/kr-index.html)
  * [인공 신경망 및 심층 신경망 소개](http://deeplearning4j.org/kr-neuralnet-overview.html)
  * [A Beginner’s Guide to Recurrent Networks and LSTMs](http://deeplearning4j.org/lstm.html)
  * [Using Neural Networks With Regression](http://deeplearning4j.org/linear-regression.html)
  * [RBM with DL4J for Deep Learning](http://www.slideshare.net/uspace/rbm-with-dl4j-for-deep-learning-50955012)
  * [NN Models with DL4J for Deep Learning](http://www.slideshare.net/uspace/nn-models-with-dl4j-for-deep-learning)
  * [A Beginner’s Guide to Eigenvectors, PCA, Covariance and Entropy](http://deeplearning4j.org/eigenvector)
  * [“딥러닝, 게을러지려고 연구하죠”...아담 깁슨 DL4J 창시자](https://www.imaso.co.kr/news/article_view.php?article_idx=20150824223056)
  * [Exploring convolutional neural networks with DL4J](http://brooksandrew.github.io/simpleblog/articles/convolutional-neural-network-training-with-dl4j/)
  * [Deep Learning Using DL4J and Spark on HDP for Fun and Profit](https://www.youtube.com/watch?v=XCX0GsswDfM)
  * [rl4j - Reinforcement Learning for the JVM](https://github.com/deeplearning4j/rl4j)
  * [MLPClassifierLinear](https://www.youtube.com/watch?v=BN_g2t0ykxg) This is a screencast that shows building a Linear Classifier using a Neural Network
* [Elephas: Distributed Deep Learning with Keras & Spark](http://maxpumperla.github.io/elephas)
* [Eesen - The official repository of the Eesen project](https://github.com/srvk/eesen)
* [gemmlowp: a small self-contained low-precision GEMM library](https://github.com/google/gemmlowp)
* [Gradient Boosting Interactive Playground](http://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html)
* [Keras: Deep Learning library for Theano and TensorFlow](http://keras.io/)
  * [Deep Learning: Keras Short Tutorial](https://www.youtube.com/watch?v=Tp3SaRbql4k)
  * [Keras로 Multi Layer Percentron 구현하기](http://iostream.tistory.com/111)
    * [Keras_MNIST_Example.ipynb](https://github.com/dolpang2/Keras-Examples/blob/master/Keras_MNIST_Example.ipynb)
  * [github.com/jaeho-kang/deep-learning/tree/master/keras](https://github.com/jaeho-kang/deep-learning/tree/master/keras)
  * [Keras에서 CNN학습 시키기](https://github.com/jaeho-kang/deep-learning/blob/master/keras/keras%E1%84%8B%E1%85%A6%E1%84%89%E1%85%A5_cnn%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8_%E1%84%89%E1%85%B5%E1%84%8F%E1%85%B5%E1%84%80%E1%85%B5.md)
  * [Keras로 대용량 이미지 처리하기](https://github.com/jaeho-kang/deep-learning/blob/master/keras/keras%E1%84%85%E1%85%A9_%E1%84%83%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%85%E1%85%A3%E1%86%BC_%E1%84%8B%E1%85%B5%E1%84%86%E1%85%B5%E1%84%8C%E1%85%B5_%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5.md)
  * [What are good resources/tutorials to learn Keras (deep learning library in Python)?](https://www.quora.com/What-are-good-resources-tutorials-to-learn-Keras-deep-learning-library-in-Python)
  * [Predicting sequences of vectors (regression) in Keras using RNN - LSTM](http://danielhnyk.cz/predicting-sequences-vectors-keras-using-rnn-lstm/)
  * [Binary Classification Tutorial with the Keras Deep Learning Library](http://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/)
  * [Installing Keras for deep learning](http://www.pyimagesearch.com/2016/07/18/installing-keras-for-deep-learning/)
  * [Regression Tutorial with the Keras Deep Learning Library in Python](http://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/)
  * [keras-rl](https://tensorflowkorea.wordpress.com/2016/08/03/keras-rl/)
  * [Pre-trained DL Model for Keras](https://tensorflowkorea.wordpress.com/2016/08/04/pre-trained-dl-model-for-keras/)
  * [Text Generation With LSTM Recurrent Neural Networks in Python with Keras](http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/)
  * [Music auto-tagging models and trained weights in keras/theano](https://github.com/keunwoochoi/music-auto_tagging-keras)
  * [How convolutional neural networks see the world](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html)
  * [Sequence to Sequence Learning with Keras](https://github.com/farizrahman4u/seq2seq)
  * [Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras](http://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/)
  * [A simple neural network with Python and Keras](http://www.pyimagesearch.com/2016/09/26/a-simple-neural-network-with-python-and-keras/)
  * [Display Deep Learning Model Training History in Keras](http://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/)
  * [Tutorial: Optimizing Neural Networks using Keras (with Image recognition case study)](https://www.analyticsvidhya.com/blog/2016/10/tutorial-optimizing-neural-networks-using-keras-with-image-recognition-case-study/)
  * [Keras LSTMs](http://sachinruk.github.io/blog/Keras-LSTM/)
* [Labellio: Scalable Cloud Architecture for Efficient Multi-GPU Deep Learning](http://devblogs.nvidia.com/parallelforall/labellio-scalable-cloud-architecture-efficient-multi-gpu-deep-learning/)
* Lasagne
  * [Lasagne-CTC](https://github.com/skaae/Lasagne-CTC)
* [Mind - Flexible neural networks in JavaScript](http://www.mindjs.net/)
* [Mindori - On-demand GPUs for neural networks](http://mindori.com/)
* [mxnet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Scala, Go, Javascript and more http://mxnet.rtfd.org](https://github.com/dmlc/mxnet)
* [PyCNN - Image Processing in Cellular Neural Networks with Python](http://blog.ankitaggarwal.me/PyCNN/)
  * [Sequence To Sequence Attention Models In PyCNN](https://talbaumel.github.io/attention)
* [pylearn2-practice](https://github.com/zygmuntz/pylearn2-practice)
* [SINGA is a general distributed deep learning platform for training big deep learning models over large datasets](http://singa.apache.org/docs/overview.html)
* [VELES - Distributed platform for rapid Deep learning application development](https://velesnet.ml/)

# Microsoft
* The Microsoft Cognitive Toolkit 마이크로소프트에서 개발한 딥러닝 프레임워크 CNTK
  * [blog](https://blogs.microsoft.com/next/2016/10/25/microsoft-releases-beta-microsoft-cognitive-toolkit-deep-learning-advances)
  * [website](https://www.microsoft.com/en-us/research/product/cognitive-toolkit/)
  * [github.com/Microsoft/CNTK](https://github.com/Microsoft/CNTK)
  * CNTK v1.x; 속도는 빠르지만 C++, C# API만 지원하고 실서비스 배포가 불편한 문제
  * CNTK v2.0; Python API 지원, 최적화된 분산 학습 가능
  * [1-bit SGD, Model sharing 등 최적화된 대용량 처리에 초점을 맞춰서 개발](https://www.microsoft.com/en-us/research/product/cognitive-toolkit/features/)
  * [Image, Speech, Text 분야의 다양한 학습 모델](https://www.microsoft.com/en-us/research/product/cognitive-toolkit/model-gallery/)
  * [학습된 모델을 Azure로 배포, 서비스 가능](https://github.com/Microsoft/CNTK/wiki/Evaluate-a-model-in-an-Azure-WebApi)

# Mooc
* [Deep Learning Courses](http://machinelearningmastery.com/deep-learning-courses/)

# Paper
* [9 Key Deep Learning Papers, Explained](http://www.kdnuggets.com/2016/09/9-key-deep-learning-papers-explained.html/3)
  * 이름 / 이해 난이도 / 읽기 수월함 / 필수성 / 선행지식
  * AlexNet (2012) 하 / 쉬움 / 필수 / 콘볼루션 오퍼레이션 지식, 이미지넷 챌린지
  * ZF Net (2013) 하 / 쉬움 / 옵션 (Segmentation, Localization을 하겠다고 하면 필수) / AlexNet
  * VGG Net (2014) 하 / 쉬움 / 옵션 / AlexNet
  * GoogLeNet (2015) 상 / 어려움 / 옵션 / AlexNet, Hebb 법칙
  * Microsoft ResNet (2015) 중 / 쉬움 / 필수와 옵션의 중간 / AlexNet, VGG Net, NiN(Network in Network)
  * Region Based CNNs (R-CNN - 2013, Fast R-CNN - 2015, Faster R-CNN - 2015)
    * 하 (부분적 상) / 중간 / 옵션 (Segmentation, Localization을 하겠다고 하면 필수. Fast R-CNN을 중심으로 보는게 좋음) / PASCAL 챌린지
  * Generative Adversarial Networks (2014) ? / ? / 필수 / VGG Net
  * Generating Image Descriptions (2014) 상 / 쉬움 / 중간 (이미지 to 문장을 하겠다고 하면 필수) / LSTM, 캡셔닝 챌린지
  * Spatial Transformer Networks (2015) 중 / 어려움 / 옵션 (아직 불명) / 공간변환
* [Deep Learning Papers Reading Roadmap](https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap)
* [Solving Verbal Comprehension Questions in IQ Test by Knowledge-Powered Word Embedding](http://arxiv.org/pdf/1505.07909v1.pdf)
* [Decoupled Neural Interfaces using Synthetic Gradients](https://arxiv.org/pdf/1608.05343.pdf) synthetic gradient - 뉴럴넷 업데이트 과정의 모듈간 강결합을 decouple
* ["Transfer Learning from Deep Features for Remote Sensing and Poverty Mapping"](http://arxiv.org/abs/1510.00098)
  * [code & data](https://github.com/nealjean/predicting-poverty)
* [Stacked Approximated Regression Machine: A Simple Deep Learning Approach](https://arxiv.org/pdf/1608.04062v1.pdf)
  * SARM이라는 layer wise training 기법
  * Back propagation 없이 layer 단위로 학습을 시켜도 현재 state of the art DNN과 비슷하거나 더 나은 성능을 보인다는 주장
  * PCANet에 non linearity를 추가
* MNIST 숫자 인식기 Gaussian Bayesian 확률 모델로 구현
  * 목표
    * MNIST 데이터 특성 시각적으로 이해하기
    * Python, numpy, matplotlib 사용해 보기
    * Bayesian Theorem 이해하고 구현해 보기
    * Multivariate Gaussian Distribution 이해하고 구현해 보기
  * 실험 데이터
    * 학습 데이터: MNIST 기본 60,000개
    * 테스트 데이터: MNIST 기본 10,000개
  * 실험 결과
    * Bayesian 확률 모델만으로 분류 정확도가 대략 84% 정도 나오는 것을 확인
    * Multivariate Gaussian 적용하니까 분류 정확도가 대략 92% 정도까지 올라가는 것을 확인
  * 코드
    * [메인 프로그램](https://github.com/dgtgrade/HumanLearning/blob/master/2001a.py)
      * numpy, matplotlib 외에 본격 머신러닝 라이브러리는 전혀 사용하지 않았음
      * 머신러닝 관련 부분 대략 200줄 이하로 매우 짧음
      * 시각화 관련 코드 및 코멘트 등이 대략 300줄 정도임
    * [MNIST 데이터 파일](https://github.com/dgtgrade/HumanLearning/tree/master/data) MNIST 공식 홈페이지에서 받은 그대로
    * [MNIST 데이터 로딩 프로그램](https://github.com/dgtgrade/HumanLearning/blob/master/mnist2ndarray.py)
    * [Multivariate Gaussian 적용하지 않고 Bayesian 확률 모형만으로 돌아가는 코드: 위 2001a.py 옛날 버전](https://github.com/dgtgrade/HumanLearning/blob/8e57a2b3340da3b38956b83cf24433d3a9fbd11b/2001a.py)
  * 실험 동영상
    * 학습: 실험데이터 전체 60000개를 학습하는 과정을 보여줌
    * 테스트: 테스트 데이터 전체 10000개를 테스트 하는 과정을 보여줌
    * 테스트 과정에서 정답률은 1번 후보만으로 구했으나, 표시는 3번후보까지 하였음
* [Arxiv Sanity Preserver](http://www.arxiv-sanity.com/)
* [SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](https://arxiv.org/pdf/1609.05473v2.pdf)
  * GAN이 처음으로 sequence generation task에 사용
  * GAN은 진짜같은 Fake data를 만들어내는 Generator과 진짜 data와 Fake data를 구분해내는 Discriminator를 학습시키는 알고리즘
  * 실수 픽셀들로 이루어진 그림과 달리 discrete한 토큰들의 sequence를 생성해낼 때 현재 얼마나 Generator가 잘 학습을 하고 있는지 평가할 방법이 마땅치 않아 sequence generation task에서는 사용되지 않음
  * 이번에 발표된 SeqGAN 은 discriminator를 Policy Gradient 의 Reward 로 사용해서 이 문제를 해결, Text Generation, Music Generation Task 에 적용
  * [Ian Goodfellow (GAN 저자) 의 Reddit 문답(왜 NLP에 GAN이 사용되기 힘든가)](https://www.reddit.com/r/MachineLearning/comments/40ldq6/generative_adversarial_networks_for_text/)
* ["Distributed Training of Deep Neuronal Networks: Theoretical and Practical Limits of Parallel Scalability](http://arxiv.org/abs/1609.06870v1)
  * 여러 노드를 썼을 때, 네트웍 벤드위쓰와 전체 노드에서의 계산을 기다리면 어떻게 되는지
  * 싱글 노드에서 할 때 batch 사이즈를 달리하면 어느 layer 계산이 bottleneck인지
  * 이런 문제를 방지하기 위해 디자인을 바꿀 때 어디부터 보면 되는지
  * 계산량을 어떻게 계산하는지
* [Computer Vision and Pattern Recognition (cs.CV)](https://scirate.com/arxiv/cs.CV) arXiv에 올라온, CV/PR 주제 논문의 초록만 모아 보여줌
* [The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition ECCV 2016](https://arxiv.org/pdf/1511.06789v3.pdf)
  * Fine-Grained Recognition을 할 때 Noisy Fine-Grained Data, 즉 Web에서 검색한 Noisy하지만 큰 데이터가 도움이 된다는 내용
  * Noisy Fine-Grained Data 구축
    * 새의 종을 구별하는 데이터베이스를 구축한다면, Wikipedia에서 종을 검색하여 그 키워드를 기반으로 구글링하여 이미지 구축
    * 여러 카테고리에 동시에 등장하는 그림을 지우는 등의 간단한 정제작업을 추가
    * 여전히 이 데이터베이스는 롱테일 문제도 있고 에러도 존재
  * 실험 결과, 퀄리티가 좋지만 작은 데이터보다 성능이 좋다
  * 큰 Noisy Fine-Grained Data로 학습한 후 좋은 데이터로 튜닝하면 더 좋다
  * [Factors in Finetuning Deep Model for Object Detection with Long-tail Distribution](http://www.ee.cuhk.edu.hk/~wlouy…/…/OuyangFactors_CVPR16.pdf)
    * Long-tail Distribution을 가진 DB의 문제점 지적
  * [Training Region-based Object Detectors with Online Hard Example Mining](https://arxiv.org/abs/1604.03540,
CVPR2016)
    * easy examples과 hard examples의 너무 큰 차이에 대해서 문제를 지적
  * Fine-Grained Recognition이라는 테스크에 한정된 실험, 분석, 수학적인 설명 부족한 논문
* [ICLR 2017 - Conference Track International Conference on Learning Representations](http://openreview.net/group?id=ICLR.cc%2F2017%2Fconference)
* [A Review on a Deep Learning that Reveals the Importance of Big Data](https://fananymi.wordpress.com/)

# Reinforcement Learning, RL
* [Fundamental of Reinforcement Learning](https://dnddnjs.gitbooks.io/rl/content/)
* [Ujava.org reinforcement-learning](http://www.slideshare.net/uspace/ujavaorg-reinforcementlearning)
* [ujava.org Reinforcement Learning (2nd)](http://www.slideshare.net/uspace/ujavaorg-reinforcement-learning-2nd)
* [ujava.org workshop : Reinforcement Learning with Thompson Sampling](http://www.slideshare.net/uspace/ujavaorg-workshop-reinforcement-learning-with-thompson-sampling)
* [Reinforcement Learning and DQN, learning to play from pixels](https://rubenfiszel.github.io/posts/rl4j/2016-09-08-DQN-Learning-to-play-from-pixels-step-by-step.html)
* [Guest Post (Part I): Demystifying Deep Reinforcement Learning](http://www.nervanasys.com/demystifying-deep-reinforcement-learning/)
  * [딥 강화학습 쉽게 이해하기](http://ddanggle.github.io/ml/ai/cs/2016/09/24/demystifyingDL.html)
* [Deep Learning in a Nutshell: Reinforcement Learning](https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-reinforcement-learning/)
* [Bayesian Programming and Learning for Multi-Player Video Games Application to RTS AI](http://emotion.inrialpes.fr/people/synnaeve/phdthesis/phdthesis.html)
  * [Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks](http://arxiv.org/abs/1609.02993)
  * 스타크래프트 같은 실시간 전략 (RTS) 게임은 체스나 바둑과는 다르게 제한된 자원(미네랄, 가스 등)과 불확실한 정보 (보이지 않는 상대방의 플레이 등) 속에서 의사결정을 해야하는 어려움이 존재
  * 이 논문에서는 “참/거짓”으로 표현되는 boolean logic이 아닌 베이지언 모델링으로 이런 정보의 불확실함(uncertainty)를 처리
* [Deep Learning in a Nutshell: Reinforcement Learning](https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-reinforcement-learning/)
* [강화 학습 기초 Reinforcement Learning an introduction](http://www.slideshare.net/carpedm20/reinforcement-learning-an-introduction-64037079)
* [async-rl-tensorflow - Asynchronous Methods for Deep Reinforcement Learning](https://github.com/devsisters/async-rl-tensorflow)
* [LEARNING REINFORCEMENT LEARNING (WITH CODE, EXERCISES AND SOLUTIONS)](http://www.wildml.com/2016/10/learning-reinforcement-learning/)
* [Reinforcement Learning 101 (in 15 minutes)](https://www.facebook.com/SKTBrain/posts/311444575893030)
* [Bandit 101](https://www.facebook.com/SKTBrain/posts/313678162336338) Multi-Armed Bandit (MAB) 입문자료

# Spark
* [DeepSpark: Spark-Based Deep Learning Supporting Asynchronous Updates and Caffe Compatibility](http://hgpu.org/?p=15511)
* [yahoo/CaffeOnSpark](https://github.com/yahoo/CaffeOnSpark)
* [SparkNet: Training Deep Networks in Spark](http://arxiv.org/abs/1511.06051)
* [spark-summit.org/2016/schedule](https://spark-summit.org/2016/schedule/)
  * Large-Scale Deep Learning with TensorFlow (Jeff Dean)
    * [slide](http://www.slideshare.net/JenAman/large-scale-deep-learning-with-tensorflow)
    * [video](https://youtu.be/XYwIDn00PAo)
  * [AI: The New Electricity (Andrew Ng)](https://youtu.be/4eJhcxfYR4I)
  * Large Scale Multimedia Data Intelligence And Analysis On Spark (Baidu)
    * [slide](http://www.slideshare.net/JenAman/large-scale-multimedia-data-intelligence-and-analysis-on-spark)
    * [video](https://youtu.be/LrtdyCWphvs)
  * Scaling Machine Learning To Billions Of Parameters (Yahoo)
    * [slide](http://www.slideshare.net/JenAman/scaling-machine-learning-to-billions-of-parameters)
    * [video](https://youtu.be/l_1S7W_l2cI)
  * CaffeOnSpark: Deep Learning On Spark Cluster (Yahoo)
    * [slide](http://www.slideshare.net/JenAman/caffeonspark-deep-learning-on-spark-cluster)
    * [video](https://youtu.be/Mn7QEdUFSnQ)
  * Scalable Deep Learning in Baidu
    * [slide](http://www.slideshare.net/JenAman/scalable-deep-learning-platform-on-spark-in-baidu)
    * [video](https://youtu.be/n9yZNmC20pc)
