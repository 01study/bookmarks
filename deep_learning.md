Deep Learning
=============
* [입문자를 위한 딥러닝 튜토리얼 ](http://courseshare.co.kr/course/39?pageType=Intro)
* [테리의 딥러닝 토크](https://www.youtube.com/playlist?list=PL0oFI08O71gKEXITQ7OG2SCCXkrtid7Fq)
* [쉽게 풀어쓴 딥러닝(Deep Learning)의 거의 모든 것](http://t-robotics.blogspot.kr/2015/05/deep-learning.html)
* [완전쉬운 딥러닝](https://docs.google.com/document/d/11A7207YsYcKU7F3uq117pNGRGIReEP88__gZmflIXrs/edit)
  * [완전쉬운 딥러닝 동영상 원고](http://kr.deductiontheory.com/2017/03/blog-post_3.html)
  * [Super Easy Deep Learning](https://www.youtube.com/watch?v=8NkZohHnxck&feature=youtu.be)
* [구글, 유다시티에 딥러닝 강의 무료 공개](http://www.bloter.net/archives/248374)
* [[딥러닝] Gradient Descent & Normal Eq.](https://www.youtube.com/watch?v=M9Gsi3VBTYM&feature=youtu.be)
* [모두를 위한 딥러닝 강좌](http://www.se.or.kr/m/post/161)
  * **[기본적인 머신러닝과 딥러닝 강의](http://hunkim.github.io/ml/)**
  * [TensorFlow-Tutorials](https://github.com/hunkim/TensorFlow-Tutorials)
  * [TensorFlow Basic Tutorial Labs](https://github.com/hunkim/DeepLearningZeroToAll)
  * [Lec 00 - Machine/Deep learning 수업의 개요와 일정](https://www.youtube.com/watch?v=BS6O0zOGX4E)
  * [lec11-1 ConvNet의 Conv 레이어 만들기](https://www.youtube.com/watch?v=Em63mknbtWo&feature=youtu.be)
  * [lab11: ConvNet을 TensorFlow로 구현하자 (MNIST 99%)](https://www.youtube.com/watch?v=6KlkiKyjEu0&feature=youtu.be)
  * [lec12: NN의 꽃 RNN 이야기](https://www.youtube.com/watch?v=-SHPG_KMUkQ&feature=youtu.be)
  * [lab12: TensorFlow에서 RNN 구현하기](https://www.youtube.com/watch?v=A8wJYfDUYCk&feature=youtu.be)
* [모두를 위한 딥러닝 강좌](http://www.se.or.kr/161)
* [C++로 배우는 딥러닝](http://blog.naver.com/atelierjpro/220697890605)
  * [C++로 배우는 딥러닝](https://www.youtube.com/playlist?list=PLNfg4W25Tapy5hIBmFZgT5coii1HUX6BD)
  * [딥러닝 4-3. 프로그래머를 위한 경사 하강법 The Gradient Descent Method for Programmers](http://blog.naver.com/atelierjpro/220755873110)
  * [딥러닝 4-4. 프로그래머를 위한 연쇄 미분 Chain Rule](http://m.blog.naver.com/atelierjpro/220760659825)
  * [딥러닝 4.4 - 연쇄 미분 ChainRule](https://www.youtube.com/watch?v=g3nhLjYRT5I&feature=youtu.be)
  * [딥러닝 6. Fully Connected Neural Network](http://blog.naver.com/atelierjpro/220773276384)
  * [딥러닝 7. Implementing FCNN](http://blog.naver.com/atelierjpro/220774988242)
* [딥러닝 용어 정리](http://docs.likejazz.com/deep-learning-glossary/)
* [완전쉬운 딥러닝](http://kr.deductiontheory.com/2017/01/blog-post.html)
* [수학포기자를 위한 딥러닝-#1 머신러닝과 딥러닝 개요](http://bcho.tistory.com/1140)
* [수학포기자를 위한 딥러닝-#2 - 선형회귀분석을 통한 머신러닝의 기본 개념 이해](http://bcho.tistory.com/1139)
* [수학포기자를 위한 딥러닝-#3 텐서플로우로 선형회귀 학습을 구현해보자](http://bcho.tistory.com/1141)
* [수학포기자를 위한 딥러닝-#4 로지스틱 회귀를 이용한 분류 모델](http://bcho.tistory.com/1142)
* **[Practical Deep Learning For Coders, Part 1](http://course.fast.ai/)**
* [러닝 딥러닝](https://www.youtube.com/playlist?list=PL1H8jIvbSo1q6PIzsWQeCLinUj_oPkLjc)
* [6.S191: Introduction to Deep Learning](http://introtodeeplearning.com/)
  * [MIT 6.S191: Introduction to Deep Learning](https://www.youtube.com/playlist?list=PLkkuNyzb8LmxFutYuPA7B4oiMn6cjD6Rs)
* [A Beginner’s Guide to Deep Neural Networks](http://googleresearch.blogspot.kr/2015/09/a-beginners-guide-to-deep-neural.html)
* [Deep Learning for Beginners](http://randomekek.github.io/deep/deeplearning.html)
* [Welcome to the Deep Learning Tutorial!](http://deeplearning.stanford.edu/tutorial/)
* [ISBA 2015 Morning Tutorial: Deep Learning (March 23, 2015)](https://www.youtube.com/watch?v=gCwYO7zVJs0)
* [tutorial, implementations](https://github.com/dsindex/blog/wiki/%5Bdeep-learning%5D-tutorial,-implementations)
* [Tutorial on Deep Learning](https://simons.berkeley.edu/talks/tutorial-deep-learning)
* [Building Safe A.I.  A Tutorial for Encrypted Deep Learning](https://iamtrask.github.io/2017/03/17/safe-ai)
* [Deep Learning - Taking machine learning to the next level](https://www.udacity.com/course/deep-learning--ud730)
* **[Awesome Deep Learning](https://github.com/ChristosChristofidis/awesome-deep-learning)**
* [Awesome - Most Cited Deep Learning Papers](https://github.com/terryum/awesome-deep-learning-papers)
* [Deep Learning - 2016년 8월부터 딥러닝공부를 하면서 봤던 강의영상, 동영상, 블로그들의 목록입니다](https://github.com/GunhoChoi/Deep_Learning_Collection)
* [digest.deeplearningweekly.com](http://digest.deeplearningweekly.com/)
* [Top Deep Learning Projects](https://github.com/hunkim/DeepLearningStars)
* [Deep Learning Resources](https://omtcyfz.github.io/2016/08/29/Deep-Learning-Resources.html)
* [Up to Speed on Deep Learning: September, Part 2 and October, Part 1](https://medium.com/the-mission/up-to-speed-on-deep-learning-september-part-2-and-october-part-1-d72d7e5df1ea)
* [My playlist – Top YouTube Videos on Machine Learning, Neural Network & Deep Learning](http://www.analyticsvidhya.com/blog/2015/07/top-youtube-videos-machine-learning-neural-network-deep-learning/)
* [Designing Machine Learning Models: A Tale of Precision and Recall](http://nerds.airbnb.com/designing-machine-learning-models/)
* [[deep learning] tutorial, implementations](https://github.com/dsindex/blog/wiki/%5Bdeep-learning%5D-tutorial,-implementations)
* [Deep Learning Study](http://deeplearningstudy.github.io/material/) Caffe, TensorFlow
* [The Deep Learning Playbook](https://medium.com/@jiefeng/deep-learning-playbook-c5ebe34f8a1a)
* [A Brief Overview of Deep Learning](http://yyue.blogspot.kr/2015/01/a-brief-overview-of-deep-learning.html)
* [github.com/wbaek/deeplearing_exercise](https://github.com/wbaek/deeplearing_exercise)
* [deepcumen.com](http://deepcumen.com/)
* [Deep Learning Study - Study of HeXA at Ulsan National Institute of Science and Technology](https://github.com/carpedm20/deep-learning-study)
* [[ML] My Journal from Neural Network to Deep Learning: A Brief Introduction to Deep Learning. Contents](http://haohanw.blogspot.kr/2015/01/deep-learning-introduction.html)
* [Deep learning - Yann LeCun, Yoshua Bengio & Geoffrey Hinton](http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html)
* [Deep learning](http://neuralnetworksanddeeplearning.com)
* [nvidia Deep Learning Courses](https://developer.nvidia.com/deep-learning-courses)
  * [Deep Learning Course](https://www.youtube.com/playlist?list=PL5B692fm6--tI-ijknnVZWbXU2H4JpSYe)
* [CS224d: Deep Learning for Natural Language Processing](http://cs224d.stanford.edu/)
* [Why GEMM is at the heart of deep learning](http://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/)
* [GitHub Special: Data Scientists to Follow & Best Tutorials on GitHub](http://www.analyticsvidhya.com/blog/2015/07/github-special-data-scientists-to-follow-best-tutorials/)
* [딥러닝 워크샵: 딥러닝의 현재와 미래](http://mlcenter.postech.ac.kr/workshop)
  * [후기](http://whydsp.org/262)
* [Deep Learning at Flickr, Pierre Garrigues](http://techjaw.com/2015/03/04/deep-learning-at-flickr-pierre-garrigues/)
* [Andrew Ng: Why ‘Deep Learning’ Is a Mandate for Humans, Not Just Machines](http://www.wired.com/2015/05/andrew-ng-deep-learning-mandate-humans-not-just-machines/)
* [Andrew Ng: Deep Learning, Self-Taught Learning and Unsupervised Feature Learning](https://www.youtube.com/watch?v=n1ViNeWhC24)
* [The tensor renaissance in data science](http://radar.oreilly.com/2015/05/the-tensor-renaissance-in-data-science.html)
* [The Paradox of Deeper Learning: The Unlearning Curve](http://blogs.edweek.org/edweek/learning_deeply/2015/04/the_paradox_of_deeper_learning_the_unlearning_curve.html)
* [Are there Deep Reasons Underlying the Pathologies of Today’s Deep Learning Algorithms?](http://goertzel.org/DeepLearning_v1.pdf)
* [집단지성프로그래밍 05. 최적화(optimization) 김지은_20150522](http://www.slideshare.net/yeswldms/05-optimization-20150522)
* [ICLR 2015](http://www.iclr.cc/doku.php?id=iclr2015%3Amain)
  * [Artificial Tasks for Artificial Intelligence](https://www.dropbox.com/s/ly9y136saba0915/ICLR2015_Oral_Slides_All.pdf?oref=e&n=117881854)
* [Deep Learning Trends @ ICLR 2016](http://www.computervisionblog.com/2016/06/deep-learning-trends-iclr-2016.html)
* [ICLR 2017 - Conference Track International Conference on Learning Representations](http://openreview.net/group?id=ICLR.cc%2F2017%2Fconference)
  * [ICLR2017 Paper Index](https://tensorflowkorea.wordpress.com/2016/11/16/iclr2017-paper-index/)
  * [ICLR 2017 workshop track open review](http://nuit-blanche.blogspot.com/2017/02/iclr-2017-workshop-track-open-review.html)
  * [Learning to remember rare events](https://www.slideshare.net/ssuser06e0c5/learning-to-remember-rare-events)
* [Deep Learning for Computer Vision Barcelona](http://imatge-upc.github.io/telecombcn-2016-dlcv/)
* [Algorithms of the Mind](https://medium.com/deep-learning-101/algorithms-of-the-mind-10eb13f61fc4)
* [What's Wrong With Deep Learning?](https://drive.google.com/file/d/0BxKBnD5y2M8NVHRiVXBnOVpiYUk/edit)
* [Why does Deep Learning work?](https://charlesmartin14.wordpress.com/2015/03/25/why-does-deep-learning-work/)
* [Why Deep Learning Works II: the Renormalization Group](https://charlesmartin14.wordpress.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/)
* [Normalization 방법](https://www.slideshare.net/ssuser06e0c5/normalization-72539464)
* [18 Great Deep Learning Resources, most free](http://blog.sense.io/18-great-deep-learning-resources)
* [인공지능의 눈으로 바라본 세상](http://techneedle.com/archives/20800)
* [XLDB2015: Accelerating Deep Learning at Facebook](https://www.youtube.com/watch?v=KviuMAF4pEA)
* [The Brain vs Deep Learning Part I: Computational Complexity — Or Why the Singularity Is Nowhere Near](https://timdettmers.wordpress.com/2015/07/27/brain-vs-deep-learning-singularity/)
* [A Beginner’s Guide to Restricted Boltzmann Machines](http://deeplearning4j.org/restrictedboltzmannmachine.html)
* [Energy based models and boltzmann machines - v2.0](http://www.slideshare.net/blaswan/energy-based-models-and-boltzmann-machines-v20)
* [내맘대로 이해하는 Deep Belief Network와Restricted Boltzmann Machine](http://whydsp.org/283)
* [Deep Learning - Geoffrey Hinton - how to do backpropagation in a brain](https://www.youtube.com/watch?v=kxp7eWZa-2M&feature=youtu.be&t=38m13s)
* [Yes you should understand backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)
* [Yes you should understand backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)
* [Deep learning for assisting the process of music composition (part 1)](https://highnoongmt.wordpress.com/2015/08/11/deep-learning-for-assisting-the-process-of-music-composition-part-1/)
* [Andrew Ng: Deep Learning, Self-Taught Learning and Unsupervised Feature Learning](https://www.youtube.com/watch?v=n1ViNeWhC24)
* [Deep Learning Summer School 2015](https://sites.google.com/site/deeplearningsummerschool/)
  * [Deep Learning Summer School, Montreal 2015](http://videolectures.net/deeplearning2015_montreal/)
* [Deep Learning Summer School 2016](https://sites.google.com/site/deeplearningsummerschool2016/)
  * [Deep Learning Summer School, Montreal 2016](http://videolectures.net/deeplearning2016_montreal/)
  * [What I learned from Deep Learning Summer School 2016](https://www.linkedin.com/pulse/what-i-learned-from-deep-learning-summer-school-2016-hamid-palangi)
* [26 THINGS I LEARNED IN THE DEEP LEARNING SUMMER SCHOOL](http://www.marekrei.com/blog/26-things-i-learned-in-the-deep-learning-summer-school/)
* [Deep Learning and Neural Networks](http://cl.naist.jp/~kevinduh/a/deep2014/)
* [한국에서 처음 열린 GTC, 딥러닝의 현재를 이야기하다](http://chitsol.com/2273)
* [네이버, 사람 없이 이미지 뉴스 만든다](http://www.bloter.net/archives/238742)
* [Deep Learning Startups, Applications and Acquisitions – A Summary](http://blog.dennybritz.com/2015/10/13/deep-learning-startups-applications-and-acquisitions-a-summary/)
* [Theoretical Motivations for Deep Learning](http://rinuboney.github.io/2015/10/18/theoretical-motivations-deep-learning.html)
* [How We Use Deep Learning to Classify Business Photos at Yelp](http://engineeringblog.yelp.com/2015/10/how-we-use-deep-learning-to-classify-business-photos-at-yelp.html)
* [Artificial Intelligence, Neural Networks, and Deep Learning](http://kimschmidtsbrain.com/2015/10/29/artificial-intelligence-neural-networks-and-deep-learning/)
* [Deep Learning in a Single File for Smart Devices](http://dmlc.ml/mxnet/2015/11/10/deep-learning-in-a-single-file-for-smart-device.html)
* [Boosting Methods](http://enginius.tistory.com/m/post/606)
* [Evaluation of Deep Learning Toolkits](https://github.com/zer0n/deepframeworks/blob/master/README.md)
* [Deep Residual Networks](https://github.com/KaimingHe/deep-residual-networks)
* [stat212b - Topics Course on Deep Learning for Spring 2016](https://github.com/joanbruna/stat212b)
* [Fujitsu develops new deep learning technology to analyze time-series data with high precision](http://phys.org/news/2016-02-fujitsu-deep-technology-time-series-high.html)
* 2016-02-17~18 자연어처리 튜토리얼 심층학습과 언어처리 응용
  * TensorFlow Tutorial; SKT 정상근 박사님
    * [GitHub](https://github.com/hugman/deep_learning)
    * mnist.py 파일을 먼저 본다.
    * mnist_with_monitoring.py
      * TensorBoard 서버에 모니터링 로그를 보내고 웹으로 볼 수 있다
    * Keras
      * computaion backend를 추상화하여 편하게 적용할 수 있도록 하는 라이브러리
      * theano, tensorflow 둘다 지원
      * 형태소 분석기같은 걸 빠르게 만들어 보기에 좋다
    * pos_tagger_fcn.py
      * word embedding => wikipedia로 SENNA에서 만들어서 오픈한 데이터
      * 변수명은 mnist와 일부러 동일하게 뒀으니 비교해서 보면 좋음
      * 참고: fcn => fully connected network
    * pos_tagger_rnn_seq.py
      * 궁극의 코드!
      * learning rate를 처음에는 크게해서 점점 작게 만들어 준다
      * optimizer를 바꿔가며 학습을 할 수도 있다
      * epoch이 50이 넘으면 중간에 바꿔라! 등
    * TensorFlow github 소스에 보면,
      * models -> rnn -> translate 구글의 기계번역 소스가 있다
      * https://github.com/tensorflow/tensorflow/tree/master/tensorflow/models/rnn/translate
  * Deep Learning for NLP 응용; 강원대 이창기 박사님
    * [NLP from Scratch](http://arxiv.org/abs/1103.0398)
    * [SENNA](http://ronan.collobert.com/senna)
      * CNN + CRF
    * RNN(LSTM) + attention score
      * attention score는 alignment prob과 동일한 역할을 해준다.
    * t-SNE scatter plot
      * 자질의 차원을 축소하여 2차원으로 뿌려주는 방식
* [Visualizing Deep Learning with t-SNE (Tutorial and Video)](https://medium.com/@awjuliani/visualizing-deep-learning-with-t-sne-tutorial-and-video-e7c59ee4080c)
* [Make Your Own 3D t-SNE Visualizations (Download Binary and Code)](https://medium.com/@awjuliani/make-your-own-3d-t-sne-visualizations-download-e0cdfe80d6e3)
* [오픈 소스 딥러닝 소프트웨어](http://kernelstudy.net/t/topic/188)
* [DataScience/Deep Learning](http://khanrc.tistory.com/category/DataScience/Deep%20Learning)
* [Introduction to Deep Learning for Image Analysis at Strata NYC, Sep 2015](http://www.slideshare.net/dato-inc/introduction-to-deep-learning-for-image-analysis-at-strata-nyc-sep-2015)
* [Show and tell takmin: A Neural Image Caption Generator](http://www.slideshare.net/takmin/show-andtell-takmin)
* [딥러닝 임팩트가 온다](http://techholic.co.kr/archives/51820)
* [Deep Learning for Visual Question Answering](http://avisingh599.github.io/deeplearning/visual-qa/)
* [When Does Deep Learning Work Better Than SVMs or Random Forests?](http://www.kdnuggets.com/2016/04/deep-learning-vs-svm-random-forest.html)
* [openai.com](https://openai.com)
  * [OpenAI Gym Beta](https://openai.com/blog/openai-gym-beta/)
  * [requests-for-research](https://openai.com/requests-for-research/)
  * [PIXELCNN++: A PIXELCNN IMPLEMENTATION WITH DISCRETIZED LOGISTIC MIXTURE LIKELIHOOD AND OTHER MODIFICATIONS](https://openreview.net/pdf?id=BJrFC6ceg)
    * [pixel-cnn++ - This is a Python3 / Tensorflow implementation of PixelCNN++](https://github.com/openai/pixel-cnn)
  * [OpenAI Universe (OpenAI)](https://universe.openai.com/)
  * [오픈소스로…인공지능 학습 플랫폼](http://techholic.co.kr/archives/64126)
  * [GTA V + Universe](https://openai.com/blog/GTA-V-plus-Universe/)
  * [Actor Critic with OpenAI Gym](http://www.rage.net/~greg/2016-07-05-ActorCritic-with-OpenAI-Gym.html)
  * [Learning to communicate](https://openai.com/blog/learning-to-communicate/)
* [Neural Programmer-Interpreters](http://www-personal.umich.edu/~reedscot/iclr_project.html)
* [Video Recordings of the ICML’15 Deep Learning Workshop](http://dpkingma.com/?page_id=483)
  * [딥러닝 워크샵 패널토의 @ ICML2015](http://t-robotics.blogspot.com/2015/07/icml2015.html#.VyyRohWLSZ0)
* [Deep Learning: Nine Lectures at Collège de France](http://cilvr.nyu.edu/doku.php?id=courses%3Adeeplearning-cdf2016%3Astart)
* [Deep Learning with RE•WORK #reworkDL](https://www.youtube.com/playlist?list=PLnDbcXCpYZ8m412d2KX5paGKdGUxxWCEP)
* [csl.sony.fr/publications](https://www.csl.sony.fr/publications.php)
  * [인공지능이 편곡한 '환희의 송가'](http://www.yonhapnews.co.kr/local/0899000000.html?cid=MYH20160511017400797)
  * [Machine Learning Techniques for Reorchestrating the European Anthem](https://www.youtube.com/watch?list=PLuOoXrWK6Kz5ySULxGMtAUdZEg9SkXDoq&v=0qnTaAz-xtQ)
* [Deep Patient: An Unsupervised Representation to Predict the Future of Patients from the Electronic Health Records](http://www.nature.com/articles/srep26094)
* [deepnumbers.com](http://www.deepnumbers.com/)
* [10 Deep Learning Terms Explained in Simple English](http://www.datasciencecentral.com/m/blogpost?id=6448529%3ABlogPost%3A410633)
* [A Statistical View of Deep Learning](http://blog.shakirm.com/ml-series/a-statistical-view-of-deep-learning/)
* [Deep Learning in Practice: Speech Recognition and Beyond](http://events.technologyreview.com/emtech/digital/16/video/watch/andrew-ng-deep-learning/)
* [CHAR2WAV: END-TO-END SPEECH SYNTHESIS](https://mila.umontreal.ca/en/publication/char2wav-end-to-end-speech-synthesis/)
* [Google DeepMind Teaches Artificial Intelligence Machines to Read](http://www.technologyreview.com/view/538616/google-deepmind-teaches-artificial-intelligence-machines-to-read/)
* [WaveNet: A Generative Model for Raw Audio](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)
  * [A TensorFlow implementation of DeepMind's WaveNet paper](https://github.com/ibab/tensorflow-wavenet)
* [10 Deep Learning Trends at NIPS 2015](http://codinginparadise.org/ebooks/html/blog/ten_deep_learning_trends_at_nips_2015.html)
  * [딥러닝의 10가지 트렌드 from NIPS 2015](http://t-robotics.blogspot.com/2016/01/10-from-nips-2015.html)
* [NIPS 2016](http://beamandrew.github.io/deeplearning/2016/12/12/nips-2016.html)
  * [nips.cc/Conferences/2016/Schedule](https://nips.cc/Conferences/2016/Schedule)
  * [[Project] All Code Implementations for NIPS 2016 papers](https://cdn.ampproject.org/c/s/amp.reddit.com/r/MachineLearning/comments/5hwqeb/project_all_code_implementations_for_nips_2016/)
  * [LET'S DISCUSS: LEARNING METHODS FOR DIALOGUE NIPS 2016 WORKSHOP](http://letsdiscussnips2016.weebly.com/schedule.html)
  * [[NIPS 2016 tutorial - Summary] Nuts and bolts of building AI applications using Deep Learning](http://jaejunyoo.blogspot.com/2017/03/nips-2016-tutorial-summary-nuts-and-bolts-of-building-AI-AndrewNg.html)
    * [초짜 대학원생의 입장에서 정리한 NIPS 2016 tutorial: Nuts and bolts of building AI applications using Deep Learning by Andrew Ng](http://jaejunyoo.blogspot.com/2017/03/kr-nips-2016-tutorial-summary-nuts-and-bolts-of-building-AI-AndrewNg.html)
  * [History of Bayesian Neural Networks (Keynote talk)](https://www.youtube.com/watch?v=FD8l2vPU5FY&feature=youtu.be)
* [Nuts and Bolts of Building Deep Learning Applications: Ng @ NIPS2016](http://www.computervisionblog.com/2016/12/nuts-and-bolts-of-building-deep.html)
* Fast and Provably Good Seedings for k-Means
  * [paper](http://papers.nips.cc/paper/6478-fast-and-provably-good-seedings-for-k-means.pdf)
  * [code](https://github.com/obachem/kmc2)
  * [slide](http://olivierbachem.ch/files/afkmcmc-oral-pdf.pdf)
  * [spotlight](https://youtu.be/QtQyeka-tlQ)
  * k-means 클러스터링에서 k-means++으로 초기 클러스터를 정하게 되면 O(log k)번 안에 최적의 클러스터로 수렴하는것이 증명되었지만 초기 O(nkd) 연산이 필요해 대용량 데이터에서는 적합하지 않은 문제가 있음
  * 기존의 k-means++보다 수백배 빠르면서도 결과가 근사한 Assumption-free K-MC^2 를 제안
  * 파이썬 패키지로 배포되어 있으며 scikit-learn에서도 바로 사용 가능
* [DeepMind Papers @ NIPS (Part 1)](https://deepmind.com/blog/deepmind-papers-nips-part-1/)
* [DeepMind Papers @ NIPS (Part 2)](https://deepmind.com/blog/deepmind-papers-nips-part-2/)
* [DeepMind Papers @ NIPS (Part 3)](https://deepmind.com/blog/deepmind-papers-nips-part-3/)
* [Repo. for NIPS 2016 papers](https://tensorflow.blog/2016/12/15/repo-for-nips-2016-papers/)
* [Bayesian Deep Learning NIPS 2016 Workshop](http://bayesiandeeplearning.org/#schedule)
* [Magenta wins "Best Demo" at NIPS 2016!](https://magenta.tensorflow.org/2016/12/16/nips-demo/)
  * [Magenta AI Jam Session](https://www.youtube.com/watch?v=QlVoR1jQrPk&feature=youtu.be)
  * [Magenta: Music and Art Generation with Machine Intelligence](https://github.com/tensorflow/magenta)
  * [Interactive Musical Improvisation with Magenta (NIPS 2016)](https://github.com/tensorflow/magenta/tree/master/magenta/demos/NIPS_2016)
* [DeepMind Lab (DeepMind)](https://deepmind.com/blog/open-sourcing-deepmind-lab)
* [[모두의연구소] 쫄지말자딥러닝](http://www.slideshare.net/modulabs/ss-62503747)
* [쫄지말자딥러닝2 - CNN RNN 포함버전](http://www.slideshare.net/modulabs/2-cnn-rnn)
* [www.modulabs.co.kr/DeepLAB](http://www.modulabs.co.kr/DeepLAB)
  * [딥러닝연구실](http://whydsp.org/m/post?categoryId=525022) 과거 자료
* [Bayesian Deep Learning](http://twiecki.github.io/blog/2016/06/01/bayesian-deep-learning/)
* [Bayesian Machine Learning, Explained](http://www.rightrelevance.com/search/articles/hero?article=5f8cc010177776a7f4d48089ec4e539dc42a1ff9)
* [Five Hundred Deep Learning Papers, Graphviz and Python](http://dnlcrl.github.io/projects/2015/10/10/500-deep-learning-papers-graphviz-python.html?imm_mid=0dd0f3&cmp=em-data-na-na-newsltr_20151202)
* [What My Deep Model Doesn't Know...](http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html)
* [Deep Learning with Eigenvalue Decay Regularizer](https://www.researchgate.net/publication/301648136_Deep_Learning_with_Eigenvalue_Decay_Regularizer)
* [Deep Network with Stochastic Depth](https://www.evernote.com/shard/s462/sh/2de09526-e8fe-48d9-90da-9baa356d5e1a/7a4259299b26c41d60e05e894dbbc2fa)
* [Prof. Geoff Hinton - Deep Learning](https://www.youtube.com/watch?v=VhmE_UXDOGs&feature=youtu.be)
* [Autoencoders](http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/)
* [인공 신경망에 관한 설명. 스탠포드 대학 앤드류 응 교수의 sparse autoencoder 정리 노트로 인공신경망 이해하기](http://woongheelee.com/m/entry/%EC%9D%B8%EA%B3%B5-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%97%90-%EA%B4%80%ED%95%9C-%EC%84%A4%EB%AA%85-%EC%8A%A4%ED%83%A0%ED%8F%AC%EB%93%9C-%EB%8C%80%ED%95%99-%EC%95%A4%EB%93%9C%EB%A5%98-%EC%9D%91-%EA%B5%90%EC%88%98%EC%9D%98-sparse-autoencoder-%EC%A0%95%EB%A6%AC-%EB%85%B8%ED%8A%B8%EB%A1%9C-%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0)
* [What're the differences between PCA and autoencoder?](http://stats.stackexchange.com/questions/120080/whatre-the-differences-between-pca-and-autoencoder)
* [What is variational autoencoder?](http://nolsigan.com/blog/what-is-variational-autoencoder/) VAE
* [1. 오토인코더(Sparse Autoencoder) 1 – AutoEncoders & Sparsity](http://solarisailab.com/archives/113?ckattempt=1)
* [A Gentle Autoencoder Tutorial (with keras)](http://www.birving.com/presentations/autoencoders/index.html#/)
* [Variational Auto-Encoder for MNIST](https://github.com/hwalsuklee/tensorflow-mnist-VAE)
* [CS 7931: Deep Learning Seminar](http://ml.cs.utah.edu/deep-learning/)
* [Stanford Seminar - Song Han of Stanford University](https://www.youtube.com/watch?v=hfFkS_vHslI&feature=youtu.be)
  * ["Techniques for Efficient Implementation of Deep Neural Networks," a Presentation from Stanford](http://www.slideshare.net/embeddedvision/techniques-for-efficient-implementation-of-deep-neural-networks-a-presentation-from-stanford)
* [Deep Learning, Tools and Methods workshop](https://portal.klewel.com/watch/webcast/deep-learning-tools-and-methods-workshop/)
* [How to Start Learning Deep Learning](http://www.kdnuggets.com/2016/07/start-learning-deep-learning.html)
* [Summary of Deep Learning Environments](https://www.facebook.com/notes/239472486233783/Summary%20of%20Deep%20Learning%20Environments/587130401467988/)
* [Deep learning tutorials (2nd ed.)](https://github.com/sjchoi86/dl_tutorials)
* [Deep Learning Tutorial 4th](https://github.com/sjchoi86/dl_tutorials_4th)
* [Deep Learning for Everyone – and (Almost) Free](http://www.datasciencecentral.com/profiles/blogs/deep-learning-for-everyone-and-almost-free)
* [Deep Learning for Object Detection with DIGITS](https://devblogs.nvidia.com/parallelforall/deep-learning-object-detection-digits/)
* [khanrc.tistory.com/category/DataScience/Deep Learning](http://khanrc.tistory.com/category/DataScience/Deep%20Learning)
* [도커와 AWS를 활용한 클라우드 딥러닝 환경 구축](https://gist.github.com/haje01/f13053738853f39ce5a2)
  * [Decoupled Neural Interfaces using Synthetic Gradients[1608.05343] Summary](https://tensorflowkorea.wordpress.com/2016/08/22/decoupled-neural-interfaces-using-synthetic-gradients1608-05343-summary/)
* [Initialization Of Deep Networks Case of Rectifiers](http://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/)
* [ANN 구현하고 x^2 근사함수 찾기](https://github.com/dgtgrade/HumanLearning/blob/master/1001.py)
  * Universal Approximation Theorem 에 따르면 간단한 ANN으로도 가능
  * 구현된 간단한 ANN
    * 입력 레이어: 노드 1개 
      * x 그대로임. 즉, feature 로서 다항식이나 비선형 함수 사용하지 않음
    * 히든 레이어: 1개, 노드: 50개
      * a = sigmoid(wx + b1)
    * 출력 레이어: 노드 1개
      * o = wa + b2
    * 코스트 함수: Squared Error
    * 히든 레이어가 하나라서 DNN이라고 적지 않음
    * ANN 중 가장 표준적이고 기초적이라고 할 수 있는 ANN 그대로, 또는 그중에서도 가장 간단한 형태라고 보면 됨
  * 실험 결과
    * t(x)=x^2 함수: x=[1.0~12.0]까지 학습 시켰는데 잘 되었음
    * t(x)=sin(x) 함수: x=[1.0~10.0]까지 학습 시켰는데 잘 되었음
    * t(x)=x^3+3*sin(x)^2-10 함수: x=[-5.5~5.5]까지 학습 시켰는데 잘 되었음
    * 히든 노드수를 늘릴 수록 더 넓은 범위의 x 값을 커버할 수 있음을 일정 범위 내에서 확인
  * [코드](https://github.com/dgtgra…/HumanLearning/blob/master/1001.py)
    * numpy 외에 아무런 라이브러리도 사용하지 않았음
    * Back Propagation 외에 요구되는 배경 지식은 없음
    * python 3.5 환경에서 작성 하였고, numpy만 있으면 실행 됨
  * [실행 동영상](https://www.facebook.com/dgtgrade/videos/1161340170591514/)
    * iteration: 학습 회수
    * train: 학습 데이터
    * test: 학습되지 않은 데이터
    * x: 입력
    * h: 학습된 네트워크의 결과
    * y: 정답 출력
    * cost: squared error
* "손으로 아무렇게나 그린 함수"를 "초간단 ANN으로 근사 시키기"; Universal Approximation Theorem의 내용 직접 실험
  * [Human Learning #1003 : Visual Test of Universal Approximation Theorem]((https://www.youtube.com/watch?v=SahmdQs6X74&list=PLefQdA1SdkhtRUuN_D3PdxaR2XTGQw8Ph&index=9)
  * ANN; 지난 글에서와 마찬가지 초간단 (Deep도 아닌) ANN
    * 히든 레이어 1개, 히든 노드 100개
    * 입력 레이어: 노드 1개 
      * x 그대로임. 즉, feature 로서 다항식이나 비선형 함수 사용하지 않음
    * 히든 레이어: 1개, 노드: 100개
      * a = sigmoid(w1*x + b1)
    * 출력 레이어: 노드 1개
      * o = w2*a + b2
    * 코스트 함수: Squared Error
  * [코드](https://github.com/dgtgra…/HumanLearning/blob/master/1003.py)
    * Artificial Neural Network의 기초, Gradient Descent의 기초, Back Propagation의 기초
    * python에서 매트릭스 다루는 법: numpy
    * python에서 그래프 그리는 법: matplotlib 
    * python에서 이미지 읽는 법: skimage
    * Learning Rate 변경에 따른 학습 능력의 변화, 히든 노드수 변경에 따른 학습 능력의 변화
  * 실행환경
    * python3.5 및 numpy
    * [처음부터 새로 설치 하려면 다음 영상을 참고](https://www.youtube.com/watch?v=pMkwjXFZdH4)
  * 실행방법
    * python 1003.py [이미지파일경로]
    * 이미지 파일 경로에 data/1003_plot1.png 등을 넣어주면 됨
    * [예제 이미지 git 페이지](https://github.com/dgtgrade/HumanLearning/tree/master/data)
* [Universal Approximation Theorem](https://en.wikipedia.org/wi…/Universal_approximation_theorem)
* [Universal Approximation Theorem](http://www.slideshare.net/theeluwin/universal-approximation-theorem-70937339)
  * 수식 t(x)는 아무 곡선이나 임의로 그려 보기 위해서 사용한 도구일 뿐인 것으로 이해해야 함
  * 즉, 수식 t(x)의 식이 중요한 것은 아님. 수식 t(x)의 내용을 문제 출제자도 모르는 상태에서 아무렇게나 (물론 함수로 표현은 가능하게) 곡선들을 그려넣으면 단순한 ANN으로도 언제나 그 곡선이 표현 가능
  * 즉 그 곡선에 거의 딱 맞는 함수 t(x)를 (사람은 모르고, 아마 만들 낼수도 없어도) 기계가 (단순 함수 f1, f2 등의 조합으로) 만들수 낼수 있다는 것을 실험해 본 것
  * 그렇기 때문에 학습한 범위 밖의 t(x)를 추정 할수 있느냐 없느냐는 여기서는 중요하지 않음
  * 왜냐하면 수식 t(x)는 범위 안의 값을 그리기 위해서 사용한 도구였으므로 사실 t(x)가 아니라 (범위 안의 출력만 일치 한다면) 수식 t2(x) 또는 t3(x)였어도 상관이 없음
  * ANN은 머신러닝을 통해서 수식 t(x)가 아니라 수식 t10(x)를 만들어 낸 것
  * 자율운전의 이상적인 정답 함수 세트 T(x)가 존재 한다면 그 함수들은 사람이 수식으로 쓸수는 없으나(!) (운전 데이터를 통해서) 곡선 그림 t(x)를 그려주면 기계가 머신러닝으로 근사함수 h(x)를 만들어 낼수도 있지 않을까? 하는 것을 보여주는 실험
  * 바둑의 이상적인 정답 함수 세트 T(x) 또한 사람은 그 함수들의 수식을 정리할 능력이 없지만 기초 학습을 위한 그림 t(x)는 (기보 데이터를 통해서) 그려줄수는 있고 알파고는 우선 그 사람이 그려준 그림 t(x)에 근사하는 h(x)를 머신러닝을 통해서 만들어 보는 것으로 시작
  * 그 근사가 어느 정도 완료된 이후엔 자기 h1(x) vs 자기 h2(x) 싸움을 통한 학습에 들어가므로 t(x)는 불필요
  * 실험 실행 영상; 다음 3가지 함수에 대하여 실험한 영상 첨부
    * x^2
    * 8*x^2-X^3
    * 10*sin(X)+(X-4)^2-10
    * 초록색 선: 실제 함수
    * 파란색 점: 학습용 정답 데이터
    * 빨간색 점: 학습 결과 만들어진 근사 함수의 출력 데이터
  * [코드](https://github.com/dgtgra…/HumanLearning/blob/master/1002.py)
  * ANN; 지난 글에서와 마찬가지 초간단 (Deep도 아닌) ANN이고, 히든 노드만 100개로 변경함
    * 입력 레이어: 노드 1개 
      * x 그대로임. 즉, feature 로서 다항식이나 비선형 함수 사용하지 않음
    * 히든 레이어: 1개, 노드: 100개
      * a = sigmoid(w1*x + b1)
    * 출력 레이어: 노드 1개
      * o = w2*a + b2
    * 코스트 함수: Squared Error
  * 실행 환경 준비; python3.5, numpy, matplotlib [설치](https://www.youtube.com/watch?v=pMkwjXFZdH4)
  * 목표 함수 t에 따라서 사람이 조정해야 하는 값
    * Learning Rate: 너무 작게 하면 학습이 느리고, 너무 크게 하면 학습이 안 됨
    * 히든 노드수: 너무 적으면 학습이 불가능할테고, 너무 많으면 학습이 느려짐
  * [머신 러닝이란 무엇일까?](https://www.youtube.com/watch?v=3vcG61VC90c)
* [research.artifacia.com](http://research.artifacia.com/)
* [Source Code Classification Using Deep Learning](http://blog.aylien.com/source-code-classification-using-deep-learning/)
* [Deep Learning Cases: Text and Image Processing](http://www.slideshare.net/grigorysapunov/deep-learning-cases-text-and-image-processing)
* [Introduction to Deep Learning part 1](https://www.youtube.com/watch?v=hoN1mnUBUyI)
* [Introduction to Deep Learning part 2](https://www.youtube.com/watch?v=E71SNUqi2cw)
* [An introduction to Deep Learning by Breandan Considine](https://speakerdeck.com/breandan/an-introduction-to-deep-learning)
* [딥러닝의 인공지능 수단으로서의 성격과 방향](http://www.slideshare.net/neuralix/deep-learning-aswaveextractor)
* [CM 세미나](https://www.youtube.com/playlist?list=PLzWH6Ydh35ggVGbBh48TNs635gv2nxkFI)
* [Deep Learning in real world @Deep Learning Tokyo](http://www.slideshare.net/pfi/deep-learning-in-real-world-deep-learning-tokyo)
* [Deep learning tutorials](https://github.com/sjchoi86/dl-workshop)
* [Bay Area DL School Live Stream!](https://tensorflowkorea.wordpress.com/2016/09/24/bay-area-dl-school-live-stream/)
  * [Bay Area Deep Learning School Day 1 at CEMEX auditorium, Stanford](https://www.youtube.com/watch?v=eyovmAtoUx0&feature=youtu.be)
  * [Bay Area Deep Learning School Day 2 at CEMEX auditorium, Stanford](https://www.youtube.com/watch?v=9dXiAecyJrY&feature=youtu.be)
* [Deep Generative Models](http://www.slideshare.net/MijungKim9/deep-generative-models)
* [Generative Model 101](https://www.facebook.com/SKTBrain/posts/313726382331516) 실제와 유사한 음악이나 이미지를 만들어내는 "Generative Model" 주요 논문 정리
* [Deep Advances in Generative Modeling](https://www.youtube.com/watch?v=KeJINHjyzOU)
* [Nuts and Bolts of Applying Deep Learning: Tips and Tricks by Andrew Ng](https://bigdatascientistblog.wordpress.com/2016/09/26/nuts-and-bolts-of-applying-deep-learning-tips-and-tricks-by-andrew-ng/)
* [Deep Learning Frameworks](https://developer.nvidia.com/deep-learning-frameworks) 주요 프레임워크들의 설치를 쉽게 안내하는 엔비디아 페이지
* [Comparison of deep learning software](https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software) 위키피디아의 방대한 딥러닝 프레임워크 비교 표
* [Comparison of deep learning software/Resources](https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software/Resources) 위에서 커버되지 않은 최신 프레임워크들
* [A Look at Popular Machine Learning Frameworks](http://redmonk.com/fryan/2016/06/06/a-look-at-popular-machine-learning-frameworks/) 프레임워크들의 깃허브와 스택오버플로에서의 관심도 차이
* [DEEP LEARNING Your daily dose of Deep learning](http://www.notey.com/blogs/deep-learning) 딥러닝에 대한 기사
* [The Next Wave of Deep Learning Architectures](http://www.nextplatform.com/2016/09/07/next-wave-deep-learning-architectures/) 이후 딥러닝 HW에 대한 전망 (2016년 3Q 기준)
* [Reward Augmented Maximum Likelihood for Neural Structured Prediction](http://static.googleusercontent.com/…/pubs/archive/45580.pdf)
  * reinforcement learning에서의 아이디어를 가져와 maximum likelihood objective를 확장해 training data로부터 추가적인 데이터를 샘플링
  * 결과적으로 알고리즘은 간단한 데이터 전처리에 불과한, Speech recognition과 neural machine translation 모두에 있어서 상당한 성능의 향상
  * reinforcement learning과 supervised learning의 아이디어가 결합. structured prediction에서 전통적인 기계학습의 아이디어와 신경망이 결합해 좋은 결과를 가져옴
* [Deep Learning Reading Group: SqueezeNet](http://www.kdnuggets.com/2016/09/deep-learning-reading-group-squeezenet.html)
* [youtube.com/user/dvbuntu/featured Self-drinving car Transfer Learning Model](https://www.youtube.com/user/dvbuntu/featured)
* [Uncertainty in Deep Learning (PhD Thesis)](http://mlg.eng.cam.ac.uk/yarin/blog_2248.html)
* [Tensor Physics for Deep Learning](http://www.slideshare.net/uspace/tensor-physics-for-deep-learning)
* [Deep Visualization Toolbox](https://www.youtube.com/watch?v=AgkfIQ4IGaM)
* [DEVIEW 2016](https://deview.kr/2016/schedule) 딥러닝/머신러닝 관련 슬라이드
  * [통역하는 앵무새 파파고 이야기](http://www.slideshare.net/deview/134papago)
  * [딥러닝을 이용한 지역 컨텍스트 검색](http://www.slideshare.net/deview/221-67605830)
  * [딥러닝을 활용한 이미지 검색: 포토요약과 타임라인](http://www.slideshare.net/deview/222-20161024)
  * [딥러닝과 강화 학습으로 나보다 잘하는 쿠키런 AI 구현하기](http://www.slideshare.net/deview/ai-67608549)
    * [[리뷰] DEVIEW : 쿠키런 AI 구현하기](https://mingrammer.com/review-deview-cookierun-ai)
    * [발표 자료](http://www.slideshare.net/carpedm20/ai-67616630)
    * [데모 영상](https://www.youtube.com/watch?v=exXD6wJLJ6s)
    * [첫번째 시도 (Deep Q-Network)](https://youtu.be/XsJWbAd6rYk)
    * [두번째 시도 (+ Double Q-learning)](https://youtu.be/rurJICmHfHQ)
    * [세번째 시도 (+ Dueling Network)](https://youtu.be/XQJA1Rob0ng)
    * [Deep Q-learning](https://github.com/devsisters/DQN-tensorflow/)
    * [Dueling Newtork, Double Q-learning](https://github.com/carpedm20/deep-rl-tensorflow/)
    * [쿠키런과 같은 discrete action space가 아닌 continuous action space에서의 강화 학습 방법](https://github.com/carpedm20/NAF-tensorflow/)
  * [Backend 개발자의 Neural Machine Translation 개발기](http://www.slideshare.net/deview/224-backend-neural-machine-translation-67608580)
  * [YARN 기반의 Deep Learning Application Cluster 구축](http://www.slideshare.net/deview/225yarn-deep-learning-application-cluster)
  * [Multimodal Residual Learning for Visual Question-Answering](http://www.slideshare.net/deview/multimodal-residual-learning-for-visual-questionanswering)
  * [딥러닝 예제로 보는 개발자를 위한 통계](http://www.slideshare.net/deview/216-67609104)
  * [Deep Recurrent Neural Network를 이용한 대용량 텍스트 마이닝 기술 및 실제 응용사례](http://www.slideshare.net/deview/226-67609105)
  * [빅데이터 분석에 적합한 LDA & HDP 베이지안 토픽모형에 대한 알고리즘](http://www.slideshare.net/deview/214-67608573)
* [Deep Learning is Revolutionary - 10 reasons why deep learning is living up to the hype](https://medium.com/@olivercameron/deep-learning-is-revolutionary-d0f3667bafa0)
* [Intelligence Platform Stack](https://medium.com/@surmenok/intelligence-platform-stack-8c623f71f990)
* [UFLDL Tutorial](http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial)
* [Batch Normalization 설명 및 구현](https://shuuki4.wordpress.com/2016/01/13/batch-normalization-%EC%84%A4%EB%AA%85-%EB%B0%8F-%EA%B5%AC%ED%98%84/)
* [Deep Learning - Achieve faster training of deep neural networks on a robust, scalable infrastructure](https://software.intel.com/en-us/ai/deep-learning)
* [CPU, GPU Put to Deep Learning Framework Test](https://www.nextplatform.com/2016/09/01/cpu-gpu-put-deep-learning-framework-test/)
* [딥러닝의 역사와 기본 개념](http://bcho.tistory.com/1147)
* [김현호: 오늘 당장 딥러닝 실험하기 - PyCon Korea 2015](https://www.youtube.com/watch?v=j-CojQwIt70)
* [Nuts and Bolts of Applying Deep Learning](https://kevinzakka.github.io/2016/09/26/applying-deep-learning/)
* [한눈에 보는 실리콘밸리 AI 트렌드(2)](https://brunch.co.kr/@synabreu/13)
* [The major advancements in Deep Learning in 2016](https://tryolabs.com/blog/2016/12/06/major-advancements-deep-learning-2016/)
* [Deep Learning Into Advance - 1. Image, ConvNet](http://www.slideshare.net/hellovista/deep-learning-into-advance-1-image-convnet)
* [Deep Learning SIMPLIFIED](https://www.youtube.com/playlist?list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu)
* [Intel® Distribution for Python for high performance to supercharge all your Python applications on modern Intel platforms](https://software.intel.com/en-us/intel-distribution-for-python)
* [Deep Learning Demystified](https://www.youtube.com/watch?v=Q9Z20HCPnww&spfreload=10)
* [Recognizing Sounds (A Deep Learning Case Study)](https://medium.com/@awjuliani/recognizing-sounds-a-deep-learning-case-study-1bc37444d44d)
* [deeplearning-papernotes](https://github.com/nolsigan/deeplearning-papernotes)
* [Has Deep Learning Made Traditional Machine Learning Irrelevant?](http://www.datasciencecentral.com/profiles/blogs/has-deep-learning-made-traditional-machine-learning-irrelevant)
* [Feedback Networks](https://youtu.be/MY5Uhv38Ttg)
  * [paper](https://arxiv.org/abs/1612.09508)
  * 영상분야 Deep Learning에서 일반적인 학습 모델은 연속적인 ConvNets layers를 이용하여 Feature를 추출한 다음 classification layer가 이어지는 모델을 기반
  * 본 논문에서는 이러한 일반적인 Feedfoward Multi Layers 대신 동일한 목표를 달성 할 수있는 대안을 제시
  * Recurrent Neural Networks의 개념을 도입하여 이전 출력에서 받은 피드백을 기반으로 반복적으로 표현이 형성되는 Feedback 기반 접근 방식을 제시
  * Feedback 기반 접근 방식은 Feedfoward보다 몇 가지 장점
    * 연산과정 중 초기에 예측 가능
    * 출력은 자연스럽게 레이블 공간의 계층 구조 (예 : 분류법)를 따르며 커리큘럼의 새로운 기초를 제공
    * Feedback 네트워크는 이러한 장점외에 Feedfoward 대응 네트워크와 비교하여 상당히 다른 표현의 개발이 가능(Feedback architecture (예 : skip connections in time) 및 디자인 선택 (예 : 피드백 길이))
* [A Theory Explains Deep Learning](http://www.deductiontheory.com/2016/12/study-deep-learning-from-scratch.html)
* [Numerical Gradient Descent vs. BackPropagation](https://github.com/dgtgrade/HumanLearning/blob/master/1102.py)
* [20170121 한국인공지능협회 - 제7차 오픈세미나 - 딥러닝](https://www.youtube.com/watch?v=FtHwOo5aICI)
  * [딥러닝 살짝 보기](https://docs.google.com/presentation/d/1K7imkoZy0drztv5_ylP8ZajuM_lUO9wk_nlhp9Z1-vQ/)
* [DeepMind just published a mind blowing paper: PathNet.](https://medium.com/@thoszymkowiak/deepmind-just-published-a-mind-blowing-paper-pathnet-f72b1ed38d46)
* [Bringing HPC Techniques to Deep Learning](http://research.baidu.com/bringing-hpc-techniques-deep-learning/)
  * 여러대 GPU머신을 이용하여 parallel하게 학습할 때 네트웍 오버헤드 때문에 오히려 속도가 감소
  * 바이두에서 ring allreduce라는 알고리즘으로 해결
* [実世界の人工知能@DeNA TechCon 2017](https://www.slideshare.net/pfi/dena-techcon-2017)
* [Deep Forest: Towards An Alternative to Deep Neural Networks](https://arxiv.org/abs/1702.08835)
* **[Asynchronous Advantage Actor-Critic (A3C)](https://jay.tech.blog/2017/01/19/asynchronous-advantage-actor-critic-a3c/)**
* [스스로 코딩을 하는 인공지능의 현 주소-Deepcoder](http://etinow.me/187)
* [Improving Hardware Efficiency for DNN Applications](https://www.slideshare.net/ChesterChen/improving-hardware-efficiency-for-dnn-applications)
* [DeepLearning 연구 2016 년의 정리](https://translate.google.com/translate?sl=ja&tl=ko&js=y&prev=_t&hl=ko&ie=UTF-8&u=http%3A%2F%2Fqiita.com%2Feve_yk%2Fitems%2Ff4b274da7042cba1ba76&edit-text) 일본어 번역
* [Squeezing Deep Learning Into Mobile Phones](https://www.slideshare.net/anirudhkoul/squeezing-deep-learning-into-mobile-phones)
* [An overview of gradient descent optimization algorithms](http://sebastianruder.com/optimizing-gradient-descent/)
* [딥러닝 분산처리 기술동향](https://ettrends.etri.re.kr/ettrends/pubreader.do?volume=31&issue=3&page=131&paperno=0905002137)
* [Understanding deep learning requires rethinking generalization (2017) 1/2](https://www.slideshare.net/JungHoonSeo2/understanding-deep-learning-requires-rethinking-generalization-2017-12)
* [Understanding deep learning requires rethinking generalization (2017) 2 2(2)](https://www.slideshare.net/JungHoonSeo2/understanding-deep-learning-requires-rethinking-generalization-2017-2-22)
* [Deep Learning From A to Z (Raphael Gontijo Lopes)](https://www.youtube.com/watch?v=DYlHnxfrrZY)
* [the morning paper](https://blog.acolyer.org/)
* [Linear algebra cheat sheet for deep learning](https://medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c)
* [The Black Magic of Deep Learning - Tips and Tricks for the practitioner](https://nmarkou.blogspot.com/2017/02/the-black-magic-of-deep-learning-tips.html)
* [Game Theory reveals the Future of Deep Learning](https://medium.com/intuitionmachine/game-theory-maps-the-future-of-deep-learning-21e193b0e33a)
* [todaysdeeplearning.com](http://www.todaysdeeplearning.com/)
* [Try Deep Learning in Python now with a fully pre-configured VM](https://medium.com/@ageitgey/try-deep-learning-in-python-now-with-a-fully-pre-configured-vm-1d97d4c3e9b) VMWare

# AlphaGo
* [Rochester-NRT/AlphaGo](https://github.com/Rochester-NRT/AlphaGo)
* [AlphaGo의 인공지능 알고리즘 분석](http://spri.kr/post/14725)
* [AlphaGo 알고리즘 요약](http://www.slideshare.net/zenithon/alphago?from_m_app=android)
* [알파고 (바둑 인공지능)의 작동 원리](http://www.slideshare.net/ShaneSeungwhanMoon/ss-59226902)
* [이세돌과 대국으로 ‘알파고’ 설계자가 꿈꾸는 것은?](http://www.bloter.net/archives/251528)
* [모두의 알파고](http://www.slideshare.net/DonghunLee20/ss-59338971)
* [Mastering the game of Go with deep neural networks and tree search](http://www.willamette.edu/~levenick/cs448/goNature.pdf)
* [AlphaGo에 적용된 딥러닝 알고리즘 분석](https://brunch.co.kr/@justinleeanac/2)
* [알파고는 어떻게 바둑을 둘까](https://brunch.co.kr/@madlymissyou/9)
* [이세돌이 알파고를 이기려면 선입견을 버려야 한다](http://m.blog.daum.net/_blog/_m/articleView.do?blogid=0NovT&articleno=3331)
* [바둑인을 위한 알파고](http://www.slideshare.net/DonghunLee20/ss-59413793)
* [알파고 해부하기 1부](http://www.slideshare.net/DonghunLee20/1-59501887)
* [알파고 해부하기 2부](http://www.slideshare.net/DonghunLee20/2-59620244)
* [알파고 해부하기 3부](http://www.slideshare.net/DonghunLee20/3-61454159)
* [알파고, 강화학습을 현실에 데뷔시키다](http://t-robotics.blogspot.co.id/2016/03/blog-post_26.html)
* [알파고는 어떤 컴퓨터를 썼을까?](http://www.slideshare.net/jysoo/ss-61950212)
* [AlphaGo 대국 - 한국어](https://deepmind.com/research/alphago/alphago-games-korean/)

# Amazon
* [Amazon DSSTNE: Deep Scalable Sparse Tensor Network Engine](https://github.com/amznlabs/amazon-dsstne)

# Backpropagation
* [Neural Networks: The Backpropagation algorithm in a picture](http://www.datasciencecentral.com/profiles/blogs/neural-networks-the-backpropagation-algorithm-in-a-picture)
* **[Backpropagation 예제와 함께 완전히 이해하기](http://jaejunyoo.blogspot.com/2017/01/backpropagation.html)**
* [A Derivation of Backpropagation in Matrix Form](http://sudeepraja.github.io/Neural/)
* **[Calculus on Computational Graphs: Backpropagation](http://colah.github.io/posts/2015-08-Backprop/index.html)**
* [Gradient Descent with Backpropagation](http://outlace.com/Beginner-Tutorial-Backpropagation/)
* [A Step by Step Backpropagation Example](http://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)
* [A Step by Step Backpropagation Example](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)

# Baidu
* [Silicon Valley AI Lab](https://svail.github.io/)

# Book
* [[eBook] 머신러닝에서 딥러닝까지](http://digital.kyobobook.co.kr/digital/ebook/ebookDetail.ink?selectedLargeCategory=001&barcode=480150001023P&orderClick=LAN&Kc)
* [C++와 CUDA C로 구현하는 딥러닝 알고리즘 Vol.1 [Restricted Boltzman Machine의 이해와 Deep Belief Nets 구현]](http://www.acornpub.co.kr/book/dbn-cuda-vol1)
* [Deep Learning 이론과 실습 (개정중)](https://wikidocs.net/book/498)
* [Deep Learning - A Practitioner's Approach](http://shop.oreilly.com/product/0636920035343.do)
* [Fundamentals of Deep Learning](http://shop.oreilly.com/product/0636920039709.do)
  * [‘Fundamental of Deep Learning’ Preview](https://tensorflowkorea.wordpress.com/2016/04/18/fundamental-of-deep-learning-preview/#more-2018)
* [Deep Learning - An MIT Press book in preparation](http://www.deeplearningbook.org/)
  * [DeepLearningBook](https://github.com/HFTrader/DeepLearningBook)
* [Book: Deep Learning With Python](http://www.datasciencecentral.com/forum/topics/book-deep-learning-with-python) Theano and TensorFlow using Keras
* [Deep Learning With Python](https://machinelearningmastery.com/deep-learning-with-python/)
* [Fundamental of Reinforcement Learning](https://dnddnjs.gitbooks.io/rl/content/)
* [Reinforcement Learning: An Introduction](https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf)
  * [교과서 읽고 느낀점](http://blog.naver.com/atelierjpro/220896756412)
* [도서 마인드맵](https://www.mindmeister.com/812276967/_)
* [Artificial Intelligence Book of January 2017](http://artificialbrain.xyz/artificial-intelligence-book-of-january-2017/)
* [Deep-Learning-for-Beginners - Sample code in MATLAB/Octave and Python for Deep Learning for Beginners](https://github.com/philbooks/Deep-Learning-for-Beginners)
* [Free Deep Learning Textbook](http://www.datasciencecentral.com/profiles/blogs/free-deep-learning-textbook)

# Deep Q Learning DQL
* [Deep Q-Learning (Space Invaders)](http://maciejjaskowski.github.io/2016/03/09/space-invaders.html)
* [Using Deep Q-Network to Learn How To Play Flappy Bird](https://github.com/DeepLearningProjects/DeepLearningFlappyBird)
* [Hello DeepQ](http://koaning.io/hello-deepq.html)
* [Deep Q Learning with Keras and Gym](https://keon.io/rl/deep-q-learning-with-keras-and-gym/)
* [Q-learning Test](http://computingkoreanlab.com/app/jAI/jQLearning/)

# Extreme Learning Machines
* [Extreme Learning Machines](http://www.ntu.edu.sg/home/egbhuang/pdf/IEEE-IS-ELM.pdf)
* [Basic ELM Algorithms](http://www.ntu.edu.sg/home/egbhuang/elm_codes.html)

# GAN Generative Adversarial Networks
* NIPS 2016 Tutorial: Generative Adversarial Networks [paper](https://arxiv.org/pdf/1701.00160v1.pdf) [slide](http://www.iangoodfellow.com/slides/2016-12-04-NIPS.pdf)
* [SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](https://arxiv.org/pdf/1609.05473v2.pdf)
  * GAN이 처음으로 sequence generation task에 사용
  * GAN은 진짜같은 Fake data를 만들어내는 Generator과 진짜 data와 Fake data를 구분해내는 Discriminator를 학습시키는 알고리즘
  * 실수 픽셀들로 이루어진 그림과 달리 discrete한 토큰들의 sequence를 생성해낼 때 현재 얼마나 Generator가 잘 학습을 하고 있는지 평가할 방법이 마땅치 않아 sequence generation task에서는 사용되지 않음
  * 이번에 발표된 SeqGAN 은 discriminator를 Policy Gradient 의 Reward 로 사용해서 이 문제를 해결, Text Generation, Music Generation Task 에 적용
  * [Ian Goodfellow (GAN 저자) 의 Reddit 문답(왜 NLP에 GAN이 사용되기 힘든가)](https://www.reddit.com/r/MachineLearning/comments/40ldq6/generative_adversarial_networks_for_text/)
* [번역 - Generative Adversarial Network (GAN) 설명](http://keunwoochoi.blogspot.com/2016/12/generative-adversarial-network-gan.html)
* [GANs will change the world](https://medium.com/@Moscow25/gans-will-change-the-world-7ed6ae8515ca)
* [A tensorflow implementation of Junbo et al's Energy-based generative adversarial network ( EBGAN ) paper](https://github.com/buriburisuri/ebgan)
* [초짜 대학원생 입장에서 이해하는 Generative Adversarial Nets (1)](http://jaejunyoo.blogspot.com/2017/01/generative-adversarial-nets-1.html)
* [초짜 대학원생 입장에서 이해하는 Generative Adversarial Nets (2)](http://jaejunyoo.blogspot.com/2017/01/generative-adversarial-nets-2.html)
* **[Generative adversarial networks](http://www.slideshare.net/ssuser77ee21/generative-adversarial-networks-70896091)**
* [Generative Adversarial Networks](https://github.com/nlintz/TensorFlow-Tutorials/blob/master/11_gan.ipynb)
* [PaintsChainer Demo](http://paintschainer.preferred.tech/)
  * [PaintsCahiner Code](https://github.com/pfnet/PaintsChainer)
  * [tai2an 본인이 올린 글](http://qiita.com/taizan/items/cf77fd37ec3a0bef5d9d)
  * [U-Net](http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)
* [초짜 대학원생의 입장에서 이해하는 Domain-Adversarial Training of Neural Networks (DANN) (1)](http://jaejunyoo.blogspot.com/2017/01/domain-adversarial-training-of-neural.html)
* [초짜 대학원생의 입장에서 이해하는 Domain-Adversarial Training of Neural Networks (DANN) (2)](http://jaejunyoo.blogspot.com/2017/01/domain-adversarial-training-of-neural-2.html)
* [초짜 대학원생의 입장에서 이해하는 Domain-Adversarial Training of Neural Networks (DANN) (3)](http://jaejunyoo.blogspot.com/2017/01/domain-adversarial-training-of-neural-3.html)
* [tf-dann-py35 - Tensorflow-gpu (1.0.0.rc2, Window, py35) implementation of Domain Adversarial Neural Network](https://github.com/jaejun-yoo/tf-dann-py35)
* [DOMAIN ADVERSARIAL NEURAL NETWORK](https://github.com/sjchoi86/advanced-tensorflow/blob/master/dann/dann_mnist.ipynb)
* [초짜 대학원생의 입장에서 이해하는 Deep Convolutional Generative Adversarial Network (DCGAN) (1)](http://jaejunyoo.blogspot.com/2017/02/deep-convolutional-gan-dcgan-1.html)
* [초짜 대학원생의 입장에서 이해하는 Deep Convolutional Generative Adversarial Network (DCGAN) (2)](http://jaejunyoo.blogspot.com/2017/02/deep-convolutional-gan-dcgan-2.html)
* [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](http://www.slideshare.net/ssuser06e0c5/infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets-72268213)
* [Generative Adversarial Networks (GANs) in 50 lines of code (PyTorch)](https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f)
* [초짜 대학원생의 입장에서 이해하는 Unrolled Generative Adversarial Networks (1)](http://jaejunyoo.blogspot.com/2017/02/unrolled-generative-adversarial-network-1.html)
* [초짜 대학원생의 입장에서 이해하는 Unrolled Generative Adversarial Networks (2)](http://jaejunyoo.blogspot.com/2017/02/unrolled-generative-adversarial-network-2.html)
* [초짜 대학원생의 입장에서 이해하는 InfoGAN (1)](http://jaejunyoo.blogspot.com/2017/03/infogan-1.html)
* [초짜 대학원생의 입장에서 이해하는 InfoGAN (2)](http://jaejunyoo.blogspot.com/2017/03/infogan-2.html)
* [Read-through: Wasserstein GAN](http://www.alexirpan.com/2017/02/22/wasserstein-gan.html)
* [A collection of Keras GAN notebooks](https://github.com/osh/KerasGAN)
* [[AI기획]경쟁 통해 배우는 인공지능 기술 GAN](http://techm.kr/bbs/?t=Wh)
* [Generative Adversarial Networks Explained](http://www.rubedo.com.br/2017/03/generative-adversarial-networks.html)
* [겐스는 왜 기존 비창의 인공지능과 다른가?](https://www.linkedin.com/pulse/%EA%B2%90%EC%8A%A4-%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%EC%9D%80-%EC%99%9C-%EA%B8%B0%EC%A1%B4-%EB%B9%84%EC%B0%BD%EC%9D%98-%EA%B8%B0%EC%88%A0%EA%B3%BC-%EB%8B%A4%EB%A5%B8%EA%B0%80-sung-jin-james-kim)
* [Adversarial Attacks on Neural Network Policies](http://rll.berkeley.edu/adversarial/)
* [Disco GAN - SK T-Brain Research](https://www.facebook.com/notes/sk-t-brain/sk-t-brain-research/398821727155314)
  * [DiscoGAN - Official PyTorch implementation of Learning to Discover Cross-Domain Relations with Generative Adversarial Networks](https://github.com/SKTBrain/DiscoGAN)
  * [DiscoGAN in PyTorch - PyTorch implementation of Learning to Discover Cross-Domain Relations with Generative Adversarial Networks](https://github.com/carpedm20/DiscoGAN-pytorch)
  * [discogan_tensorflow.py](https://github.com/wiseodd/generative-models/blob/master/GAN/disco_gan/discogan_tensorflow.py)
  * [Tensorflow Implementation of DiscoGAN](https://github.com/GunhoChoi/DiscoGAN_TF)
  * [[논문반/논문세미나] DiscoGAN](http://www.modulabs.co.kr/DeepLAB_library/12820)
    * [Discover Cross-Domain Relations with GAN (DiscoGAN) with TensorFlow & slim](https://github.com/ilguyi/discoGAN.tensorflow.slim)
* [Generative Models - Collection of generative models, e.g. GAN, VAE in Pytorch and Tensorflow](https://github.com/wiseodd/generative-models)
  * [GAN](https://github.com/wiseodd/generative-models/tree/master/GAN)
* [Keras Adversarial Models](https://github.com/bstriner/keras-adversarial)
* [초짜 대학원생의 입장에서 이해하는 LSGAN (1)](http://jaejunyoo.blogspot.com/2017/03/lsgan-1.html)
* [초짜 대학원생의 입장에서 이해하는 LSGAN (2)](http://jaejunyoo.blogspot.com/2017/04/lsgan-2.html)
* [Generative Adversarial Networks](http://cs.stanford.edu/people/karpathy/gan/)
* [aliensunmin.github.io/project/accv16tutorial](http://aliensunmin.github.io/project/accv16tutorial/)
* [Deep Feedforward Generative Models](http://aliensunmin.github.io/project/accv16tutorial/media/generative.pdf)
* [Generative Adversarial Networks to Make 8-bit Pixel Art](http://www.rubedo.com.br/2017/03/generative-adversarial-networks-to-make.html)
* [겐(GANs)이 꿈꾸는 인공지능 번역 끝판왕](https://medium.com/@jskDr/%EA%B2%90-gans-%EC%9D%B4-%EA%BF%88%EA%BE%B8%EB%8A%94-%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%B2%88%EC%97%AD-%EB%81%9D%ED%8C%90%EC%99%95-4df872ffa13f)
* [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://junyanz.github.io/CycleGAN/)

# Keras
* [Keras: Deep Learning library for Theano and TensorFlow](http://keras.io/)
* [Deep Learning: Keras Short Tutorial](https://www.youtube.com/watch?v=Tp3SaRbql4k)
* [Keras로 Multi Layer Percentron 구현하기](http://iostream.tistory.com/111)
* [Keras_MNIST_Example.ipynb](https://github.com/dolpang2/Keras-Examples/blob/master/Keras_MNIST_Example.ipynb)
* [github.com/jaeho-kang/deep-learning/keras](https://github.com/jaeho-kang/deep-learning/tree/master/keras)
* [Keras에서 CNN학습 시키기](https://github.com/jaeho-kang/deep-learning/blob/master/keras/keras%E1%84%8B%E1%85%A6%E1%84%89%E1%85%A5_cnn%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8_%E1%84%89%E1%85%B5%E1%84%8F%E1%85%B5%E1%84%80%E1%85%B5.md)
* [Keras로 대용량 이미지 처리하기](https://github.com/jaeho-kang/deep-learning/blob/master/keras/keras%E1%84%85%E1%85%A9_%E1%84%83%E1%85%A2%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%85%E1%85%A3%E1%86%BC_%E1%84%8B%E1%85%B5%E1%84%86%E1%85%B5%E1%84%8C%E1%85%B5_%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5.md)
* [What are good resources/tutorials to learn Keras (deep learning library in Python)?](https://www.quora.com/What-are-good-resources-tutorials-to-learn-Keras-deep-learning-library-in-Python)
* [Predicting sequences of vectors (regression) in Keras using RNN - LSTM](http://danielhnyk.cz/predicting-sequences-vectors-keras-using-rnn-lstm/)
* [Binary Classification Tutorial with the Keras Deep Learning Library](http://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/)
* [Installing Keras for deep learning](http://www.pyimagesearch.com/2016/07/18/installing-keras-for-deep-learning/)
* [Regression Tutorial with the Keras Deep Learning Library in Python](http://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/)
* [keras-rl](https://tensorflowkorea.wordpress.com/2016/08/03/keras-rl/)
* [Pre-trained DL Model for Keras](https://tensorflowkorea.wordpress.com/2016/08/04/pre-trained-dl-model-for-keras/)
* [Text Generation With LSTM Recurrent Neural Networks in Python with Keras](http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/)
* [Music auto-tagging models and trained weights in keras/theano](https://github.com/keunwoochoi/music-auto_tagging-keras)
* [How convolutional neural networks see the world](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html)
* [Sequence to Sequence Learning with Keras](https://github.com/farizrahman4u/seq2seq)
* [seq2seq - A general-purpose encoder-decoder framework for Tensorflow](https://google.github.io/seq2seq/)
* [Introduction For seq2seq(sequence to sequence) and RNN](https://www.slideshare.net/HyeminAhn/introduction-for-seq2seqsequence-to-sequence-and-rnn)
* [Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras](http://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/)
* [A simple neural network with Python and Keras](http://www.pyimagesearch.com/2016/09/26/a-simple-neural-network-with-python-and-keras/)
* [Display Deep Learning Model Training History in Keras](http://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/)
* [Tutorial: Optimizing Neural Networks using Keras (with Image recognition case study)](https://www.analyticsvidhya.com/blog/2016/10/tutorial-optimizing-neural-networks-using-keras-with-image-recognition-case-study/)
* [Keras LSTMs](http://sachinruk.github.io/blog/Keras-LSTM/)
* [Keras-Tutorials - Simple tutorials using Keras Framework](https://github.com/tgjeon/Keras-Tutorials)
* [RNN 기본 구조와 Keras를 사용한 RNN 구현](https://www.datascienceschool.net/view-notebook/1d93b9dc6c624fbaa6af2ce9290e2479/)
* [Quiver: Deep Visualization for Keras](https://tensorflowkorea.wordpress.com/2016/11/15/quiver-deep-visualization-for-keras/)
* [Keras Tutorial: The Ultimate Beginner’s Guide to Deep Learning in Python](https://elitedatascience.com/keras-tutorial-deep-learning-in-python)
* [GPU-accelerated Theano & Keras on Windows 10 native](http://philferriere.blogspot.com/2016/07/gpu-accelerated-theano-keras-on-windows.html)
* [Radio and Machine Learning Zen](https://oshearesearch.com/)
* [Keras autoencoders (convolutional/fcc)](https://github.com/nanopony/keras-convautoencoder)
* [Why is Keras Running So Slow?](http://www.chioka.in/why-is-keras-running-so-slow/)
* [Elephas: Distributed Deep Learning with Keras & Spark](https://github.com/maxpumperla/elephas)
* [kapre - Keras Audio Preprocessors](https://github.com/keunwoochoi/kapre)
* [KERAS-DCGAN](https://github.com/jskDr/keras-dcgan)
* [jacobgil/dcgan.py](https://github.com/jacobgil/keras-dcgan/blob/master/dcgan.py)
* [keras-tqdm - Keras integration with TQDM progress bars](https://github.com/bstriner/keras-tqdm)
* [Keras Adversarial Models](https://github.com/bstriner/keras-adversarial)
* [Minimal Monte Carlo Policy Gradient (REINFORCE) Algorithm Implementation in Keras](https://github.com/keon/policy-gradient)
* [Intro into Image classification using Keras](https://www.youtube.com/watch?v=KhU4CGfE5m4&sns=tw)
* [케라스와 텐서플로우와의 통합](https://tykimos.github.io/Keras/2017/02/22/Integrating_Keras_and_TensorFlow/)
* [케라스 강좌 내용](https://tykimos.github.io/Keras/2017/01/27/Keras_Lecture_Contents/)
* [Keras CNN tutorial](https://byeongkijeong.github.io/Keras-cnn-tutorial/)
* [Image denoising with Autoencoder in Keras](https://byeongkijeong.github.io/Keras-Autoencoder/)
* [Keras resources - This is a directory of tutorials and open-source code repositories for working with Keras, the Python deep learning library](https://github.com/fchollet/keras-resources)
* [Building powerful image classification models using very little data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)
* [Introducing Keras 2](https://blog.keras.io/introducing-keras-2.html)
* [github.com/jskDr/jamespy_py3](https://github.com/jskDr/jamespy_py3)
* [One Shot Learning with Siamese Networks in Keras!](https://sorenbouma.github.io/blog/oneshot/)
* [Deep Learning with Emojis (not Math)](https://tech.instacart.com/deep-learning-with-emojis-not-math-660ba1ad6cdc)
* [Keras + Theano 시도: 3. 이미지 준비](http://forcecore.tistory.com/m/1333)
* [simple_rnn_encoder_decoder.py](https://github.com/skyer9/simple_rnn_encoder_decoder/blob/master/simple_rnn_encoder_decoder.py)
* [Integrating Keras & TensorFlow: The Keras workflow, expanded (TensorFlow Dev Summit 2017)](https://www.youtube.com/watch?v=UeheTiBJ0Io)

# Neural Network
* [Google's AI Chief Geoffrey Hinton - How Neural Networks Really Work](https://www.youtube.com/watch?v=l2dVjADTEDU&feature=player_embedded)
* [1. Overview of Mini Batch Gradient Descent](https://www.youtube.com/watch?v=GvHmwBc9N30&feature=share)
* [Learning How To Code Neural Networks](https://medium.com/learning-new-stuff/how-to-learn-neural-networks-758b78f2736e)
  * [뉴럴네트워크 코드 짜는 법 배우기](http://ddanggle.github.io/ml/ai/cs/2016/07/16/LearningHowToCodeNeuralNetworks.html)
* [Machine Learning 스터디 (18) Neural Network Introduction](http://sanghyukchun.github.io/74/)
* [Artificial Neural Networks for Beginners](http://blogs.mathworks.com/loren/2015/08/04/artificial-neural-networks-for-beginners/)
* [A Visual Explanation of the Back Propagation Algorithm for Neural Networks](http://www.kdnuggets.com/2016/06/visual-explanation-backpropagation-algorithm-neural-networks.html)
* [Machine Learning - Neural Networks Tutorial](http://www.existor.com/en/news-neural-networks.html)
* [A Fast and Accurate Dependency Parser using Neural Networks](http://cs.stanford.edu/~danqi/papers/emnlp2014.pdf)
* [waifu2x - Image Super-Resolution for Anime/Fan-Art](https://github.com/nagadomi/waifu2x)
* [Visualizing and Understanding Deep Neural Networks by Matt Zeiler](https://www.youtube.com/watch?v=ghEmQSxT6tw)
* [Machine-Learning Algorithm Mines Rap Lyrics, Then Writes Its Own](http://www.technologyreview.com/view/537716/machine-learning-algorithm-mines-rap-lyrics-then-writes-its-own/)
* [시인 뉴럴](http://pail.unist.ac.kr/carpedm20/poet/)
* [VGG Convolutional Neural Networks Practical](http://www.robots.ox.ac.uk/~vgg/practicals/cnn/index.html)
* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
* [Spectral Representations for Convolutional Neural Networks](http://arxiv.org/pdf/1506.03767.pdf)
* [How to implement a neural network: Part 1](http://peterroelants.github.io/posts/neural_network_implementation_part01/)
* [Inceptionism: Going Deeper into Neural Networks](http://googleresearch.blogspot.kr/2015/06/inceptionism-going-deeper-into-neural.html)
* [Quantifying Creativity in Art Networks](http://arxiv.org/pdf/1506.00711v1.pdf)
* [Neural network의 변천사 이태영](https://www.slideshare.net/secret/dzVcikxOkWg8TP)
* [ai junkie - neural networks in plain english](http://www.ai-junkie.com/ann/evolved/nnt1.html)
* [10 Billion Parameter Neural Networks in your Basement](http://on-demand.gputechconf.com/gtc/2014/presentations/S4694-10-billion-parameter-neural-networks.pdf)
* [Understanding Neural Networks Through Deep Visualization](http://yosinski.com/deepvis)
  * ["Understanding Neural Networks Through Deep Visualization" (2015), J. Yosinski et al.](http://yosinski.com/media/papers/Yosinski__2015__ICML_DL__Understanding_Neural_Networks_Through_Deep_Visualization__.pdf)
  * [github.com/yosinski/deep-visualization-toolbox](https://github.com/yosinski/deep-visualization-toolbox)
* [Interactive Deep Neural Net Hallucinations (+source code) Large Scale Deep Neural Net visualizing top level features](https://317070.github.io/Dream/)
* [Neural Network for Concrete Strength using R](http://andersonjo.github.io/neural-network/2015/07/25/Neural-Network-for-concrete/)
* [fann.js - FANN compiled through Emscripten](https://github.com/louisstow/fann.js/)
* [neurogram - Creating abstract art by evolving neural networks in Javascript](http://blog.otoro.net/2015/07/31/neurogram/)
* [Neural Network for Concrete Strength using R](http://andersonjo.github.io/neural-network/2015/07/25/Neural-Network-for-concrete/)
* [Recent Trends in Neural Net Policy Learning](http://www.slideshare.net/samchoi7/recent-trends-in-neural-net-policy-learning)
* [Hardware Guide: Neural Networks on GPUs](http://pjreddie.com/darknet/hardware-guide/)
* [neural networks by browser](http://neurovis.dataphoric.com/)
* [Scalable Bayesian Optimization Using Deep Neural Networks](http://arxiv.org/abs/1502.05700)
* **[An implementation of the paper 'A Neural Algorithm of Artistic Style'](https://github.com/kaishengtai/neuralart)**
  * [거장의 그림을 30초만에 만들다: DeepStyle](http://redtea.kr/?b=3&n=951)
* [neural-style - Torch implementation of neural style algorithm](https://github.com/jcjohnson/neural-style)
* [Comparing Artificial Artists](https://medium.com/@kcimc/comparing-artificial-artists-7d889428fce4)
* [Neural Networks, Types, and Functional Programming](http://colah.github.io/posts/2015-09-NN-Types-FP/)
* **[Implementing a Neural Network from Scratch – An Introduction](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/)**
* [Hacker's guide to Neural Networks](http://karpathy.github.io/neuralnets/)
  * **[해커가 알려주는 뉴럴 네트워크](https://tensorflowkorea.wordpress.com/2016/09/13/%ED%95%B4%EC%BB%A4%EA%B0%80-%EC%95%8C%EB%A0%A4%EC%A3%BC%EB%8A%94-%EB%89%B4%EB%9F%B4-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC/)**
* [neural-network-papers](https://github.com/robertsdionne/neural-network-papers)
* [Pedestrian detection using convolutional neural networks](http://www.diva-portal.org/smash/get/diva2:839692/FULLTEXT01.pdf)
* [Scalable Distributed DNN Training Using Commodity GPU Cloud Computing](https://drive.google.com/file/d/0B6dKRGPLFSd0UGNOYkNaSC1UZTA/view)
* [Deep Style: Inferring the Unknown to Predict the Future of Fashion](http://multithreaded.stitchfix.com/blog/2015/09/17/deep-style/)
* [DeepHear - Composing and harmonizing music with neural networks](http://web.mit.edu/felixsun/www/neural-music.html)
* [Why are Eight Bits Enough for Deep Neural Networks?](http://petewarden.com/2015/05/23/why-are-eight-bits-enough-for-deep-neural-networks/)
* [Neural Net in C++ Tutorial](https://vimeo.com/19569529)
* [A too naive approach to video compression using artificial neural networks](https://github.com/Dobiasd/articles/blob/master/a_too_naive_approach_to_video_compression_using_artificial_neural_networks.md)
* [An interactive introduction to neural network](http://neurovis.mitchcrowe.com/)
* [Understanding Natural Language with Deep Neural Networks Using Torch](http://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch)
* [Skynet for Beginners - Using a Neural Network to Train a Ruby Twitter bot](http://www.fullstackfest.com/agenda/skynet-for-beginners-using-a-neural-network-to-train-a-ruby-twitter-bot)
* [Neural Networks, Manifolds, and Topology](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)
* [Deep Neural Network with Pre-training](http://enginius.tistory.com/607)
* [Build your own neural network classifier in R](http://junma5.weebly.com/data-blog/build-your-own-neural-network-classifier-in-r)
* [GNU Gneural Network](https://www.gnu.org/software/gneuralnetwork/)
* [Neural Networks Demystified](http://lumiverse.io/series/neural-networks-demystified)
* [colornet](https://techstory.shma.so/colornet-c10ec398cd45)
* [Sketch-simplifying neural network lets artists leap from pencil to ink](http://boingboing.net/2016/04/28/sketch-simplifying-neural-netw.html)
* [Neural Networks Are Impressively Good At Compression](https://probablydance.com/2016/04/30/neural-networks-are-impressively-good-at-compression/)
* [An Analysis of Deep Neural Network Models for Practical Applications](https://arxiv.org/pdf/1605.07678v1.pdf)
* [Why are Eight Bits Enough for Deep Neural Networks?](https://petewarden.com/2015/05/23/why-are-eight-bits-enough-for-deep-neural-networks/)
* [Adventures learning Neural Nets and Python](http://katbailey.github.io/post/neural-nets-in-python/)
* [Deep Learning in Neural Networks: An Overview](http://arxiv.org/abs/1404.7828)
* [Using Neural Networks With Regression](http://deeplearning4j.org/linear-regression.html)
* [Neural networks for algorithmic trading. Part One — Simple time series forecasting](https://medium.com/@alexrachnog/neural-networks-for-algorithmic-trading-part-one-simple-time-series-forecasting-f992daa1045a)
* [Deep Learning using Deep Neural Networks](https://www.linkedin.com/pulse/deep-learning-using-neural-networks-niraj-kumar)
* [K-Fold Cross-Validation for Neural Networks](https://jamesmccaffrey.wordpress.com/2013/10/25/k-fold-cross-validation-for-neural-networks/)
* [What is the Role of the Activation Function in a Neural Network?](http://www.kdnuggets.com/2016/08/role-activation-function-neural-network.html)
  * 신경망에서 activation 함수와 cost(또는 loss, target, objective) 함수는 별개
    * Activation함수, cost함수에 어떤 심오한 (과학적) 의미는 없고, 그냥 인공 신경망을 잘 동작시키기 위해 만든 함수
  * Activation 함수
    * 개별 뉴런에 적용
    * 그 뉴런에 들어온 입력(들)의 합을 출력으로 바꾸는 역할
    * 입력을 그냥 scale 정도 해서 그대로 출력으로 내 보내는 linear 타입, sigmoid 타입, hyperbolic tangent 타입이 존재
  * Cost/loss 함수
    * activation 함수와는 별개로 보통은 신경망 전체에 적용
    * NN이 weight나 bias를 학습할 때 (최적화 할 때) 지표 (metric)이 되는 함수
    * weight, bias를 변수로 갖고 있고, 보통 이 loss/cost를 낮추는 (함수가 에러/cost를 나타낼 때, gradient descent (gradient 반대 방향) 또는 높이는 방향 (함수가 장점/merit을 나탸낼 때, gradient 방향) 으로 만드는 weight(bias)를 계산
    * Sigmoid activation 함수는 초기부터 사용
      * Squared Error 타입 cost 함수와 같이 쓰면 saturation 발생(입력이 매우 negative, 또는 positive 여서 activation이 0 또는 1에 가까울 때), 초기화 잘못으로 학습이 거의 일어나지 않음
    * Cross-entropy loss 함수같은 것은 sigmoid neuron에 대해 써도 이런 saturation 문제 미발생
    * (linear 타입) Relu 를 쓰면, Squared Error 타입 loss 함수를 써도 saturation 미발생
* [Sketch Simplification](http://hi.cs.waseda.ac.jp/~esimo/en/research/sketch/)
* [Neural Network-based Sketch Simplification](http://hi.cs.waseda.ac.jp:8081/)
* [논문 요약 - Deep Neural Networks for YouTube Recommendations](http://keunwoochoi.blogspot.com/2016/09/deep-neural-networks-for-youtube.html)
* [Neural Network Architectures](https://culurciello.github.io/tech/2016/06/04/nets.html)
* [THE NEURAL NETWORK ZOO](http://www.asimovinstitute.org/neural-network-zoo/)
  * [번역](https://www.facebook.com/SKTBrain/photos/pcb.306040569766764/306035899767231/?type=3&theater)
* [10 misconceptions about Neural Networks](http://www.turingfinance.com/misconceptions-about-neural-networks/)
* [딥러닝_Neural Network_멀티 퍼셉트론1](http://m.blog.naver.com/dunopiorg/220180453865)
* [인공지능(뉴럴 네트워크) 베토벤 월광소나타 훈련시키기](http://blog.naver.com/atelierjpro/220851418829)
* [Four Experiments in Handwriting with a Neural Network](http://distill.pub/2016/handwriting/)
* [Neural Network Architectures](https://culurciello.github.io/tech/2016/06/04/nets.html)
* [CorrNet - an implementation of Correlational Neural Network (CorrNet)](https://github.com/jskDr/CorrNet)
* [Coding a Deep Neural Network to Steer a Car: Step By Step](https://medium.com/udacity/coding-a-deep-neural-network-to-steer-a-car-step-by-step-c075a12108e2)
* [뉴럴네트워크, 그것이 알고싶다](https://medium.com/@deepvalidation/%EB%89%B4%EB%9F%B4%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-%EA%B7%B8%EA%B2%83%EC%9D%B4-%EC%95%8C%EA%B3%A0%EC%8B%B6%EB%8B%A4-8de810a97e69)
* [Stuttgart Neural Network Simulator](http://www.ra.cs.uni-tuebingen.de/SNNS/)

## ConvNets
* [컨볼루셔널 뉴럴넷 (Convolutional Neural Network)](http://t-robotics.blogspot.com/2016/05/convolutional-neural-network_31.html)
  * [ConvNet을 시계열 데이터에 적용하는 세가지 방법](https://www.facebook.com/terryum/posts/10154337242359417)
    * Convolutional Neural Network (ConvNet, 또는 CNN)은 원래는 2D 이미지를 인식하기 위해 만듦
      * 뛰어난 성능에 다른 영역에서도 점점 CNN을 적용
      * CNN은 기본적으로 shared parameter를 통해 계산량을 줄이는 동시에 overfitting도 완화해주고 더욱 유용한 피쳐를 생성해주는 등 classification에 좋음
    * 이것을 시계열 데이터(time-series data)에 적용하려면 기본적으로 각각의 데이터마다 길이가 다른 문제를 해결해야 함
      * 예를 들어 음성인식을 한다고 하면 각각 단어마다 길이가 다른데, 뉴럴넷은 기본적으로 고정된 사이즈의 벡터를 인풋으로 받는다는 것이 문제
    * 가장 간단한 해결책은 아마 fixed size window를 슬라이딩하면서 적용하는 것
      * 예를 들어 길이가 하나는 1000이고, 하나는 1200이라면 사이즈 100짜리 윈도우로 각각 10개, 12개의 벡터들을 뽑고 각각을 독립된 예제들로 간주
    * 하지만 이건 그렇게 좋은 방법은 아님
      * 왜냐하면 어떤건 앞쪽 부분을 보고, 어떤건 가운데를 보고, 어떤건 뒤쪽을 보는데 이들을 모두 같은 데이터로 학습해야하기 때문
      * 물론 이 데이터 위에 RNN과 같은 것을 쌓을 수도 있겠지만, 암튼 이건 좀 bruteforce
    * 음성인식에선 이것을 HMM을 통해 해결
      * 딥러닝이 나오기 이전, 음성인식은 보통 HMM-GMM (Hidden Markov Model - Gaussian Mixture Model)을 이용해 해결
      * 아주 간단히 말해 연속된 데이터를 몇 개의 Gaussian의 states로 모델링하고 이를 학습
      * 최근의 딥러닝의 도입은 GMM을 딥러닝으로 대체함으로서 GMM-DNN모델을 제시
    * CNN을 음성인식에 적용하는 기본적인 방법은 먼저 HMM-GMM을 통해 대략의 states를 학습한 후 GMM을 CNN으로 대체해 다시 학습
      * 이렇게 하면 기존엔 아주 많은 윈도우를 각각 학습했어야 하는 것과 달리, 이제는 적절한 크기의 states들만 학습하면 됨
    * 자연어처리에선 max pooling over time을 통해 이 문제를 해결
      * 예를 들어 "나는 오늘 아침에 학교에 갔어요"란 문장을 배운다면 Convolution window를 (나는, 오늘), (나는, 오늘, 아침에), (오늘, 아침에) 등등에 적용한 이후 각각의
윈도우로부터 딱 한 개의 값들만을 max pool
      * 이렇게 하면 만약 feature map의 갯수만 같다면 원래 문장의 길이와는 상관없이 동일한 길이의 벡터가 추출
        *  각각의 피쳐맵에서 딱 한 개씩만 값들을 추출하기 때문
      * 이걸 마지막에 기본 뉴럴넷(FFNN)에 넣음으로서 문장 분류와 같은 일을 함
    * 음성인식과 자연어처리가 다른 점
      * 자연어처리(문장 분류)는 이미 문장 단위로 segment 되어있는 상태에서 다른 길이들을 처리
      * 음성인식은 연속적인 데이터에서 임의로 states를 나누는 경우라는 점
    * [Convolutional neural networks for speech recognition (2014)](http://research-srv.microsoft.com/…/…/TASLP2339736-proof.pdf)
    * [Convolutional neural networks for sentence classification (2014)](http://arxiv.org/pdf/1408.5882)
    * [[1509.01626] Character-level Convolutional Networks for Text Classification](http://arxiv.org/abs/1509.01626) 자연어를 word 단위로 보는 것이 아니라 character 단위로 보고 마치 한글자 한글자를 웨이브의 한 점처럼 생각
    * Max over time pooling같은 경우 대부분의 sentence classification류의 문제에서 '실용적으로' 잘 동작
      * 굳이 한계점을 꼽자면 feature가 (예로 들어주신 것 처럼, '나는 오늘'과 같은 단어들을 검출할거라고 예상되는) 문장 내에서 나왔는지/없었는지만을 볼 수 있고, 몇 번 나왔는지는 알 수 없다는 단점
      * 따라서, 긴 문장, 혹은 대화/문서까지를 다룬다고 하면 feature extractor로써 적절하지 않을 것
      * 이를 조금 보완한 것이 [dynamic k-max pooling](http://www.aclweb.org/anthology/P14-1062)
    * 시계열을 다룰때는 (음성인식이나, 자연어처리나) RNN이 더 적합하다고 생각
      * 물론 task가 단순하고, 데이터가 적다면 CNN이나 심지어는 전통적인 TF-IDF방법이 더 좋은 경우도 있음
* [My 1st Kaggle ConvNet: Getting to 3rd Percentile in 3 months](http://ilyakava.tumblr.com/post/125230881527/my-1st-kaggle-convnet-getting-to-3rd-percentile)
* [Image Scaling using Deep Convolutional Neural Networks](http://engineering.flipboard.com/2015/05/scaling-convnets/)
* [ConvnetJS demo: Image "Painting"](http://cs.stanford.edu/people/karpathy/convnetjs/demo/image_regression.html)
* [Fast Convolutional Nets With fbfft: A GPU Performance Evaluation](https://research.facebook.com/publications/695244360582147/fast-convolutional-nets-with-fbfft-a-gpu-performance-evaluation/)
* [Learning Game of Life with a Convolutional Neural Network](http://danielrapp.github.io/cnn-gol/)
* [A Tutorial on Deep Learning Part 2: Autoencoders, Convolutional Neural Networks and Recurrent Neural Networks](http://www-cs.stanford.edu/~quocle/tutorial2.pdf)
* [Texture Synthesis with Convolutional Neural Networks](http://bethgelab.org/deeptextures/)
* [Understanding Convolutional Neural Networks for NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)
* [Convolutional Neural Network (CNN)](http://enginius.tistory.com/608)
* [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/syllabus.html)
  * [Convolutional Neural Networks (CNNs / ConvNets)](http://cs231n.github.io/convolutional-networks/)
  * [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/)
  * [archive.org/download/cs231n-CNNs](https://archive.org/download/cs231n-CNNs)
  * [Andrej Karpathy](https://www.youtube.com/channel/UCPk8m_r6fkUSYmvgCBwq-sw)
  * [CS231n : Neural Networks Part 1: Setting up the Architecture (한국어 번역)](http://ishuca.tistory.com/381)
  * [CS231n Winter 2016 Lecture 4 Backpropagation, Neural Networks 1-Q_UWHTY_TEQ.mp4](https://www.youtube.com/watch?v=GZTvxoSHZIo&feature=youtu.be&t=1h11m38s)
  * [Visualizing what ConvNets learn](http://cs231n.github.io/understanding-cnn/)
  * [CS231n/Module 1: Neural Networks](http://ishuca.tistory.com/category/CS231n/Module%201%3A%20Neural%20Networks)
  * [CONVOLUTIONAL NEURAL NETWORKS FOR VISUAL RECOGNITION](http://online.stanford.edu/course/convolutional-neural-networks-visual-recognition)
* [Implementing a CNN for Text Classification in TensorFlow](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/)
* [Case Study of Convolutional Neural Network](http://www.slideshare.net/nmhkahn/case-study-of-convolutional-neural-network-61556303)
* [Denoising auto encoders(d a)](http://www.slideshare.net/taeyounglee1447/denoising-auto-encodersd-a)
* [Must Know Tips/Tricks in Deep Neural Networks (by Xiu-Shen Wei)](http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html)
* [Kashif Rasul - Intro to ConvNets](https://www.youtube.com/watch?v=W9_SNGymRwo)
* [ConvNetJS CIFAR-10 demo](http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html)
* [CNN(Convolution Neural Network)으로 인물을 인식 시켜보자...](https://github.com/jaeho-kang/deep-learning/blob/master/blog/post1/contents.md)
* [VGG Convolutional Neural Networks Practical](http://www.robots.ox.ac.uk/~vgg/practicals/cnn/)
* [Q Learning과 CNN을 이용한 Object Localization](http://www.slideshare.net/ssuser06e0c5/q-learning-cnn-object-localization)
* [Benchmarks for popular CNN models](https://github.com/jcjohnson/cnn-benchmarks)
* [A Beginner's Guide To Understanding Convolutional Neural Networks](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/)
  * [[번역] A Beginner's Guide To Understanding Convolutional Neural Networks](http://steady7.tistory.com/m/7)
* [A Beginner's Guide To Understanding Convolutional Neural Networks Part 2](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/)
  * [[번역] A Beginner's Guide To Understanding Convolutional Neural Networks Part 2](http://steady7.tistory.com/8)
* [The 9 Deep Learning Papers You Need To Know About (Understanding CNNs Part 3)](https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)
* [Faster R-CNN in MXNet with distributed implementation and data parallelization](https://github.com/dmlc/mxnet/tree/master/example/rcnn)
* [Faster R-CNN](https://curt-park.github.io/2017-03-17/faster-rcnn/)
* [A guide to convolution arithmetic for deep learning](https://tensorflowkorea.wordpress.com/a-guide-to-convolution-arithmetic-for-deep-learning/)
* [An Intuitive Explanation of Convolutional Neural Networks](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)
* [An Intuitive Explanation of Convolutional Neural Networks](https://www.opendatascience.com/blog/an-intuitive-explanation-of-convolutional-neural-networks/?utm_source=Open+Data+Science+Newsletter&utm_campaign=f4ea9cc60f-EMAIL_CAMPAIGN_2016_12_21&utm_medium=email&utm_term=0_2ea92bb125-f4ea9cc60f-245860601)
* [‘구글 맵’ 영상에 AI 접목하니, 빈곤국가 경제실태 한눈에](http://www.dongascience.com/news/view/13461)
* [Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks](https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721)
* [Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78)
* [NeoCognitron](https://youtu.be/Qil4kmvm2Sw)
  * [1980년 논문: Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position](http://www.cs.princeton.edu/…/co…/Readings/Fukushima1980.pdf)
  * [홈페이지](http://personalpage.flsi.or.jp/fukushima/)
  * [Scholarpedia](http://www.scholarpedia.org/article/Neocognitron)
* [GRAPH CONVOLUTIONAL NETWORKS](http://tkipf.github.io/graph-convolutional-networks/)
* [Convolutional neural network in practice](http://www.slideshare.net/ssuser77ee21/convolutional-neural-network-in-practice)
* [A Beginner's Guide To Understanding Convolutional Neural Networks](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/)
* [딥러닝 - 초보자를 위한 컨볼루셔널 네트워크를 이용한 이미지 인식의 이해](http://bcho.tistory.com/1149)
* [딥러닝을 이용한 숫자 이미지 인식 #1/2](http://bcho.tistory.com/1156)
* [딥러닝을 이용한 숫자 이미지 인식 #2/2](http://bcho.tistory.com/1157)
* [Speed/accuracy trade-offs for modern convolutional object detectors](https://arxiv.org/pdf/1611.10012v1.pdf)
  * 구글에서 요즘 나오는 CNN 기반의 object detectors들을 정리
  * Faster R-CNN, R-FCN, SSD 등 디텍션 알고리즘을 다양한 방법으로 실험해 보고 자세히 결과를 리포트
* [CNN VS Preschool Student Eyes](http://loveayase.tumblr.com/post/155708552419/cnn-vs-preschool-student-eyes)
* [DyNet - The Dynamic Neural Network Toolkit](https://github.com/clab/dynet)
* **[CNN 역전파를 이해하는 가장 쉬운 방법 The easist way to understand CNN backpropagation](https://metamath1.github.io/cnn/index.html)**
* [Paints Chainer - line drawing colorizer using chainer. Using CNN, you can colorize your scketch automatically / semi-automatically](https://github.com/taizan/PaintsChainer)
  * [paintschainer.preferred.tech](http://paintschainer.preferred.tech/)
* [Convolutional Neural Networks (CNNs): An Illustrated Explanation](http://xrds.acm.org/blog/2016/06/convolutional-neural-networks-cnns-illustrated-explanation/)
* [CNNs from different viewpoints - Prerequisite: Basic neural networks](https://medium.com/@matthewkleinsmith/cnns-from-different-viewpoints-fab7f52d159c)

## LSTM
* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
* [엘에스티엠 네트워크 이해하기 (Understanding LSTM Networks)](http://roboticist.tistory.com/m/post/571)
* [GRU & LSTM for machine translations.ipynb](https://github.com/kobikun/study/blob/master/babelpish/GRU_LSTM_for_machine_translation/GRU%20%26%20LSTM%20for%20machine%20translations.ipynb)
* [Backpropogating an LSTM: A Numerical Example](http://blog.aidangomez.ca/2016/04/17/Backpropogating-an-LSTM-A-Numerical-Example/)
* [LSTM Networks for Sentiment Analysis](http://deeplearning.net/tutorial/lstm.html)
* [[번역] 니코니코동화의 공개코멘트 데이터를 Deep Learning로 해석하기](https://blog.umay.be/2016/06/02/niconico-nlp.html)
  * [わかるLSTM ～ 最近の動向と共に](http://qiita.com/t_Signull/items/21b82be280b46f467d1b)
* [Unsupervised Learning of Video Representations using LSTMs](http://nbviewer.jupyter.org/github/babelpish/deep-elastic/blob/master/part2/paper/unsupervised_lstms_video/Unsupervised_Learning_of_Video_Representations_using_LSTMs.ipynb)
* [LSTMVis - Visual Analysis for Recurrent Neural Networks](http://lstm.seas.harvard.edu/)
* [LSTM(RNN) 소개](https://brunch.co.kr/@chris-song/9)
* [Understanding the new Google Translate](https://codesachin.wordpress.com/2017/01/18/understanding-the-new-google-translate/)
* [Knowing when to look : Adaptive Attention via A Visual Sentinel for Image Captioning](http://www.slideshare.net/ssuser06e0c5/knowing-when-to-look-adaptive-attention-via-a-visual-sentinel-for-image-captioning)
  * 본 논문에서는 Hidden layer 뒤에 추가적인 새로운 시각중지 벡터 (visual sentinel vector)를 갖는 LSTM의 확장형을 채택함
  * 시각신호로부터 필요 시 언어모델로 전환이 가능한 Adaptive attention encoder-decoder framework을 제안
  * 이로 인하여 “white”, “bird”, “stop,”과 같은 시각적 단어에 대해서는 좀 더 이미지에 집중하고, “top”, “of”, “on.”의 경우에는 시각중지를 사용함으로서 Image Captioning의 정확도를 향상
  * “Image Captioning”기술은 위성이나 항공영상 분석의 경우 아주 중요한 기술
  * 일반적인 Image Captioning에 비해 예상외로 아주 수월하게 처리가 가능
* [Clickbaits Revisited: Deep Learning on Title + Content Features to Tackle Clickbaits](https://www.linkedin.com/pulse/clickbaits-revisited-deep-learning-title-content-features-thakur)
  * [Clickbaits Revisited](https://github.com/abhishekkrthakur/clickbaits_revisited)

## Python
* [A Neural Network in 11 lines of Python (Part 1)](http://iamtrask.github.io/2015/07/12/basic-python-network/)
  * [11줄의 파이썬 코드로 뉴럴 네트워크를 만들어보자](http://ddanggle.github.io/ml/ai/cs/2016/07/16/11lines.html)
* [A Neural Network in 13 lines of Python (Part 2 - Gradient Descent)](http://iamtrask.github.io/2015/07/27/python-network-part2/)
  * [13줄의 파이썬 코드로 뉴럴 네트워크를 만들어보자. (파트2 - 경사하강법)](http://ddanggle.github.io/ml/ai/cs/2016/09/03/13lines.html)
  * [13Lines.ipynb](https://github.com/DDanggle/blogNetwork/blob/master/13Lines.ipynb)
* [Hinton's Dropout in 3 Lines of Python](http://iamtrask.github.io/2015/07/28/dropout/)
* [NeuPy - Neural Networks in Python](http://neupy.com/)
* [Neural Doodle - Use a deep neural network to borrow the skills of real artists and turn your two-bit doodles into masterpieces](https://github.com/alexjc/neural-doodle)
  * [Feed-forward neural doodle](http://dmitryulyanov.github.io/feed-forward-neural-doodle/)
  * [Online neural doodle](https://likemo.net/)
* [Training (deep) Neural Networks Part: 1](http://upul.github.io/2015/10/12/Training-(deep)-Neural-Networks-Part:-1/)
* [Deep learning – Convolutional neural networks and feature extraction with Python](http://blog.christianperone.com/2015/08/convolutional-neural-networks-and-feature-extraction-with-python/)
* [Irene Chen A Beginner's Guide to Deep Learning PyCon 2016](https://www.youtube.com/watch?v=nCPf8zDJ0d0)
* [Introduction to Deep Learning with Python](https://www.youtube.com/watch?v=S75EdAcXHKk&feature=share)
* [A Complete Guide on Getting Started with Deep Learning in Python](https://www.analyticsvidhya.com/blog/2016/08/deep-learning-path/)
* [Python Code Suggestions Using a Long Short-Term Memory RNN](http://blog.algorithmia.com/python-code-suggestions-lstm-rnn)
* [Python for Image Understanding: Deep Learning with Convolutional Neural Nets](http://www.slideshare.net/roelofp/python-for-image-understanding-deep-learning-with-convolutional-neural-nets)
* [neon Fast, scalable, easy-to-use Python based Deep Learning Framework by Nervana™ http://neon.nervanasys.com/](https://github.com/NervanaSystems/neon)
* [Fitting Gaussian Process Models in Python](https://blog.dominodatalab.com/fitting-gaussian-process-models-python/)
  * [Triple Pendulum CHAOS!](http://jakevdp.github.io/blog/2017/03/08/triple-pendulum-chaos/)
* [hamait.tistory.com/category/통계&머신러닝&딥러닝](http://hamait.tistory.com/category/%ED%86%B5%EA%B3%84%20%26%20%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%26%20%EB%94%A5%EB%9F%AC%EB%8B%9D)
  * [파이썬 코딩으로 말하는 데이터 분석 - 6. 경사하강법](http://hamait.tistory.com/747)

## Recurrent Flow Net
* [Recurrent Flow Network on Different Error Rates](https://www.youtube.com/watch?v=twR3wYjwLrM)
  * binary matrix를 입력으로 받아서, 미래의 matrix를 예측하며, 각 cell의 속도 역시 구할 수 있음
  * 예를 들어서 matrx의 1과 0이 해당 공간이 점유됨과 비어있음을 의미하면, 이 네트워크는 무인 자동차와 같은 어플리케이션에서 장애물들의 속도와 미래에 어떻게 움직일지를 예측 가능
  * 구조적으론 재귀 신경망 구조이지만, 기존의 RNN과는 모든 연산과 구조가 다릅니다.
  * 장점
    * 노이즈에 강인. 동영상에서 알 수 있듯이 왼쪽의 입력 matrix에 노이즈가 많이 있어도 오른쪽의 예측된 방향은 꽤나 정확
    * 빠른 속도. 200 * 200의 행렬을 받아 처리하는데 40ms 이하(MATLAB coder 환경)
  * [매트랩 코드](https://github.com/sjchoi86/RecurrentFlowNet)

## Recurrent Neural Net
* [Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN) Baby steps to your neural network's first memories.](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)
* **[Awesome Recurrent Neural Networks - A curated list of resources dedicated to recurrent neural networks](https://github.com/kjw0612/awesome-rnn)**
* **[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)**
* [opinion mining with deep recurrent nets](http://www.cs.cornell.edu/~oirsoy/drnt.htm)
* [Composing Music With Recurrent Neural Networks](http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/)
* [Composing Music with LSTM Recurrent Networks - Blues Improvisation](http://people.idsia.ch/~juergen/blues/)
* [Training a Recurrent Neural Network to Compose Music](https://maraoz.com/2016/02/02/abc-rnn/)
* [A Recurrent Neural Network Music Generation Tutorial](https://magenta.tensorflow.org/2016/06/10/recurrent-neural-network-generation-tutorial/)
* [Recurrent Neural Network, Fractal for Deep Learning](http://www.slideshare.net/uspace/recurrent-neural-network-fractal-for-deep-learning)
* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
* [Multi-layer Recurrent Neural Networks (LSTM, GRU, RNN) for character-level language models in Torch](https://github.com/karpathy/char-rnn)
* [mrchrisjohnson Recurrent Neural Shady](https://soundcloud.com/mrchrisjohnson/recurrent-neural-shady)
* [Recurrent neural network (depth=3) generates next 1,000 bytes of "Let It Go":](http://elnn.snucse.org/sandbox/music-rnn/)
* [recurrent neural network handwriting generation demo](http://www.cs.toronto.edu/~graves/handwriting.cgi?text=Recurrent+neural+nets+are+fucking+magical.&style=&bias=0.5&samples=3)
* [Teaching recurrent Neural Networks about Monet](http://blog.manugarri.com/teaching-recurrent-neural-networks-about-monet/)
* [Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)
  * [Recurrent Neural Network (RNN) Tutorial - Part 1](http://aikorea.org/blog/rnn-tutorial-1/)
* [Recurrent Neural Networks Tutorial, Part 2 – Implementing a RNN with Python, Numpy and Theano](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/)
  * [RNN Tutorial Part 2 - Python, NumPy와 Theano로 RNN 구현하기](http://aikorea.org/blog/rnn-tutorial-2/)
* [Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)
  * [RNN Tutorial Part 3 - BPTT와 Vanishing Gradient 문제](http://aikorea.org/blog/rnn-tutorial-3/)
* [Recurrent Neural Network Tutorial, Part 4 – Implementing a GRU/LSTM RNN with Python and Theano](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/)
  * [RNN Tutorial Part 4 - GRU/LSTM RNN 구조를 Python과 Theano를 이용하여 구현하기](http://aikorea.org/blog/rnn-tutorial-4/)
* [Auto-Generating Clickbait With Recurrent Neural Networks](http://larseidnes.com/2015/10/13/auto-generating-clickbait-with-recurrent-neural-networks/)
* [Modeling Molecules with Recurrent Neural Networks](http://csvoss.github.io/projects/2015/10/08/rnns-and-chemistry.html)
* [Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN) Baby steps to your neural network's first memories](http://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)
* [char_rnn_ari 한글 Character RNN 구현](https://github.com/bluedisk/char_rnn_ari)
* [Introduction to Recurrent Networks in TensorFlow](http://www.kdnuggets.com/2016/05/intro-recurrent-networks-tensorflow.html)
* [Recurrent Flow Network for Occupancy Flow](https://github.com/sjchoi86/RecurrentFlowNet)
* [Large-Scale Item Categorization in e-Commerce Using Multiple Recurrent Neural Networks](http://www.kdd.org/kdd2016/subtopic/view/large-scale-item-categorization-in-e-commerce-using-multiple-recurrent-neur/)
* [RNNS IN TENSORFLOW, A PRACTICAL GUIDE AND UNDOCUMENTED FEATURES](http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/)
  * [RNNs in Tensorflow, A Practical Guide and Undocumented Features](https://tgjeon.github.io/post/rnns-in-tensorflow/)
* [Recurrent Neural Network tutorial (2nd)](http://www.slideshare.net/uspace/recurrent-neural-network-tutorial-2nd)
* [Attention and Augmented Recurrent Neural Networks](http://distill.pub/2016/augmented-rnns/)
* [Fast Weights RNN](https://tensorflowkorea.wordpress.com/2016/10/25/fast-weights-rnn/)
* [Recurrent Neural Networks for Beginners (Tutorial)](https://medium.com/@awjuliani/recurrent-neural-networks-for-beginners-24288e37ac91)
* [RNN(Recurrent Neural Network)과 Torch로 발라드곡 작사하기](http://www.popit.kr/rnnrecurrent-neural-network%EA%B3%BC-torch%EB%A1%9C-%EB%B0%9C%EB%9D%BC%EB%93%9C%EA%B3%A1-%EC%9E%91%EC%82%AC%ED%95%98%EA%B8%B0/)
* [Recurrent Neural Network Writes Music and Shakespeare Novels | Two Minute Papers](https://www.youtube.com/watch?v=Jkkjy7dVdaY)

# Library
* [프로그래밍 언어별 딥러닝 라이브러리 정리](http://aikorea.org/blog/dl-libraries/)
* [어떤 Deep Learning Library를 선택해야하나요?](http://tmmse.xyz/choosing-deep-learning-libraries/)
* [15 Deep Learning Libraries](http://www.datasciencecentral.com/profiles/blogs/here-are-15-libraries-in-various-languages-to-help-implement-your)
* [15 Deep Learning Tutorials](http://www.datasciencecentral.com/profiles/blogs/15-deep-learning-tutorials)
* [50 Deep Learning Software Tools and Platforms, Updated](http://www.kdnuggets.com/2015/12/deep-learning-tools.html)
* [A.I. Duet - A piano that responds to you](https://github.com/googlecreativelab/aiexperiments-ai-duet)
* [Computational Network Toolkit (CNTK)](https://cntk.codeplex.com)
* Caffe
  * [윈도우에서 Caffe 이용하기](https://github.com/jaeho-kang/deep-learning/blob/master/%EC%9C%88%EB%8F%84%EC%9A%B0%EC%97%90%EC%84%9C%20caffe%20%EC%9D%B4%EC%9A%A9%ED%95%98%EA%B8%B0.md)
  * [Setting Caffe on Windows with CUDA & Python](http://m.blog.naver.com/bsh0128/220733003127)
  * [A DSL for deep neural networks, supporting Caffe and Torch http://ajtulloch.github.io/dnngraph](https://github.com/ajtulloch/dnngraph)
  * [Deep Dreams (with Caffe)](https://github.com/google/deepdream/blob/master/dream.ipynb)
  * [Running Google’s Deep Dream on Windows (with or without CUDA) – The Easy Way](http://thirdeyesqueegee.com/deepdream/2015/07/19/running-googles-deep-dream-on-windows-with-or-without-cuda-the-easy-way/)
  * [Deep Learning and Caffe](http://whydsp.org/319)
  * [[Deep Learning] 영상을 이용하기위한 Convolutional Neural Networks, CNN](http://jangjy.tistory.com/181)
  * [Modeling Images, Videos and Text Using the Caffe Deep Learning Library, part 1 (by Kate Saenko)](http://www.slideshare.net/ktoshik/kate-saenko-msr-russia-summer-school-modeling-images-video-text-caffe-dl-part1)
  * [Apply simple pruning on Caffemodel](https://github.com/garion9013/impl-pruning-caffemodel)
  * [Caffe to TensorFlow](https://github.com/ethereon/caffe-tensorflow)
  * [github.com/DeepLearningStudy/caffe/tree/master/examples](https://github.com/DeepLearningStudy/caffe/tree/master/examples)
  * [C++ Example 1. Hello Caffe](http://deeplearningstudy.github.io/doc_caffe_example_1hellocaffe.html)
    * [Caffe C++ API on Windows](http://blog.naver.com/atelierjpro/220835313030)
  * [SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd)
  * [Netscope CNN Analyzer - A web-based tool for visualizing and analyzing convolutional neural network ](https://dgschwend.github.io/netscope/quickstart.html)
    * CNN Model 분석을 도와줌
    * Model을 prototxt 형태로 넣어주면, 네트워크 구조와, 하단에 CNN Dimension, parameter 수 등의 세부 정보를 정리
  * [caffe-boo - My own caffe-windows with additional layers and features](https://github.com/seokhoonboo/caffe-boo)
* [deepart.io](http://www.deepart.io/) - Generate images styled like your favorite artist
* [DL4J Deep Learning for Java](http://deeplearning4j.org/)
  * [DL4J Java자바를 위한 딥 러닝](http://deeplearning4j.org/kr-index.html)
  * [인공 신경망 및 심층 신경망 소개](http://deeplearning4j.org/kr-neuralnet-overview.html)
  * [A Beginner’s Guide to Recurrent Networks and LSTMs](http://deeplearning4j.org/lstm.html)
  * [Using Neural Networks With Regression](http://deeplearning4j.org/linear-regression.html)
  * [RBM with DL4J for Deep Learning](http://www.slideshare.net/uspace/rbm-with-dl4j-for-deep-learning-50955012)
  * [NN Models with DL4J for Deep Learning](http://www.slideshare.net/uspace/nn-models-with-dl4j-for-deep-learning)
  * [A Beginner’s Guide to Eigenvectors, PCA, Covariance and Entropy](http://deeplearning4j.org/eigenvector)
  * [“딥러닝, 게을러지려고 연구하죠”...아담 깁슨 DL4J 창시자](https://www.imaso.co.kr/news/article_view.php?article_idx=20150824223056)
  * [Exploring convolutional neural networks with DL4J](http://brooksandrew.github.io/simpleblog/articles/convolutional-neural-network-training-with-dl4j/)
  * [Deep Learning Using DL4J and Spark on HDP for Fun and Profit](https://www.youtube.com/watch?v=XCX0GsswDfM)
  * [rl4j - Reinforcement Learning for the JVM](https://github.com/deeplearning4j/rl4j)
  * [MLPClassifierLinear](https://www.youtube.com/watch?v=BN_g2t0ykxg) This is a screencast that shows building a Linear Classifier using a Neural Network
  * [Introduction to Deep Neural Networks](https://deeplearning4j.org/neuralnet-overview)
* [Elephas: Distributed Deep Learning with Keras & Spark](http://maxpumperla.github.io/elephas)
* [Eesen - The official repository of the Eesen project](https://github.com/srvk/eesen)
* [gemmlowp: a small self-contained low-precision GEMM library](https://github.com/google/gemmlowp)
* [Gradient Boosting Interactive Playground](http://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html)
* [Labellio: Scalable Cloud Architecture for Efficient Multi-GPU Deep Learning](http://devblogs.nvidia.com/parallelforall/labellio-scalable-cloud-architecture-efficient-multi-gpu-deep-learning/)
* Lasagne
  * [Lasagne-CTC](https://github.com/skaae/Lasagne-CTC)
* [Mind - Flexible neural networks in JavaScript](http://www.mindjs.net/)
* [Mindori - On-demand GPUs for neural networks](http://mindori.com/)
* [mxnet - Flexible and Efficient Library for Deep Learning](http://mxnet.io/)
  * [mxnet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Scala, Go, Javascript and more http://mxnet.rtfd.org](https://github.com/dmlc/mxnet)
  * [MXNet - Deep Learning Framework of Choice at AWS](http://www.allthingsdistributed.com/2016/11/mxnet-default-framework-deep-learning-aws.html)
  * [Alex Smola at AI Frontiers: Scalable Deep Learning Using MXNet](https://www.slideshare.net/AIFrontiers/scalable-deep-learning-using-mxnet)
* [neural enhance - Super Resolution for images using deep learning](https://github.com/alexjc/neural-enhance)
* [PyCNN - Image Processing in Cellular Neural Networks with Python](http://blog.ankitaggarwal.me/PyCNN/)
  * [Sequence To Sequence Attention Models In PyCNN](https://talbaumel.github.io/attention)
* [pylearn2-practice](https://github.com/zygmuntz/pylearn2-practice)
* [SINGA is a general distributed deep learning platform for training big deep learning models over large datasets](http://singa.apache.org/docs/overview.html)
* [VELES - Distributed platform for rapid Deep learning application development](https://velesnet.ml/)

# Microsoft
* The Microsoft Cognitive Toolkit 마이크로소프트에서 개발한 딥러닝 프레임워크 CNTK
  * [blog](https://blogs.microsoft.com/next/2016/10/25/microsoft-releases-beta-microsoft-cognitive-toolkit-deep-learning-advances)
  * [website](https://www.microsoft.com/en-us/research/product/cognitive-toolkit/)
  * [github.com/Microsoft/CNTK](https://github.com/Microsoft/CNTK)
  * CNTK v1.x; 속도는 빠르지만 C++, C# API만 지원하고 실서비스 배포가 불편한 문제
  * CNTK v2.0; Python API 지원, 최적화된 분산 학습 가능
  * [1-bit SGD, Model sharing 등 최적화된 대용량 처리에 초점을 맞춰서 개발](https://www.microsoft.com/en-us/research/product/cognitive-toolkit/features/)
  * [Image, Speech, Text 분야의 다양한 학습 모델](https://www.microsoft.com/en-us/research/product/cognitive-toolkit/model-gallery/)
  * [학습된 모델을 Azure로 배포, 서비스 가능](https://github.com/Microsoft/CNTK/wiki/Evaluate-a-model-in-an-Azure-WebApi)
* [Building Deep Neural Networks in the Cloud with Azure GPU VMs, MXNet and Microsoft R Server](https://blogs.technet.microsoft.com/machinelearning/2016/09/15/building-deep-neural-networks-in-the-cloud-with-azure-gpu-vms-mxnet-and-microsoft-r-server/)
* [AirSim - Open source simulator based on Unreal Engine for autonomous vehicles from Microsoft AI & Research](https://github.com/Microsoft/AirSim)

# Mooc
* [Deep Learning Courses](http://machinelearningmastery.com/deep-learning-courses/)
* [6.S094: Deep Learning for Self-Driving Cars](http://selfdrivingcars.mit.edu/)
  * [MIT 6.S094: Introduction to Deep Learning and Self-Driving Cars](https://www.youtube.com/watch?v=1L0TKZQcUtA&feature=youtu.be&list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf)
* [How To Get The Best Deep Learning Education For Free](https://www.linkedin.com/pulse/how-get-best-deep-learning-education-forfree-mariya-yao)

# Paper
* [openreview.net](https://openreview.net/)
* [9 Key Deep Learning Papers, Explained](http://www.kdnuggets.com/2016/09/9-key-deep-learning-papers-explained.html/3)
  * 이름 / 이해 난이도 / 읽기 수월함 / 필수성 / 선행지식
  * AlexNet (2012) 하 / 쉬움 / 필수 / 콘볼루션 오퍼레이션 지식, 이미지넷 챌린지
  * ZF Net (2013) 하 / 쉬움 / 옵션 (Segmentation, Localization을 하겠다고 하면 필수) / AlexNet
  * VGG Net (2014) 하 / 쉬움 / 옵션 / AlexNet
  * GoogLeNet (2015) 상 / 어려움 / 옵션 / AlexNet, Hebb 법칙
  * Microsoft ResNet (2015) 중 / 쉬움 / 필수와 옵션의 중간 / AlexNet, VGG Net, NiN(Network in Network)
  * Region Based CNNs (R-CNN - 2013, Fast R-CNN - 2015, Faster R-CNN - 2015)
    * 하 (부분적 상) / 중간 / 옵션 (Segmentation, Localization을 하겠다고 하면 필수. Fast R-CNN을 중심으로 보는게 좋음) / PASCAL 챌린지
  * Generative Adversarial Networks (2014) ? / ? / 필수 / VGG Net
  * Generating Image Descriptions (2014) 상 / 쉬움 / 중간 (이미지 to 문장을 하겠다고 하면 필수) / LSTM, 캡셔닝 챌린지
  * Spatial Transformer Networks (2015) 중 / 어려움 / 옵션 (아직 불명) / 공간변환
* [Arxiv Sanity Preserver](http://www.arxiv-sanity.com/)
* [Computer Vision and Pattern Recognition (cs.CV)](https://scirate.com/arxiv/cs.CV) arXiv에 올라온, CV/PR 주제 논문의 초록만 모아 보여줌
* MNIST 숫자 인식기 Gaussian Bayesian 확률 모델로 구현
  * 목표
    * MNIST 데이터 특성 시각적으로 이해하기
    * Python, numpy, matplotlib 사용해 보기
    * Bayesian Theorem 이해하고 구현해 보기
    * Multivariate Gaussian Distribution 이해하고 구현해 보기
  * 실험 데이터
    * 학습 데이터: MNIST 기본 60,000개
    * 테스트 데이터: MNIST 기본 10,000개
  * 실험 결과
    * Bayesian 확률 모델만으로 분류 정확도가 대략 84% 정도 나오는 것을 확인
    * Multivariate Gaussian 적용하니까 분류 정확도가 대략 92% 정도까지 올라가는 것을 확인
  * 코드
    * [메인 프로그램](https://github.com/dgtgrade/HumanLearning/blob/master/2001a.py)
      * numpy, matplotlib 외에 본격 머신러닝 라이브러리는 전혀 사용하지 않았음
      * 머신러닝 관련 부분 대략 200줄 이하로 매우 짧음
      * 시각화 관련 코드 및 코멘트 등이 대략 300줄 정도임
    * [MNIST 데이터 파일](https://github.com/dgtgrade/HumanLearning/tree/master/data) MNIST 공식 홈페이지에서 받은 그대로
    * [MNIST 데이터 로딩 프로그램](https://github.com/dgtgrade/HumanLearning/blob/master/mnist2ndarray.py)
    * [Multivariate Gaussian 적용하지 않고 Bayesian 확률 모형만으로 돌아가는 코드: 위 2001a.py 옛날 버전](https://github.com/dgtgrade/HumanLearning/blob/8e57a2b3340da3b38956b83cf24433d3a9fbd11b/2001a.py)
  * 실험 동영상
    * 학습: 실험데이터 전체 60000개를 학습하는 과정을 보여줌
    * 테스트: 테스트 데이터 전체 10000개를 테스트 하는 과정을 보여줌
    * 테스트 과정에서 정답률은 1번 후보만으로 구했으나, 표시는 3번후보까지 하였음
* [A Review on a Deep Learning that Reveals the Importance of Big Data](https://fananymi.wordpress.com/)
* [Decoupled Neural Interfaces using Synthetic Gradients](https://arxiv.org/pdf/1608.05343.pdf) synthetic gradient - 뉴럴넷 업데이트 과정의 모듈간 강결합을 decouple
* [Deep Learning Papers Reading Roadmap](https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap)
* [Deep Learning Research Review Week 1: Generative Adversarial Nets](https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-1-Generative-Adversarial-Nets)
* ["Distributed Training of Deep Neuronal Networks: Theoretical and Practical Limits of Parallel Scalability](http://arxiv.org/abs/1609.06870v1)
  * 여러 노드를 썼을 때, 네트웍 벤드위쓰와 전체 노드에서의 계산을 기다리면 어떻게 되는지
  * 싱글 노드에서 할 때 batch 사이즈를 달리하면 어느 layer 계산이 bottleneck인지
  * 이런 문제를 방지하기 위해 디자인을 바꿀 때 어디부터 보면 되는지
  * 계산량을 어떻게 계산하는지
* [Highway and Residual Networks learn Unrolled Iterative Estimation](https://arxiv.org/abs/1612.07771)
  * VGGNet, GoogLeNet, ResNet 등과 같은 매우 많은 계층을 가진 Deep Net 들이 뛰어난 이유를
  * 기존의 각각의 계층이 특정한 추상적인 feature를 대표하며 이를 계층적으로 계산하기 때문이라는 "representation view" 를 뒤집고
  * 각각의 블록 또는 단계마다 단계적인 feature의 변화가 반복적으로 일어난다는 "unrolled iterative estimation" 으로 설명
* [Solving Verbal Comprehension Questions in IQ Test by Knowledge-Powered Word Embedding](http://arxiv.org/pdf/1505.07909v1.pdf)
* [Stacked Approximated Regression Machine: A Simple Deep Learning Approach](https://arxiv.org/pdf/1608.04062v1.pdf)
  * SARM이라는 layer wise training 기법
  * Back propagation 없이 layer 단위로 학습을 시켜도 현재 state of the art DNN과 비슷하거나 더 나은 성능을 보인다는 주장
  * PCANet에 non linearity를 추가
* [The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition ECCV 2016](https://arxiv.org/pdf/1511.06789v3.pdf)
  * Fine-Grained Recognition을 할 때 Noisy Fine-Grained Data, 즉 Web에서 검색한 Noisy하지만 큰 데이터가 도움이 된다는 내용
  * Noisy Fine-Grained Data 구축
    * 새의 종을 구별하는 데이터베이스를 구축한다면, Wikipedia에서 종을 검색하여 그 키워드를 기반으로 구글링하여 이미지 구축
    * 여러 카테고리에 동시에 등장하는 그림을 지우는 등의 간단한 정제작업을 추가
    * 여전히 이 데이터베이스는 롱테일 문제도 있고 에러도 존재
  * 실험 결과, 퀄리티가 좋지만 작은 데이터보다 성능이 좋다
  * 큰 Noisy Fine-Grained Data로 학습한 후 좋은 데이터로 튜닝하면 더 좋다
  * [Factors in Finetuning Deep Model for Object Detection with Long-tail Distribution](http://www.ee.cuhk.edu.hk/~wlouy…/…/OuyangFactors_CVPR16.pdf)
    * Long-tail Distribution을 가진 DB의 문제점 지적
  * [Training Region-based Object Detectors with Online Hard Example Mining](https://arxiv.org/abs/1604.03540,
CVPR2016)
    * easy examples과 hard examples의 너무 큰 차이에 대해서 문제를 지적
  * Fine-Grained Recognition이라는 테스크에 한정된 실험, 분석, 수학적인 설명 부족한 논문
* ["Transfer Learning from Deep Features for Remote Sensing and Poverty Mapping"](http://arxiv.org/abs/1510.00098)
  * [code & data](https://github.com/nealjean/predicting-poverty)
* [Learning to Remember Rare Events](http://www.gitxiv.com/posts/vnbEtdiH3qZEeKBGb/learning-to-remember-rare-events)

# Reinforcement Learning, RL
* [Fundamental of Reinforcement Learning](https://dnddnjs.gitbooks.io/rl/content/)
* [Ujava.org reinforcement-learning](http://www.slideshare.net/uspace/ujavaorg-reinforcementlearning)
* [ujava.org Reinforcement Learning (2nd)](http://www.slideshare.net/uspace/ujavaorg-reinforcement-learning-2nd)
* [ujava.org workshop : Reinforcement Learning with Thompson Sampling](http://www.slideshare.net/uspace/ujavaorg-workshop-reinforcement-learning-with-thompson-sampling)
* [Reinforcement Learning and DQN, learning to play from pixels](https://rubenfiszel.github.io/posts/rl4j/2016-09-08-DQN-Learning-to-play-from-pixels-step-by-step.html)
* [Guest Post (Part I): Demystifying Deep Reinforcement Learning](http://www.nervanasys.com/demystifying-deep-reinforcement-learning/)
  * [딥 강화학습 쉽게 이해하기](http://ddanggle.github.io/ml/ai/cs/2016/09/24/demystifyingDL.html)
* [Deep Learning in a Nutshell: Reinforcement Learning](https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-reinforcement-learning/)
* [Bayesian Programming and Learning for Multi-Player Video Games Application to RTS AI](http://emotion.inrialpes.fr/people/synnaeve/phdthesis/phdthesis.html)
  * [Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks](http://arxiv.org/abs/1609.02993)
  * 스타크래프트 같은 실시간 전략 (RTS) 게임은 체스나 바둑과는 다르게 제한된 자원(미네랄, 가스 등)과 불확실한 정보 (보이지 않는 상대방의 플레이 등) 속에서 의사결정을 해야하는 어려움이 존재
  * 이 논문에서는 “참/거짓”으로 표현되는 boolean logic이 아닌 베이지언 모델링으로 이런 정보의 불확실함(uncertainty)를 처리
* [Deep Learning in a Nutshell: Reinforcement Learning](https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-reinforcement-learning/)
* [강화 학습 기초 Reinforcement Learning an introduction](http://www.slideshare.net/carpedm20/reinforcement-learning-an-introduction-64037079)
* [async-rl-tensorflow - Asynchronous Methods for Deep Reinforcement Learning](https://github.com/devsisters/async-rl-tensorflow)
* [LEARNING REINFORCEMENT LEARNING (WITH CODE, EXERCISES AND SOLUTIONS)](http://www.wildml.com/2016/10/learning-reinforcement-learning/)
* [Reinforcement Learning 101 (in 15 minutes)](https://www.facebook.com/SKTBrain/posts/311444575893030)
* [Bandit 101](https://www.facebook.com/SKTBrain/posts/313678162336338) Multi-Armed Bandit (MAB) 입문자료
* [Nathan Epstein - Reinforcement Learning in Python](https://www.youtube.com/watch?v=rTMa04TZ_MY)
* [Lecture 10 Reinforcement Learning I](https://www.youtube.com/watch?v=IXuHxkpO5E8)
* [Reinforcement learning with unsupervised auxiliary tasks](https://deepmind.com/blog/reinforcement-learning-unsupervised-auxiliary-tasks/)
* [Designing Neural Network Architectures using Reinforcement Learning" (Under review as a conference paper at ICLR 2017)](https://arxiv.org/abs/1611.02167)
* [Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)
* [Simple Reinforcement Learning in Tensorflow: Part 1 - Two-armed Bandit](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149)
* [Simple Reinforcement Learning with Tensorflow Part 1.5: Contextual Bandits](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c)
* [Simple Reinforcement Learning with Tensorflow: Part 2 - Policy-based Agents](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724)
* [Simple Reinforcement Learning with Tensorflow: Part 3 - Model-Based RL](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99)
* [Simple Reinforcement Learning with Tensorflow Part 4: Deep Q-Networks and Beyond](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df)
* [Simple Reinforcement Learning with Tensorflow Part 5: Visualizing an Agent’s Thoughts and Actions](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a)
* [Simple Reinforcement Learning with Tensorflow Part 6: Partial Observability and Deep Recurrent Q-Networks](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc)
* [Simple Reinforcement Learning with Tensorflow Part 7: Action-Selection Strategies for Exploration](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf)
* [Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2)
* [Bridging Cognitive Science and Reinforcement Learning Part 1: Enactivism](https://medium.com/@awjuliani/bridging-cognitive-science-and-reinforcement-learning-part-1-enactivism-601af34ef122)
* [Deep Reinforcement Learning - author: David Silver, Department of Computer Science, University College London](http://videolectures.net/rldm2015_silver_reinforcement_learning/)
  * [Tutorial: Deep Reinforcement Learning - David Silver, Google DeepMind](http://icml.cc/2016/tutorials/deep_rl_tutorial.pdf)
* [Quantum Boltzman Machines for Deep Reinforcement Learning](https://theinformationageblog.wordpress.com/2017/01/20/quantum-boltzman-machines-for-deep-reinforcement-learning/)
* [모두를 위한 머신러닝/딥러닝 강의](http://hunkim.github.io/ml/)
  * [모두를위한RL강좌](https://www.youtube.com/playlist?list=PLlMkM4tgfjnKsCWav-Z2F-MMFRx-2gMGG)
  * [모두를 위한 딥러닝 - Deep Reinforcement Learning](https://www.inflearn.com/course/reinforcement-learning/)
  * [kimhun_rl_windows - I follow the lecture (https://hunkim.github.io/ml/) on windows version](https://github.com/nalsil/kimhun_rl_windows)
  * [atari_py - A Windows-MSYS2-MinGW compatible version of https://github.com/openai/ale_python_interface](https://github.com/rybskej/atari-py)
* [Reinforcement learning](https://www.youtube.com/playlist?list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT)
* [Tutorial: Introduction to Reinforcement Learning with Function Approximation](https://www.youtube.com/watch?v=ggqnxyjaKe4)
  * [Introduction to Reinforcement Learning with Function Approximation](http://media.nips.cc/Conferences/2015/tutorialslides/SuttonIntroRL-nips-2015-tutorial.pdf)
* [DeepHack.RL](http://rl.deephack.me/)
  * [DeepHack.RL (2017)](https://www.youtube.com/playlist?list=PLt1IfGj6-_-efXDATIw4JI92DjMrVed3P)
* [Building a deep learning DOOM bot](https://www.codelitt.com/blog/doom-ai/)
  * [ViZDoom is a Doom-based AI research platform for reinforcement learning from raw visual information](http://vizdoom.cs.put.edu.pl/)
    * [Tutorial](http://vizdoom.cs.put.edu.pl/tutorial)
  * [ViZDoom - Doom-based AI Research Platform for Reinforcement Learning from Raw Visual Information)](https://github.com/mwydmuch/ViZDoom)
  * [Windows에서 VizDoom 설치하기](http://ishuca.tistory.com/401)
* [ishuca.tistory.com/tag/강화학습](http://ishuca.tistory.com/tag/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5)
* [Deep Reinforcement Learning Pieter abbeel](http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/?q=Pieter+abbeel)
* [irelia - Korean Janggi AI using q learning](https://github.com/jireh-father/irelia)
* [강화학습 그리고 OpenAI - 1: Introduction to OpenAI](http://www.modulabs.co.kr/RL_library/1705)
* [Deep Reinforcement Learning: An Overview](https://arxiv.org/abs/1701.07274)
* [강화학습 튜토리알 - 인공 신경망으로 '퐁' 게임을 학습시키자 (Andrej Karpathy 포스트 번역)](http://keunwoochoi.blogspot.com/2016/06/andrej-karpathy.html)
* [Playing Atari with Deep Reinforcement Learning](https://speakerdeck.com/jacksongl/playing-atari-with-deep-reinforcement-learning)

## MOOC
* [CS294: Deep RL Start!](https://tensorflow.blog/2017/01/23/cs294-deep-rl-start/)
  * [CS294-112 Deep Reinforcement Learning Sp17](https://www.youtube.com/playlist?list=PLkFD6_40KJIwTmSbCv9OVJB3YaO4sFwkX)
* [Berkeley CS 294: Deep Reinforcement Learning, Spring 2017](http://rll.berkeley.edu/deeprlcourse/)
  * [[ RL ] CS 294: Deep Reinforcement Learning —(1) Introduction and course overview](https://medium.com/@peteryun/rl-cs-294-deep-reinforcement-learning-introduction-and-course-overview-c7d9cb550ced)
* [MIT 6.S094: Deep Learning for Self-Driving Cars (Lecture 2), 2017](http://selfdrivingcars.mit.edu/)
* [UCL, David Silver, Reinforcement Learning, 2015](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)
* [Stanford Andrew Ng CS229 Lecture 16, 2008](https://www.youtube.com/watch?v=RtxI449ZjSc)
* [Deep Reinforcement Learning](http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/)
* [Implementation of Reinforcement Learning Algorithms. Python, OpenAI Gym, Tensorflow. Exercises and Solutions to accompany Sutton's Book and David Silver's course](https://github.com/dennybritz/reinforcement-learning)
* [Reinforcement Learning: An Introduction](https://webdocs.cs.ualberta.ca/~sutton/book/the-book-2nd.html)

# Spark
* [DeepSpark: Spark-Based Deep Learning Supporting Asynchronous Updates and Caffe Compatibility](http://hgpu.org/?p=15511)
* [yahoo/CaffeOnSpark](https://github.com/yahoo/CaffeOnSpark)
* [SparkNet: Training Deep Networks in Spark](http://arxiv.org/abs/1511.06051)
* [spark-summit.org/2016/schedule](https://spark-summit.org/2016/schedule/)
  * Large-Scale Deep Learning with TensorFlow (Jeff Dean)
    * [slide](http://www.slideshare.net/JenAman/large-scale-deep-learning-with-tensorflow)
    * [video](https://youtu.be/XYwIDn00PAo)
  * [AI: The New Electricity (Andrew Ng)](https://youtu.be/4eJhcxfYR4I)
  * Large Scale Multimedia Data Intelligence And Analysis On Spark (Baidu)
    * [slide](http://www.slideshare.net/JenAman/large-scale-multimedia-data-intelligence-and-analysis-on-spark)
    * [video](https://youtu.be/LrtdyCWphvs)
  * Scaling Machine Learning To Billions Of Parameters (Yahoo)
    * [slide](http://www.slideshare.net/JenAman/scaling-machine-learning-to-billions-of-parameters)
    * [video](https://youtu.be/l_1S7W_l2cI)
  * CaffeOnSpark: Deep Learning On Spark Cluster (Yahoo)
    * [slide](http://www.slideshare.net/JenAman/caffeonspark-deep-learning-on-spark-cluster)
    * [video](https://youtu.be/Mn7QEdUFSnQ)
  * Scalable Deep Learning in Baidu
    * [slide](http://www.slideshare.net/JenAman/scalable-deep-learning-platform-on-spark-in-baidu)
    * [video](https://youtu.be/n9yZNmC20pc)
